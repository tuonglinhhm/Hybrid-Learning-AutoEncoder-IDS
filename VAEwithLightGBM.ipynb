{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, activations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "import os\n",
    "import focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = None\n",
    "seqlen = 60\n",
    "folder = '/Version3/ttp3'\n",
    "\n",
    "# ttp1\n",
    "# tactic_dict = {\"Normal\": 0, \"Discovery\": 1, \"Credential Access\": 2, \"Command and Control\": 3, \"Exfiltration\": 4}\n",
    "# technique_dict = {\"Normal\": 0, \"System Network Configuration Discovery\": 1, \"Network Service Scanning\": 2, \"Brute Force\": 3, \"Network Share Discovery\": 4, \"Remote Access Tools\": 5, \"Data Transfer Size Limits\": 6}\n",
    "# ttp2\n",
    "# tactic_dict = {\"Normal\": 0, \"Discovery\": 1, \"Command and Control\": 2}\n",
    "# technique_dict = {\"Normal\": 0, \"System Network Configuration Discovery\": 1, \"Network Service Scanning\": 2, \"Network Sniffing\": 3, \"Network Share Discovery\": 4, \"Custom Command and Control Protocol\": 5}\n",
    "# ttp3\n",
    "tactic_dict = {\"Normal\": 0, \"Discovery\": 1, \"Credential Access\": 2, \"Command and Control\": 3, \"Lateral Movement\": 4}\n",
    "technique_dict = {\"Normal\": 0, \"Network Service Scanning\": 1, \"Exploitation for Credential Access\": 2, \"Pass the Hash\": 3, \"Pass the Ticket\": 4, \"Remote Access Tools\": 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flgs</th>\n",
       "      <th>Proto</th>\n",
       "      <th>Sport</th>\n",
       "      <th>Dport</th>\n",
       "      <th>TotPkts</th>\n",
       "      <th>TotBytes</th>\n",
       "      <th>State</th>\n",
       "      <th>Dur</th>\n",
       "      <th>Mean</th>\n",
       "      <th>StdDev</th>\n",
       "      <th>Sum</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>SrcPkts</th>\n",
       "      <th>DstPkts</th>\n",
       "      <th>SrcBytes</th>\n",
       "      <th>DstBytes</th>\n",
       "      <th>Rate</th>\n",
       "      <th>SrcRate</th>\n",
       "      <th>DstRate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12.710654</td>\n",
       "      <td>0.016221</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>1.986258e-15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022439</td>\n",
       "      <td>0.022439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022439</td>\n",
       "      <td>0.022439</td>\n",
       "      <td>0.022439</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>3.725290e-08</td>\n",
       "      <td>8.760354e-16</td>\n",
       "      <td>4.237518e-07</td>\n",
       "      <td>3.435925e-15</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.496384</td>\n",
       "      <td>0.016221</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>1.656661e-15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065042</td>\n",
       "      <td>0.065042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.065042</td>\n",
       "      <td>0.065042</td>\n",
       "      <td>0.065042</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>3.725290e-08</td>\n",
       "      <td>7.546047e-16</td>\n",
       "      <td>3.678724e-07</td>\n",
       "      <td>1.185374e-15</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.908078</td>\n",
       "      <td>0.016221</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>1.543904e-15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>3.725290e-08</td>\n",
       "      <td>7.546047e-16</td>\n",
       "      <td>3.376044e-07</td>\n",
       "      <td>8.410770e-13</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.142857</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.485183</td>\n",
       "      <td>2.040154</td>\n",
       "      <td>1.927470e-17</td>\n",
       "      <td>2.775558e-15</td>\n",
       "      <td>14.468085</td>\n",
       "      <td>3.692763</td>\n",
       "      <td>3.692763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.692763</td>\n",
       "      <td>3.692763</td>\n",
       "      <td>3.692763</td>\n",
       "      <td>1.156482e-17</td>\n",
       "      <td>1.117587e-07</td>\n",
       "      <td>1.682682e-15</td>\n",
       "      <td>4.190952e-07</td>\n",
       "      <td>1.043916e-16</td>\n",
       "      <td>4.175659e-17</td>\n",
       "      <td>4.035223e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.636921</td>\n",
       "      <td>0.016221</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>1.812786e-15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015925</td>\n",
       "      <td>0.015925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015925</td>\n",
       "      <td>0.015925</td>\n",
       "      <td>0.015925</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>3.725290e-08</td>\n",
       "      <td>7.459311e-16</td>\n",
       "      <td>4.121102e-07</td>\n",
       "      <td>4.841364e-15</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85303</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.041200</td>\n",
       "      <td>15.944176</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.204170e-17</td>\n",
       "      <td>4.680851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.204170e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85304</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.041200</td>\n",
       "      <td>15.943258</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.204170e-17</td>\n",
       "      <td>4.680851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.204170e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85305</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>18.356578</td>\n",
       "      <td>0.016221</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>1.309716e-15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>3.725290e-08</td>\n",
       "      <td>7.459311e-16</td>\n",
       "      <td>2.770685e-07</td>\n",
       "      <td>8.630454e-13</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85306</th>\n",
       "      <td>11.428571</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.066073</td>\n",
       "      <td>15.044072</td>\n",
       "      <td>1.349229e-16</td>\n",
       "      <td>6.253678e-14</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>17.895837</td>\n",
       "      <td>17.895837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.895837</td>\n",
       "      <td>17.895837</td>\n",
       "      <td>17.895837</td>\n",
       "      <td>8.095374e-17</td>\n",
       "      <td>5.587935e-07</td>\n",
       "      <td>3.429548e-14</td>\n",
       "      <td>7.706694e-06</td>\n",
       "      <td>1.507868e-16</td>\n",
       "      <td>8.616383e-17</td>\n",
       "      <td>5.828613e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85307</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>16.694845</td>\n",
       "      <td>0.016221</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>1.318390e-15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010106</td>\n",
       "      <td>0.010106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010106</td>\n",
       "      <td>0.010106</td>\n",
       "      <td>0.010106</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>3.725290e-08</td>\n",
       "      <td>8.239936e-16</td>\n",
       "      <td>2.584420e-07</td>\n",
       "      <td>7.629256e-15</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85308 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Flgs  Proto      Sport      Dport       TotPkts      TotBytes  \\\n",
       "0       0.000000   20.0  12.710654   0.016221  3.854940e-18  1.986258e-15   \n",
       "1       0.000000   20.0  15.496384   0.016221  3.854940e-18  1.656661e-15   \n",
       "2       0.000000   20.0  15.908078   0.016221  3.854940e-18  1.543904e-15   \n",
       "3      17.142857   10.0  16.485183   2.040154  1.927470e-17  2.775558e-15   \n",
       "4       0.000000   20.0  10.636921   0.016221  3.854940e-18  1.812786e-15   \n",
       "...          ...    ...        ...        ...           ...           ...   \n",
       "85303   0.000000   10.0   0.041200  15.944176  0.000000e+00  5.204170e-17   \n",
       "85304   0.000000   10.0   0.041200  15.943258  0.000000e+00  5.204170e-17   \n",
       "85305   0.000000   20.0  18.356578   0.016221  3.854940e-18  1.309716e-15   \n",
       "85306  11.428571   10.0  15.066073  15.044072  1.349229e-16  6.253678e-14   \n",
       "85307   0.000000   20.0  16.694845   0.016221  3.854940e-18  1.318390e-15   \n",
       "\n",
       "           State        Dur       Mean  StdDev        Sum        Min  \\\n",
       "0       0.000000   0.022439   0.022439     0.0   0.022439   0.022439   \n",
       "1       0.000000   0.065042   0.065042     0.0   0.065042   0.065042   \n",
       "2       0.000000   0.000092   0.000092     0.0   0.000092   0.000092   \n",
       "3      14.468085   3.692763   3.692763     0.0   3.692763   3.692763   \n",
       "4       0.000000   0.015925   0.015925     0.0   0.015925   0.015925   \n",
       "...          ...        ...        ...     ...        ...        ...   \n",
       "85303   4.680851   0.000000   0.000054     0.0   0.000054   0.000054   \n",
       "85304   4.680851   0.000000   0.000024     0.0   0.000024   0.000024   \n",
       "85305   0.000000   0.000089   0.000089     0.0   0.000089   0.000089   \n",
       "85306   0.425532  17.895837  17.895837     0.0  17.895837  17.895837   \n",
       "85307   0.000000   0.010106   0.010106     0.0   0.010106   0.010106   \n",
       "\n",
       "             Max       SrcPkts       DstPkts      SrcBytes      DstBytes  \\\n",
       "0       0.022439  3.854940e-18  3.725290e-08  8.760354e-16  4.237518e-07   \n",
       "1       0.065042  3.854940e-18  3.725290e-08  7.546047e-16  3.678724e-07   \n",
       "2       0.000092  3.854940e-18  3.725290e-08  7.546047e-16  3.376044e-07   \n",
       "3       3.692763  1.156482e-17  1.117587e-07  1.682682e-15  4.190952e-07   \n",
       "4       0.015925  3.854940e-18  3.725290e-08  7.459311e-16  4.121102e-07   \n",
       "...          ...           ...           ...           ...           ...   \n",
       "85303   0.000054  3.854940e-18  0.000000e+00  5.204170e-16  0.000000e+00   \n",
       "85304   0.000024  3.854940e-18  0.000000e+00  5.204170e-16  0.000000e+00   \n",
       "85305   0.000089  3.854940e-18  3.725290e-08  7.459311e-16  2.770685e-07   \n",
       "85306  17.895837  8.095374e-17  5.587935e-07  3.429548e-14  7.706694e-06   \n",
       "85307   0.010106  3.854940e-18  3.725290e-08  8.239936e-16  2.584420e-07   \n",
       "\n",
       "               Rate       SrcRate       DstRate  \n",
       "0      3.435925e-15  0.000000e+00  0.000000e+00  \n",
       "1      1.185374e-15  0.000000e+00  0.000000e+00  \n",
       "2      8.410770e-13  0.000000e+00  0.000000e+00  \n",
       "3      1.043916e-16  4.175659e-17  4.035223e-07  \n",
       "4      4.841364e-15  0.000000e+00  0.000000e+00  \n",
       "...             ...           ...           ...  \n",
       "85303  0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "85304  0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "85305  8.630454e-13  0.000000e+00  0.000000e+00  \n",
       "85306  1.507868e-16  8.616383e-17  5.828613e-07  \n",
       "85307  7.629256e-15  0.000000e+00  0.000000e+00  \n",
       "\n",
       "[85308 rows x 20 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "import ipaddress\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "df = pd.read_csv('./{}/final_{}secs.csv'.format(folder, seqlen))\n",
    "\n",
    "df = df.drop(['StartTime', 'LastTime', 'Rank', 'Seq', 'SrcAddr', 'DstAddr'], axis=1)\n",
    "df['Flgs'] = le.fit_transform(df['Flgs'])\n",
    "df['Proto']= le.fit_transform(df['Proto']) \n",
    "df['State']= le.fit_transform(df['State'])\n",
    "\n",
    "idx = df[df['Sport'].str.contains(\"x\") | df['Dport'].str.contains(\"x\")].index\n",
    "df = df.drop(idx)\n",
    "df = df.dropna()\n",
    "\n",
    "df['Sport'] = df['Sport'].astype(int)\n",
    "df['Dport'] = df['Dport'].astype(int)\n",
    "df['ATT&CK_Tactic'] = df['ATT&CK_Tactic'].replace(tactic_dict)\n",
    "df['ATT&CK_Technique'] = df['ATT&CK_Technique'].replace(technique_dict)\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 20))\n",
    "df[df.columns[:20]] = scaler.fit_transform(df[df.columns[:20]])\n",
    "df[df.columns[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby(list(df)[:-3]).filter(lambda x: len(np.unique(x['Label'])) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flgs</th>\n",
       "      <th>Proto</th>\n",
       "      <th>Sport</th>\n",
       "      <th>Dport</th>\n",
       "      <th>TotPkts</th>\n",
       "      <th>TotBytes</th>\n",
       "      <th>State</th>\n",
       "      <th>Dur</th>\n",
       "      <th>Mean</th>\n",
       "      <th>StdDev</th>\n",
       "      <th>Sum</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>SrcPkts</th>\n",
       "      <th>DstPkts</th>\n",
       "      <th>SrcBytes</th>\n",
       "      <th>DstBytes</th>\n",
       "      <th>Rate</th>\n",
       "      <th>SrcRate</th>\n",
       "      <th>DstRate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Flgs</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.337580</td>\n",
       "      <td>0.208319</td>\n",
       "      <td>-0.089157</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>-0.141425</td>\n",
       "      <td>0.749933</td>\n",
       "      <td>0.749933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.749933</td>\n",
       "      <td>0.749933</td>\n",
       "      <td>0.749933</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>0.006871</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>0.088315</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>0.006202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Proto</th>\n",
       "      <td>-0.337580</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.464880</td>\n",
       "      <td>-0.644498</td>\n",
       "      <td>-0.002817</td>\n",
       "      <td>-0.002817</td>\n",
       "      <td>-0.447305</td>\n",
       "      <td>-0.343025</td>\n",
       "      <td>-0.343028</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.343028</td>\n",
       "      <td>-0.343028</td>\n",
       "      <td>-0.343028</td>\n",
       "      <td>-0.002817</td>\n",
       "      <td>-0.003475</td>\n",
       "      <td>-0.002817</td>\n",
       "      <td>-0.034503</td>\n",
       "      <td>-0.002817</td>\n",
       "      <td>-0.002817</td>\n",
       "      <td>-0.005118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sport</th>\n",
       "      <td>0.208319</td>\n",
       "      <td>0.464880</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.770285</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>-0.246322</td>\n",
       "      <td>0.275464</td>\n",
       "      <td>0.275461</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.275461</td>\n",
       "      <td>0.275461</td>\n",
       "      <td>0.275461</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.003158</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.026635</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.004627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dport</th>\n",
       "      <td>-0.089157</td>\n",
       "      <td>-0.644498</td>\n",
       "      <td>-0.770285</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002958</td>\n",
       "      <td>-0.002958</td>\n",
       "      <td>0.296034</td>\n",
       "      <td>-0.132743</td>\n",
       "      <td>-0.132740</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.132740</td>\n",
       "      <td>-0.132740</td>\n",
       "      <td>-0.132740</td>\n",
       "      <td>-0.002958</td>\n",
       "      <td>-0.003475</td>\n",
       "      <td>-0.002958</td>\n",
       "      <td>-0.034860</td>\n",
       "      <td>-0.002958</td>\n",
       "      <td>-0.002958</td>\n",
       "      <td>-0.004476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotPkts</th>\n",
       "      <td>0.005195</td>\n",
       "      <td>-0.002817</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>-0.002958</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002166</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotBytes</th>\n",
       "      <td>0.005195</td>\n",
       "      <td>-0.002817</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>-0.002958</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002166</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <td>-0.141425</td>\n",
       "      <td>-0.447305</td>\n",
       "      <td>-0.246322</td>\n",
       "      <td>0.296034</td>\n",
       "      <td>-0.002166</td>\n",
       "      <td>-0.002166</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.206417</td>\n",
       "      <td>-0.206416</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.206416</td>\n",
       "      <td>-0.206416</td>\n",
       "      <td>-0.206416</td>\n",
       "      <td>-0.002166</td>\n",
       "      <td>-0.002356</td>\n",
       "      <td>-0.002166</td>\n",
       "      <td>-0.025324</td>\n",
       "      <td>-0.002166</td>\n",
       "      <td>-0.002166</td>\n",
       "      <td>0.003815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dur</th>\n",
       "      <td>0.749933</td>\n",
       "      <td>-0.343025</td>\n",
       "      <td>0.275464</td>\n",
       "      <td>-0.132743</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>-0.206417</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.009479</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.093692</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.749933</td>\n",
       "      <td>-0.343028</td>\n",
       "      <td>0.275461</td>\n",
       "      <td>-0.132740</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>-0.206416</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.009479</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.093692</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StdDev</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sum</th>\n",
       "      <td>0.749933</td>\n",
       "      <td>-0.343028</td>\n",
       "      <td>0.275461</td>\n",
       "      <td>-0.132740</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>-0.206416</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.009479</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.093692</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Min</th>\n",
       "      <td>0.749933</td>\n",
       "      <td>-0.343028</td>\n",
       "      <td>0.275461</td>\n",
       "      <td>-0.132740</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>-0.206416</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.009479</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.093692</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Max</th>\n",
       "      <td>0.749933</td>\n",
       "      <td>-0.343028</td>\n",
       "      <td>0.275461</td>\n",
       "      <td>-0.132740</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>-0.206416</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.009479</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.093692</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SrcPkts</th>\n",
       "      <td>0.005195</td>\n",
       "      <td>-0.002817</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>-0.002958</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002166</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DstPkts</th>\n",
       "      <td>0.006871</td>\n",
       "      <td>-0.003475</td>\n",
       "      <td>0.003158</td>\n",
       "      <td>-0.003475</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>-0.002356</td>\n",
       "      <td>0.009479</td>\n",
       "      <td>0.009479</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009479</td>\n",
       "      <td>0.009479</td>\n",
       "      <td>0.009479</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.986465</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.997609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SrcBytes</th>\n",
       "      <td>0.005195</td>\n",
       "      <td>-0.002817</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>-0.002958</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002166</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DstBytes</th>\n",
       "      <td>0.088315</td>\n",
       "      <td>-0.034503</td>\n",
       "      <td>0.026635</td>\n",
       "      <td>-0.034860</td>\n",
       "      <td>0.985948</td>\n",
       "      <td>0.985948</td>\n",
       "      <td>-0.025324</td>\n",
       "      <td>0.093692</td>\n",
       "      <td>0.093692</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.093692</td>\n",
       "      <td>0.093692</td>\n",
       "      <td>0.093692</td>\n",
       "      <td>0.985948</td>\n",
       "      <td>0.986465</td>\n",
       "      <td>0.985948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985948</td>\n",
       "      <td>0.985948</td>\n",
       "      <td>0.984042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rate</th>\n",
       "      <td>0.005195</td>\n",
       "      <td>-0.002817</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>-0.002958</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002166</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SrcRate</th>\n",
       "      <td>0.005195</td>\n",
       "      <td>-0.002817</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>-0.002958</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002166</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DstRate</th>\n",
       "      <td>0.006202</td>\n",
       "      <td>-0.005118</td>\n",
       "      <td>0.004627</td>\n",
       "      <td>-0.004476</td>\n",
       "      <td>0.997595</td>\n",
       "      <td>0.997595</td>\n",
       "      <td>0.003815</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.997595</td>\n",
       "      <td>0.997609</td>\n",
       "      <td>0.997595</td>\n",
       "      <td>0.984042</td>\n",
       "      <td>0.997595</td>\n",
       "      <td>0.997595</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <td>0.135355</td>\n",
       "      <td>-0.078528</td>\n",
       "      <td>0.023421</td>\n",
       "      <td>-0.028573</td>\n",
       "      <td>-0.000479</td>\n",
       "      <td>-0.000479</td>\n",
       "      <td>0.070918</td>\n",
       "      <td>0.060837</td>\n",
       "      <td>0.060837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.060837</td>\n",
       "      <td>0.060837</td>\n",
       "      <td>0.060837</td>\n",
       "      <td>-0.000479</td>\n",
       "      <td>-0.000443</td>\n",
       "      <td>-0.000479</td>\n",
       "      <td>0.001834</td>\n",
       "      <td>-0.000479</td>\n",
       "      <td>-0.000479</td>\n",
       "      <td>-0.000528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ATT&amp;CK_Tactic</th>\n",
       "      <td>0.112613</td>\n",
       "      <td>-0.068235</td>\n",
       "      <td>0.018976</td>\n",
       "      <td>-0.027912</td>\n",
       "      <td>-0.000453</td>\n",
       "      <td>-0.000453</td>\n",
       "      <td>0.057395</td>\n",
       "      <td>0.062707</td>\n",
       "      <td>0.062706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062706</td>\n",
       "      <td>0.062706</td>\n",
       "      <td>0.062706</td>\n",
       "      <td>-0.000453</td>\n",
       "      <td>-0.000420</td>\n",
       "      <td>-0.000453</td>\n",
       "      <td>0.002054</td>\n",
       "      <td>-0.000453</td>\n",
       "      <td>-0.000453</td>\n",
       "      <td>-0.000466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ATT&amp;CK_Technique</th>\n",
       "      <td>0.112384</td>\n",
       "      <td>-0.068283</td>\n",
       "      <td>0.019303</td>\n",
       "      <td>-0.027953</td>\n",
       "      <td>-0.000453</td>\n",
       "      <td>-0.000453</td>\n",
       "      <td>0.057132</td>\n",
       "      <td>0.062387</td>\n",
       "      <td>0.062387</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062387</td>\n",
       "      <td>0.062387</td>\n",
       "      <td>0.062387</td>\n",
       "      <td>-0.000453</td>\n",
       "      <td>-0.000420</td>\n",
       "      <td>-0.000453</td>\n",
       "      <td>0.002028</td>\n",
       "      <td>-0.000453</td>\n",
       "      <td>-0.000453</td>\n",
       "      <td>-0.000468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Flgs     Proto     Sport     Dport   TotPkts  TotBytes  \\\n",
       "Flgs              1.000000 -0.337580  0.208319 -0.089157  0.005195  0.005195   \n",
       "Proto            -0.337580  1.000000  0.464880 -0.644498 -0.002817 -0.002817   \n",
       "Sport             0.208319  0.464880  1.000000 -0.770285  0.002783  0.002783   \n",
       "Dport            -0.089157 -0.644498 -0.770285  1.000000 -0.002958 -0.002958   \n",
       "TotPkts           0.005195 -0.002817  0.002783 -0.002958  1.000000  1.000000   \n",
       "TotBytes          0.005195 -0.002817  0.002783 -0.002958  1.000000  1.000000   \n",
       "State            -0.141425 -0.447305 -0.246322  0.296034 -0.002166 -0.002166   \n",
       "Dur               0.749933 -0.343025  0.275464 -0.132743  0.007724  0.007724   \n",
       "Mean              0.749933 -0.343028  0.275461 -0.132740  0.007724  0.007724   \n",
       "StdDev                 NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "Sum               0.749933 -0.343028  0.275461 -0.132740  0.007724  0.007724   \n",
       "Min               0.749933 -0.343028  0.275461 -0.132740  0.007724  0.007724   \n",
       "Max               0.749933 -0.343028  0.275461 -0.132740  0.007724  0.007724   \n",
       "SrcPkts           0.005195 -0.002817  0.002783 -0.002958  1.000000  1.000000   \n",
       "DstPkts           0.006871 -0.003475  0.003158 -0.003475  0.999987  0.999987   \n",
       "SrcBytes          0.005195 -0.002817  0.002783 -0.002958  1.000000  1.000000   \n",
       "DstBytes          0.088315 -0.034503  0.026635 -0.034860  0.985948  0.985948   \n",
       "Rate              0.005195 -0.002817  0.002783 -0.002958  1.000000  1.000000   \n",
       "SrcRate           0.005195 -0.002817  0.002783 -0.002958  1.000000  1.000000   \n",
       "DstRate           0.006202 -0.005118  0.004627 -0.004476  0.997595  0.997595   \n",
       "Label             0.135355 -0.078528  0.023421 -0.028573 -0.000479 -0.000479   \n",
       "ATT&CK_Tactic     0.112613 -0.068235  0.018976 -0.027912 -0.000453 -0.000453   \n",
       "ATT&CK_Technique  0.112384 -0.068283  0.019303 -0.027953 -0.000453 -0.000453   \n",
       "\n",
       "                     State       Dur      Mean  StdDev       Sum       Min  \\\n",
       "Flgs             -0.141425  0.749933  0.749933     NaN  0.749933  0.749933   \n",
       "Proto            -0.447305 -0.343025 -0.343028     NaN -0.343028 -0.343028   \n",
       "Sport            -0.246322  0.275464  0.275461     NaN  0.275461  0.275461   \n",
       "Dport             0.296034 -0.132743 -0.132740     NaN -0.132740 -0.132740   \n",
       "TotPkts          -0.002166  0.007724  0.007724     NaN  0.007724  0.007724   \n",
       "TotBytes         -0.002166  0.007724  0.007724     NaN  0.007724  0.007724   \n",
       "State             1.000000 -0.206417 -0.206416     NaN -0.206416 -0.206416   \n",
       "Dur              -0.206417  1.000000  1.000000     NaN  1.000000  1.000000   \n",
       "Mean             -0.206416  1.000000  1.000000     NaN  1.000000  1.000000   \n",
       "StdDev                 NaN       NaN       NaN     NaN       NaN       NaN   \n",
       "Sum              -0.206416  1.000000  1.000000     NaN  1.000000  1.000000   \n",
       "Min              -0.206416  1.000000  1.000000     NaN  1.000000  1.000000   \n",
       "Max              -0.206416  1.000000  1.000000     NaN  1.000000  1.000000   \n",
       "SrcPkts          -0.002166  0.007724  0.007724     NaN  0.007724  0.007724   \n",
       "DstPkts          -0.002356  0.009479  0.009479     NaN  0.009479  0.009479   \n",
       "SrcBytes         -0.002166  0.007724  0.007724     NaN  0.007724  0.007724   \n",
       "DstBytes         -0.025324  0.093692  0.093692     NaN  0.093692  0.093692   \n",
       "Rate             -0.002166  0.007724  0.007724     NaN  0.007724  0.007724   \n",
       "SrcRate          -0.002166  0.007724  0.007724     NaN  0.007724  0.007724   \n",
       "DstRate           0.003815  0.008475  0.008475     NaN  0.008475  0.008475   \n",
       "Label             0.070918  0.060837  0.060837     NaN  0.060837  0.060837   \n",
       "ATT&CK_Tactic     0.057395  0.062707  0.062706     NaN  0.062706  0.062706   \n",
       "ATT&CK_Technique  0.057132  0.062387  0.062387     NaN  0.062387  0.062387   \n",
       "\n",
       "                       Max   SrcPkts   DstPkts  SrcBytes  DstBytes      Rate  \\\n",
       "Flgs              0.749933  0.005195  0.006871  0.005195  0.088315  0.005195   \n",
       "Proto            -0.343028 -0.002817 -0.003475 -0.002817 -0.034503 -0.002817   \n",
       "Sport             0.275461  0.002783  0.003158  0.002783  0.026635  0.002783   \n",
       "Dport            -0.132740 -0.002958 -0.003475 -0.002958 -0.034860 -0.002958   \n",
       "TotPkts           0.007724  1.000000  0.999987  1.000000  0.985948  1.000000   \n",
       "TotBytes          0.007724  1.000000  0.999987  1.000000  0.985948  1.000000   \n",
       "State            -0.206416 -0.002166 -0.002356 -0.002166 -0.025324 -0.002166   \n",
       "Dur               1.000000  0.007724  0.009479  0.007724  0.093692  0.007724   \n",
       "Mean              1.000000  0.007724  0.009479  0.007724  0.093692  0.007724   \n",
       "StdDev                 NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "Sum               1.000000  0.007724  0.009479  0.007724  0.093692  0.007724   \n",
       "Min               1.000000  0.007724  0.009479  0.007724  0.093692  0.007724   \n",
       "Max               1.000000  0.007724  0.009479  0.007724  0.093692  0.007724   \n",
       "SrcPkts           0.007724  1.000000  0.999987  1.000000  0.985948  1.000000   \n",
       "DstPkts           0.009479  0.999987  1.000000  0.999987  0.986465  0.999987   \n",
       "SrcBytes          0.007724  1.000000  0.999987  1.000000  0.985948  1.000000   \n",
       "DstBytes          0.093692  0.985948  0.986465  0.985948  1.000000  0.985948   \n",
       "Rate              0.007724  1.000000  0.999987  1.000000  0.985948  1.000000   \n",
       "SrcRate           0.007724  1.000000  0.999987  1.000000  0.985948  1.000000   \n",
       "DstRate           0.008475  0.997595  0.997609  0.997595  0.984042  0.997595   \n",
       "Label             0.060837 -0.000479 -0.000443 -0.000479  0.001834 -0.000479   \n",
       "ATT&CK_Tactic     0.062706 -0.000453 -0.000420 -0.000453  0.002054 -0.000453   \n",
       "ATT&CK_Technique  0.062387 -0.000453 -0.000420 -0.000453  0.002028 -0.000453   \n",
       "\n",
       "                   SrcRate   DstRate  \n",
       "Flgs              0.005195  0.006202  \n",
       "Proto            -0.002817 -0.005118  \n",
       "Sport             0.002783  0.004627  \n",
       "Dport            -0.002958 -0.004476  \n",
       "TotPkts           1.000000  0.997595  \n",
       "TotBytes          1.000000  0.997595  \n",
       "State            -0.002166  0.003815  \n",
       "Dur               0.007724  0.008475  \n",
       "Mean              0.007724  0.008475  \n",
       "StdDev                 NaN       NaN  \n",
       "Sum               0.007724  0.008475  \n",
       "Min               0.007724  0.008475  \n",
       "Max               0.007724  0.008475  \n",
       "SrcPkts           1.000000  0.997595  \n",
       "DstPkts           0.999987  0.997609  \n",
       "SrcBytes          1.000000  0.997595  \n",
       "DstBytes          0.985948  0.984042  \n",
       "Rate              1.000000  0.997595  \n",
       "SrcRate           1.000000  0.997595  \n",
       "DstRate           0.997595  1.000000  \n",
       "Label            -0.000479 -0.000528  \n",
       "ATT&CK_Tactic    -0.000453 -0.000466  \n",
       "ATT&CK_Technique -0.000453 -0.000468  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df.corr()\n",
    "test[test.columns[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flgs</th>\n",
       "      <th>Proto</th>\n",
       "      <th>Sport</th>\n",
       "      <th>Dport</th>\n",
       "      <th>TotPkts</th>\n",
       "      <th>TotBytes</th>\n",
       "      <th>State</th>\n",
       "      <th>Dur</th>\n",
       "      <th>Mean</th>\n",
       "      <th>StdDev</th>\n",
       "      <th>...</th>\n",
       "      <th>SrcPkts</th>\n",
       "      <th>DstPkts</th>\n",
       "      <th>SrcBytes</th>\n",
       "      <th>DstBytes</th>\n",
       "      <th>Rate</th>\n",
       "      <th>SrcRate</th>\n",
       "      <th>DstRate</th>\n",
       "      <th>Label</th>\n",
       "      <th>ATT&amp;CK_Tactic</th>\n",
       "      <th>ATT&amp;CK_Technique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12.710654</td>\n",
       "      <td>0.016221</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>1.986258e-15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022439</td>\n",
       "      <td>0.022439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>3.725290e-08</td>\n",
       "      <td>8.760354e-16</td>\n",
       "      <td>4.237518e-07</td>\n",
       "      <td>3.435925e-15</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.496384</td>\n",
       "      <td>0.016221</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>1.656661e-15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065042</td>\n",
       "      <td>0.065042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>3.725290e-08</td>\n",
       "      <td>7.546047e-16</td>\n",
       "      <td>3.678724e-07</td>\n",
       "      <td>1.185374e-15</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.908078</td>\n",
       "      <td>0.016221</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>1.543904e-15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>3.725290e-08</td>\n",
       "      <td>7.546047e-16</td>\n",
       "      <td>3.376044e-07</td>\n",
       "      <td>8.410770e-13</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.142857</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.485183</td>\n",
       "      <td>2.040154</td>\n",
       "      <td>1.927470e-17</td>\n",
       "      <td>2.775558e-15</td>\n",
       "      <td>14.468085</td>\n",
       "      <td>3.692763</td>\n",
       "      <td>3.692763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.156482e-17</td>\n",
       "      <td>1.117587e-07</td>\n",
       "      <td>1.682682e-15</td>\n",
       "      <td>4.190952e-07</td>\n",
       "      <td>1.043916e-16</td>\n",
       "      <td>4.175659e-17</td>\n",
       "      <td>4.035223e-07</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.636921</td>\n",
       "      <td>0.016221</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>1.812786e-15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015925</td>\n",
       "      <td>0.015925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>3.725290e-08</td>\n",
       "      <td>7.459311e-16</td>\n",
       "      <td>4.121102e-07</td>\n",
       "      <td>4.841364e-15</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85303</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.041200</td>\n",
       "      <td>15.944176</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.204170e-17</td>\n",
       "      <td>4.680851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.204170e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85304</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.041200</td>\n",
       "      <td>15.943258</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.204170e-17</td>\n",
       "      <td>4.680851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.204170e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85305</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>18.356578</td>\n",
       "      <td>0.016221</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>1.309716e-15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>3.725290e-08</td>\n",
       "      <td>7.459311e-16</td>\n",
       "      <td>2.770685e-07</td>\n",
       "      <td>8.630454e-13</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85306</th>\n",
       "      <td>11.428571</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.066073</td>\n",
       "      <td>15.044072</td>\n",
       "      <td>1.349229e-16</td>\n",
       "      <td>6.253678e-14</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>17.895837</td>\n",
       "      <td>17.895837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.095374e-17</td>\n",
       "      <td>5.587935e-07</td>\n",
       "      <td>3.429548e-14</td>\n",
       "      <td>7.706694e-06</td>\n",
       "      <td>1.507868e-16</td>\n",
       "      <td>8.616383e-17</td>\n",
       "      <td>5.828613e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85307</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>16.694845</td>\n",
       "      <td>0.016221</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>1.318390e-15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010106</td>\n",
       "      <td>0.010106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.854940e-18</td>\n",
       "      <td>3.725290e-08</td>\n",
       "      <td>8.239936e-16</td>\n",
       "      <td>2.584420e-07</td>\n",
       "      <td>7.629256e-15</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85168 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Flgs  Proto      Sport      Dport       TotPkts      TotBytes  \\\n",
       "0       0.000000   20.0  12.710654   0.016221  3.854940e-18  1.986258e-15   \n",
       "1       0.000000   20.0  15.496384   0.016221  3.854940e-18  1.656661e-15   \n",
       "2       0.000000   20.0  15.908078   0.016221  3.854940e-18  1.543904e-15   \n",
       "3      17.142857   10.0  16.485183   2.040154  1.927470e-17  2.775558e-15   \n",
       "4       0.000000   20.0  10.636921   0.016221  3.854940e-18  1.812786e-15   \n",
       "...          ...    ...        ...        ...           ...           ...   \n",
       "85303   0.000000   10.0   0.041200  15.944176  0.000000e+00  5.204170e-17   \n",
       "85304   0.000000   10.0   0.041200  15.943258  0.000000e+00  5.204170e-17   \n",
       "85305   0.000000   20.0  18.356578   0.016221  3.854940e-18  1.309716e-15   \n",
       "85306  11.428571   10.0  15.066073  15.044072  1.349229e-16  6.253678e-14   \n",
       "85307   0.000000   20.0  16.694845   0.016221  3.854940e-18  1.318390e-15   \n",
       "\n",
       "           State        Dur       Mean  StdDev  ...       SrcPkts  \\\n",
       "0       0.000000   0.022439   0.022439     0.0  ...  3.854940e-18   \n",
       "1       0.000000   0.065042   0.065042     0.0  ...  3.854940e-18   \n",
       "2       0.000000   0.000092   0.000092     0.0  ...  3.854940e-18   \n",
       "3      14.468085   3.692763   3.692763     0.0  ...  1.156482e-17   \n",
       "4       0.000000   0.015925   0.015925     0.0  ...  3.854940e-18   \n",
       "...          ...        ...        ...     ...  ...           ...   \n",
       "85303   4.680851   0.000000   0.000054     0.0  ...  3.854940e-18   \n",
       "85304   4.680851   0.000000   0.000024     0.0  ...  3.854940e-18   \n",
       "85305   0.000000   0.000089   0.000089     0.0  ...  3.854940e-18   \n",
       "85306   0.425532  17.895837  17.895837     0.0  ...  8.095374e-17   \n",
       "85307   0.000000   0.010106   0.010106     0.0  ...  3.854940e-18   \n",
       "\n",
       "            DstPkts      SrcBytes      DstBytes          Rate       SrcRate  \\\n",
       "0      3.725290e-08  8.760354e-16  4.237518e-07  3.435925e-15  0.000000e+00   \n",
       "1      3.725290e-08  7.546047e-16  3.678724e-07  1.185374e-15  0.000000e+00   \n",
       "2      3.725290e-08  7.546047e-16  3.376044e-07  8.410770e-13  0.000000e+00   \n",
       "3      1.117587e-07  1.682682e-15  4.190952e-07  1.043916e-16  4.175659e-17   \n",
       "4      3.725290e-08  7.459311e-16  4.121102e-07  4.841364e-15  0.000000e+00   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "85303  0.000000e+00  5.204170e-16  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "85304  0.000000e+00  5.204170e-16  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "85305  3.725290e-08  7.459311e-16  2.770685e-07  8.630454e-13  0.000000e+00   \n",
       "85306  5.587935e-07  3.429548e-14  7.706694e-06  1.507868e-16  8.616383e-17   \n",
       "85307  3.725290e-08  8.239936e-16  2.584420e-07  7.629256e-15  0.000000e+00   \n",
       "\n",
       "            DstRate  Label  ATT&CK_Tactic  ATT&CK_Technique  \n",
       "0      0.000000e+00      0              0                 0  \n",
       "1      0.000000e+00      0              0                 0  \n",
       "2      0.000000e+00      0              0                 0  \n",
       "3      4.035223e-07      1              2                 2  \n",
       "4      0.000000e+00      0              0                 0  \n",
       "...             ...    ...            ...               ...  \n",
       "85303  0.000000e+00      0              0                 0  \n",
       "85304  0.000000e+00      0              0                 0  \n",
       "85305  0.000000e+00      0              0                 0  \n",
       "85306  5.828613e-07      0              0                 0  \n",
       "85307  0.000000e+00      0              0                 0  \n",
       "\n",
       "[85168 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\n",
      "Normal: 83538\n",
      "Abnormal: 1630\n",
      "after:\n",
      "Normal: 55637\n",
      "Abnormal: 1607\n",
      "            Flgs  Proto      Sport      Dport       TotPkts      TotBytes  \\\n",
      "0       0.000000   20.0  12.710654   0.016221  3.854940e-18  1.986258e-15   \n",
      "1       0.000000   20.0  15.496384   0.016221  3.854940e-18  1.656661e-15   \n",
      "2       0.000000   20.0  15.908078   0.016221  3.854940e-18  1.543904e-15   \n",
      "3      17.142857   10.0  16.485183   2.040154  1.927470e-17  2.775558e-15   \n",
      "4       0.000000   20.0  10.636921   0.016221  3.854940e-18  1.812786e-15   \n",
      "...          ...    ...        ...        ...           ...           ...   \n",
      "85303   0.000000   10.0   0.041200  15.944176  0.000000e+00  5.204170e-17   \n",
      "85304   0.000000   10.0   0.041200  15.943258  0.000000e+00  5.204170e-17   \n",
      "85305   0.000000   20.0  18.356578   0.016221  3.854940e-18  1.309716e-15   \n",
      "85306  11.428571   10.0  15.066073  15.044072  1.349229e-16  6.253678e-14   \n",
      "85307   0.000000   20.0  16.694845   0.016221  3.854940e-18  1.318390e-15   \n",
      "\n",
      "           State        Dur       Mean  StdDev        Sum        Min  \\\n",
      "0       0.000000   0.022439   0.022439     0.0   0.022439   0.022439   \n",
      "1       0.000000   0.065042   0.065042     0.0   0.065042   0.065042   \n",
      "2       0.000000   0.000092   0.000092     0.0   0.000092   0.000092   \n",
      "3      14.468085   3.692763   3.692763     0.0   3.692763   3.692763   \n",
      "4       0.000000   0.015925   0.015925     0.0   0.015925   0.015925   \n",
      "...          ...        ...        ...     ...        ...        ...   \n",
      "85303   4.680851   0.000000   0.000054     0.0   0.000054   0.000054   \n",
      "85304   4.680851   0.000000   0.000024     0.0   0.000024   0.000024   \n",
      "85305   0.000000   0.000089   0.000089     0.0   0.000089   0.000089   \n",
      "85306   0.425532  17.895837  17.895837     0.0  17.895837  17.895837   \n",
      "85307   0.000000   0.010106   0.010106     0.0   0.010106   0.010106   \n",
      "\n",
      "             Max       SrcPkts       DstPkts      SrcBytes      DstBytes  \\\n",
      "0       0.022439  3.854940e-18  3.725290e-08  8.760354e-16  4.237518e-07   \n",
      "1       0.065042  3.854940e-18  3.725290e-08  7.546047e-16  3.678724e-07   \n",
      "2       0.000092  3.854940e-18  3.725290e-08  7.546047e-16  3.376044e-07   \n",
      "3       3.692763  1.156482e-17  1.117587e-07  1.682682e-15  4.190952e-07   \n",
      "4       0.015925  3.854940e-18  3.725290e-08  7.459311e-16  4.121102e-07   \n",
      "...          ...           ...           ...           ...           ...   \n",
      "85303   0.000054  3.854940e-18  0.000000e+00  5.204170e-16  0.000000e+00   \n",
      "85304   0.000024  3.854940e-18  0.000000e+00  5.204170e-16  0.000000e+00   \n",
      "85305   0.000089  3.854940e-18  3.725290e-08  7.459311e-16  2.770685e-07   \n",
      "85306  17.895837  8.095374e-17  5.587935e-07  3.429548e-14  7.706694e-06   \n",
      "85307   0.010106  3.854940e-18  3.725290e-08  8.239936e-16  2.584420e-07   \n",
      "\n",
      "               Rate       SrcRate       DstRate  \n",
      "0      3.435925e-15  0.000000e+00  0.000000e+00  \n",
      "1      1.185374e-15  0.000000e+00  0.000000e+00  \n",
      "2      8.410770e-13  0.000000e+00  0.000000e+00  \n",
      "3      1.043916e-16  4.175659e-17  4.035223e-07  \n",
      "4      4.841364e-15  0.000000e+00  0.000000e+00  \n",
      "...             ...           ...           ...  \n",
      "85303  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "85304  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "85305  8.630454e-13  0.000000e+00  0.000000e+00  \n",
      "85306  1.507868e-16  8.616383e-17  5.828613e-07  \n",
      "85307  7.629256e-15  0.000000e+00  0.000000e+00  \n",
      "\n",
      "[85168 rows x 20 columns]\n",
      "       Label  ATT&CK_Tactic  ATT&CK_Technique\n",
      "0          0              0                 0\n",
      "1          0              0                 0\n",
      "2          0              0                 0\n",
      "3          1              2                 2\n",
      "4          0              0                 0\n",
      "...      ...            ...               ...\n",
      "85303      0              0                 0\n",
      "85304      0              0                 0\n",
      "85305      0              0                 0\n",
      "85306      0              0                 0\n",
      "85307      0              0                 0\n",
      "\n",
      "[85168 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "unique_df = df.drop_duplicates(subset=list(df)[:-3])\n",
    "print(\"before:\")\n",
    "print(\"Normal:\", len(df[df['Label'] == 0]))\n",
    "print(\"Abnormal:\", len(df[df['Label'] == 1]))\n",
    "print(\"after:\")\n",
    "print(\"Normal:\", len(unique_df[unique_df['Label'] == 0]))\n",
    "print(\"Abnormal:\", len(unique_df[unique_df['Label'] == 1]))\n",
    "\n",
    "unique_abnormal_df = unique_df[unique_df['Label'] == 1].iloc[:, :-3]\n",
    "# print(unique_abnormal_df)\n",
    "\n",
    "X, y = df.iloc[:, :-3], df.iloc[:, -3:]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "X_test_copy = X_test.copy()\n",
    "y_test_copy = y_test.copy()\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Normal:Abnormal = 50.34438583270535\n",
      "Test: Normal:Abnormal = 55.21782178217822\n"
     ]
    }
   ],
   "source": [
    "train_normal_cases = len(y_train[y_train['Label'] == 0])\n",
    "train_abnormal_cases = len(y_train[y_train['Label'] == 1])\n",
    "test_normal_cases = len(y_test[y_test['Label'] == 0])\n",
    "test_abnormal_cases = len(y_test[y_test['Label'] == 1])\n",
    "\n",
    "print(\"Train: Normal:Abnormal = {}\".format(train_normal_cases/train_abnormal_cases))\n",
    "print(\"Test: Normal:Abnormal = {}\".format(test_normal_cases/test_abnormal_cases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "original_dim = X.shape[1]\n",
    "print(original_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    '''Uses (mean, logvar) to sample z'''\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        mean, logvar = inputs\n",
    "        \n",
    "        latent_size = tf.shape(mean)\n",
    "        sample_z = tf.keras.backend.random_normal(shape=latent_size)\n",
    "        z = sample_z * tf.exp(logvar/2) + mean\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          2688        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           8256        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           1040        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 16)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4)            68          leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 4)            0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 10)           50          leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "logvar (Dense)                  (None, 10)           50          leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sampling (Sampling)             (None, 10)           0           mean[0][0]                       \n",
      "                                                                 logvar[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 12,152\n",
      "Trainable params: 12,152\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (original_dim,)\n",
    "latent_dim = 10\n",
    "\n",
    "encoder_inputs = keras.Input(shape=input_shape)\n",
    "x = layers.Dense(128)(encoder_inputs)\n",
    "x = layers.LeakyReLU()(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(64)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(16)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(4)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "mean = layers.Dense(latent_dim, name=\"mean\")(x)\n",
    "logvar = layers.Dense(latent_dim, name=\"logvar\")(x)\n",
    "z = Sampling()([mean, logvar])\n",
    "encoder = keras.Model(encoder_inputs, [mean, logvar, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                176       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 20)                2580      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 12,164\n",
      "Trainable params: 12,164\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(16)(latent_inputs)\n",
    "x = layers.LeakyReLU()(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(64)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(128)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(original_dim)(x)\n",
    "decoder_outputs = layers.LeakyReLU()(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            mean, logvar, z = encoder(data)\n",
    "            reconstruction = decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                keras.losses.mse(data, reconstruction)\n",
    "            )\n",
    "            reconstruction_loss *= original_dim\n",
    "            kl_loss = 1 + logvar - tf.square(mean) - tf.exp(logvar)\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "            kl_loss *= -0.5\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss\n",
    "        }\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        mean, logvar, z = encoder(data)\n",
    "        reconstruction = decoder(z)\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            keras.losses.mse(data, reconstruction)\n",
    "        )\n",
    "        reconstruction_loss *= original_dim\n",
    "        kl_loss = 1 + logvar - tf.square(mean) - tf.exp(logvar)\n",
    "        kl_loss = tf.reduce_mean(kl_loss)            \n",
    "        kl_loss *= -0.5\n",
    "        total_loss = reconstruction_loss + kl_loss\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1567.9969 - reconstruction_loss: 1383.1852 - kl_loss: 184.8117\n",
      "Epoch 2/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 826.3791 - reconstruction_loss: 789.4030 - kl_loss: 36.9762\n",
      "Epoch 3/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 570.1824 - reconstruction_loss: 553.1167 - kl_loss: 17.0658\n",
      "Epoch 4/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 397.8383 - reconstruction_loss: 391.5487 - kl_loss: 6.2896\n",
      "Epoch 5/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 355.3435 - reconstruction_loss: 351.2589 - kl_loss: 4.0846\n",
      "Epoch 6/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 328.0382 - reconstruction_loss: 324.9211 - kl_loss: 3.1171\n",
      "Epoch 7/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 296.4652 - reconstruction_loss: 293.6512 - kl_loss: 2.8140\n",
      "Epoch 8/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 260.1290 - reconstruction_loss: 257.4120 - kl_loss: 2.7170\n",
      "Epoch 9/1000\n",
      "4/4 [==============================] - 0s 758us/step - loss: 216.5697 - reconstruction_loss: 213.4640 - kl_loss: 3.1057\n",
      "Epoch 10/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 177.4407 - reconstruction_loss: 174.1875 - kl_loss: 3.2532\n",
      "Epoch 11/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 148.1800 - reconstruction_loss: 144.8843 - kl_loss: 3.2957\n",
      "Epoch 12/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 135.9534 - reconstruction_loss: 132.3158 - kl_loss: 3.6376\n",
      "Epoch 13/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 118.7697 - reconstruction_loss: 114.7460 - kl_loss: 4.0237\n",
      "Epoch 14/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 108.9394 - reconstruction_loss: 105.2586 - kl_loss: 3.6808\n",
      "Epoch 15/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 103.0619 - reconstruction_loss: 99.5048 - kl_loss: 3.5571\n",
      "Epoch 16/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 102.5769 - reconstruction_loss: 98.7314 - kl_loss: 3.8455\n",
      "Epoch 17/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 97.2077 - reconstruction_loss: 93.4519 - kl_loss: 3.7558\n",
      "Epoch 18/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 93.7860 - reconstruction_loss: 89.8529 - kl_loss: 3.9331\n",
      "Epoch 19/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 92.5923 - reconstruction_loss: 88.6087 - kl_loss: 3.9835\n",
      "Epoch 20/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 96.1933 - reconstruction_loss: 91.9832 - kl_loss: 4.2101\n",
      "Epoch 21/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 85.6464 - reconstruction_loss: 81.4121 - kl_loss: 4.2343\n",
      "Epoch 22/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 79.2684 - reconstruction_loss: 74.7414 - kl_loss: 4.5271\n",
      "Epoch 23/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 78.9120 - reconstruction_loss: 74.2696 - kl_loss: 4.6424\n",
      "Epoch 24/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 73.7329 - reconstruction_loss: 68.8630 - kl_loss: 4.8699\n",
      "Epoch 25/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 68.6935 - reconstruction_loss: 63.8166 - kl_loss: 4.8769\n",
      "Epoch 26/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 71.1245 - reconstruction_loss: 66.0101 - kl_loss: 5.1145\n",
      "Epoch 27/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 66.2389 - reconstruction_loss: 60.9993 - kl_loss: 5.2396\n",
      "Epoch 28/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 60.8654 - reconstruction_loss: 55.8713 - kl_loss: 4.9941\n",
      "Epoch 29/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 62.5609 - reconstruction_loss: 57.5017 - kl_loss: 5.0592\n",
      "Epoch 30/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 67.4090 - reconstruction_loss: 62.4740 - kl_loss: 4.9350\n",
      "Epoch 31/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 61.3614 - reconstruction_loss: 56.3984 - kl_loss: 4.9630\n",
      "Epoch 32/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 58.3792 - reconstruction_loss: 53.3544 - kl_loss: 5.0248\n",
      "Epoch 33/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 60.1368 - reconstruction_loss: 55.2128 - kl_loss: 4.9240\n",
      "Epoch 34/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 55.1585 - reconstruction_loss: 50.1831 - kl_loss: 4.9754\n",
      "Epoch 35/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 52.1818 - reconstruction_loss: 47.1830 - kl_loss: 4.9988\n",
      "Epoch 36/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 50.0262 - reconstruction_loss: 44.9847 - kl_loss: 5.0415\n",
      "Epoch 37/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 52.9716 - reconstruction_loss: 47.7845 - kl_loss: 5.1872\n",
      "Epoch 38/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 47.3229 - reconstruction_loss: 42.3340 - kl_loss: 4.9889\n",
      "Epoch 39/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 46.5005 - reconstruction_loss: 41.4859 - kl_loss: 5.0147\n",
      "Epoch 40/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 48.3251 - reconstruction_loss: 43.3343 - kl_loss: 4.9909\n",
      "Epoch 41/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 41.6146 - reconstruction_loss: 36.3338 - kl_loss: 5.2808\n",
      "Epoch 42/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 37.7269 - reconstruction_loss: 32.5960 - kl_loss: 5.1309\n",
      "Epoch 43/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 34.6967 - reconstruction_loss: 29.3838 - kl_loss: 5.3129\n",
      "Epoch 44/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 34.2746 - reconstruction_loss: 29.1360 - kl_loss: 5.1386\n",
      "Epoch 45/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 29.2206 - reconstruction_loss: 23.5338 - kl_loss: 5.6868\n",
      "Epoch 46/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 29.9044 - reconstruction_loss: 24.4215 - kl_loss: 5.4829\n",
      "Epoch 47/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 27.8635 - reconstruction_loss: 22.3417 - kl_loss: 5.5218\n",
      "Epoch 48/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 28.5420 - reconstruction_loss: 22.9355 - kl_loss: 5.6065\n",
      "Epoch 49/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 24.6430 - reconstruction_loss: 19.3003 - kl_loss: 5.3427\n",
      "Epoch 50/1000\n",
      "4/4 [==============================] - 0s 758us/step - loss: 24.7010 - reconstruction_loss: 19.6783 - kl_loss: 5.0227\n",
      "Epoch 51/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 24.5915 - reconstruction_loss: 19.4870 - kl_loss: 5.1046\n",
      "Epoch 52/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 21.9844 - reconstruction_loss: 16.9905 - kl_loss: 4.9939\n",
      "Epoch 53/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 25.4237 - reconstruction_loss: 20.1845 - kl_loss: 5.2392\n",
      "Epoch 54/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 21.2726 - reconstruction_loss: 16.3068 - kl_loss: 4.9658\n",
      "Epoch 55/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 20.8874 - reconstruction_loss: 15.8723 - kl_loss: 5.0151\n",
      "Epoch 56/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 21.6432 - reconstruction_loss: 16.6814 - kl_loss: 4.9618\n",
      "Epoch 57/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 20.2711 - reconstruction_loss: 15.3671 - kl_loss: 4.9040\n",
      "Epoch 58/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 19.2452 - reconstruction_loss: 14.6010 - kl_loss: 4.6442\n",
      "Epoch 59/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 18.6594 - reconstruction_loss: 13.9589 - kl_loss: 4.7005\n",
      "Epoch 60/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.7109 - reconstruction_loss: 13.0480 - kl_loss: 4.6629\n",
      "Epoch 61/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.6160 - reconstruction_loss: 13.7997 - kl_loss: 4.8163\n",
      "Epoch 62/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 15.7155 - reconstruction_loss: 11.0690 - kl_loss: 4.6465\n",
      "Epoch 63/1000\n",
      "4/4 [==============================] - 0s 512us/step - loss: 15.6610 - reconstruction_loss: 11.1551 - kl_loss: 4.5059\n",
      "Epoch 64/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 17.1522 - reconstruction_loss: 12.5710 - kl_loss: 4.5812\n",
      "Epoch 65/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 14.3332 - reconstruction_loss: 9.8028 - kl_loss: 4.5304\n",
      "Epoch 66/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 16.0295 - reconstruction_loss: 11.6689 - kl_loss: 4.3606\n",
      "Epoch 67/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 12.7496 - reconstruction_loss: 7.9947 - kl_loss: 4.7549\n",
      "Epoch 68/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 13.6715 - reconstruction_loss: 9.2705 - kl_loss: 4.4011\n",
      "Epoch 69/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 12.4508 - reconstruction_loss: 7.9076 - kl_loss: 4.5433\n",
      "Epoch 70/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 12.3441 - reconstruction_loss: 7.7810 - kl_loss: 4.5631\n",
      "Epoch 71/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 12.2474 - reconstruction_loss: 7.8705 - kl_loss: 4.3769\n",
      "Epoch 72/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 12.3564 - reconstruction_loss: 7.8794 - kl_loss: 4.4770\n",
      "Epoch 73/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 12.7685 - reconstruction_loss: 8.2417 - kl_loss: 4.5268\n",
      "Epoch 74/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 12.6784 - reconstruction_loss: 8.3079 - kl_loss: 4.3705\n",
      "Epoch 75/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 11.5347 - reconstruction_loss: 7.1617 - kl_loss: 4.3730\n",
      "Epoch 76/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.5603 - reconstruction_loss: 6.3275 - kl_loss: 4.2328\n",
      "Epoch 77/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 10.0261 - reconstruction_loss: 5.8646 - kl_loss: 4.1615\n",
      "Epoch 78/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 10.6228 - reconstruction_loss: 6.5339 - kl_loss: 4.0889\n",
      "Epoch 79/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 10.7955 - reconstruction_loss: 6.7128 - kl_loss: 4.0827\n",
      "Epoch 80/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.4081 - reconstruction_loss: 6.4039 - kl_loss: 4.0042\n",
      "Epoch 81/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 10.2703 - reconstruction_loss: 6.2155 - kl_loss: 4.0548\n",
      "Epoch 82/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 10.5097 - reconstruction_loss: 6.4744 - kl_loss: 4.0352\n",
      "Epoch 83/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 11.9356 - reconstruction_loss: 8.0070 - kl_loss: 3.9286\n",
      "Epoch 84/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 13.0593 - reconstruction_loss: 9.0702 - kl_loss: 3.9891\n",
      "Epoch 85/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 10.6864 - reconstruction_loss: 6.6769 - kl_loss: 4.0094\n",
      "Epoch 86/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 11.0506 - reconstruction_loss: 7.1474 - kl_loss: 3.9032\n",
      "Epoch 87/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.4340 - reconstruction_loss: 6.4927 - kl_loss: 3.9413\n",
      "Epoch 88/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.3927 - reconstruction_loss: 5.4209 - kl_loss: 3.9719\n",
      "Epoch 89/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 10.8371 - reconstruction_loss: 7.0643 - kl_loss: 3.7729\n",
      "Epoch 90/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.5307 - reconstruction_loss: 5.6601 - kl_loss: 3.8707\n",
      "Epoch 91/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.4900 - reconstruction_loss: 4.6341 - kl_loss: 3.8559\n",
      "Epoch 92/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.8239 - reconstruction_loss: 4.9487 - kl_loss: 3.8752\n",
      "Epoch 93/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.5815 - reconstruction_loss: 4.7603 - kl_loss: 3.8212\n",
      "Epoch 94/1000\n",
      "4/4 [==============================] - 0s 508us/step - loss: 9.7166 - reconstruction_loss: 5.8582 - kl_loss: 3.8584\n",
      "Epoch 95/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 8.9075 - reconstruction_loss: 5.1180 - kl_loss: 3.7895\n",
      "Epoch 96/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 9.3750 - reconstruction_loss: 5.4563 - kl_loss: 3.9188\n",
      "Epoch 97/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.9171 - reconstruction_loss: 7.1806 - kl_loss: 3.7365\n",
      "Epoch 98/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.2149 - reconstruction_loss: 6.4473 - kl_loss: 3.7676\n",
      "Epoch 99/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 9.0874 - reconstruction_loss: 5.4254 - kl_loss: 3.6620\n",
      "Epoch 100/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 8.9870 - reconstruction_loss: 5.3169 - kl_loss: 3.6701\n",
      "Epoch 101/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 8.5315 - reconstruction_loss: 4.9337 - kl_loss: 3.5978\n",
      "Epoch 102/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 9.2061 - reconstruction_loss: 5.5462 - kl_loss: 3.6599\n",
      "Epoch 103/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.6241 - reconstruction_loss: 3.9100 - kl_loss: 3.7141\n",
      "Epoch 104/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.6203 - reconstruction_loss: 3.8314 - kl_loss: 3.7889\n",
      "Epoch 105/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 7.7780 - reconstruction_loss: 4.1212 - kl_loss: 3.6568\n",
      "Epoch 106/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 7.8845 - reconstruction_loss: 4.1949 - kl_loss: 3.6896\n",
      "Epoch 107/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.6809 - reconstruction_loss: 4.1510 - kl_loss: 3.5298\n",
      "Epoch 108/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.7420 - reconstruction_loss: 4.0117 - kl_loss: 3.7303\n",
      "Epoch 109/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 8.1408 - reconstruction_loss: 4.4606 - kl_loss: 3.6802\n",
      "Epoch 110/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 7.6694 - reconstruction_loss: 4.0731 - kl_loss: 3.5964\n",
      "Epoch 111/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.0797 - reconstruction_loss: 3.3548 - kl_loss: 3.7248\n",
      "Epoch 112/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.7107 - reconstruction_loss: 3.0590 - kl_loss: 3.6517\n",
      "Epoch 113/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 6.8554 - reconstruction_loss: 3.3795 - kl_loss: 3.4759\n",
      "Epoch 114/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 6.7126 - reconstruction_loss: 3.1815 - kl_loss: 3.5310\n",
      "Epoch 115/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 6.8099 - reconstruction_loss: 3.3681 - kl_loss: 3.4418\n",
      "Epoch 116/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.5729 - reconstruction_loss: 2.9970 - kl_loss: 3.5759\n",
      "Epoch 117/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 6.1237 - reconstruction_loss: 2.6600 - kl_loss: 3.4638\n",
      "Epoch 118/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 7.0653 - reconstruction_loss: 3.6265 - kl_loss: 3.4388\n",
      "Epoch 119/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 6.4420 - reconstruction_loss: 2.9400 - kl_loss: 3.5020\n",
      "Epoch 120/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.5230 - reconstruction_loss: 3.1251 - kl_loss: 3.3979\n",
      "Epoch 121/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 6.2146 - reconstruction_loss: 2.8573 - kl_loss: 3.3572\n",
      "Epoch 122/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.3469 - reconstruction_loss: 2.9243 - kl_loss: 3.4226\n",
      "Epoch 123/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 6.2630 - reconstruction_loss: 2.8832 - kl_loss: 3.3797\n",
      "Epoch 124/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.9917 - reconstruction_loss: 2.6815 - kl_loss: 3.3102\n",
      "Epoch 125/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.7911 - reconstruction_loss: 2.3719 - kl_loss: 3.4192\n",
      "Epoch 126/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 6.5999 - reconstruction_loss: 3.2572 - kl_loss: 3.3427\n",
      "Epoch 127/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 6.5864 - reconstruction_loss: 3.1150 - kl_loss: 3.4714\n",
      "Epoch 128/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 7.6052 - reconstruction_loss: 4.2848 - kl_loss: 3.3204\n",
      "Epoch 129/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.5464 - reconstruction_loss: 3.2008 - kl_loss: 3.3457\n",
      "Epoch 130/1000\n",
      "4/4 [==============================] - 0s 236us/step - loss: 6.5842 - reconstruction_loss: 3.2821 - kl_loss: 3.3021\n",
      "Epoch 131/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 6.3316 - reconstruction_loss: 3.0550 - kl_loss: 3.2767\n",
      "Epoch 132/1000\n",
      "4/4 [==============================] - 0s 537us/step - loss: 5.9940 - reconstruction_loss: 2.6855 - kl_loss: 3.3085\n",
      "Epoch 133/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 5.9857 - reconstruction_loss: 2.6538 - kl_loss: 3.3319\n",
      "Epoch 134/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 6.6821 - reconstruction_loss: 3.3882 - kl_loss: 3.2939\n",
      "Epoch 135/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 6.0033 - reconstruction_loss: 2.7564 - kl_loss: 3.2469\n",
      "Epoch 136/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 6.3729 - reconstruction_loss: 3.0598 - kl_loss: 3.3131\n",
      "Epoch 137/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.6567 - reconstruction_loss: 2.3147 - kl_loss: 3.3420\n",
      "Epoch 138/1000\n",
      "4/4 [==============================] - ETA: 0s - loss: 5.3578 - reconstruction_loss: 2.0824 - kl_loss: 3.275 - 0s 0s/step - loss: 5.9880 - reconstruction_loss: 2.6566 - kl_loss: 3.3314\n",
      "Epoch 139/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.9189 - reconstruction_loss: 2.6358 - kl_loss: 3.2831\n",
      "Epoch 140/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 5.7271 - reconstruction_loss: 2.5344 - kl_loss: 3.1927\n",
      "Epoch 141/1000\n",
      "4/4 [==============================] - 0s 521us/step - loss: 6.1591 - reconstruction_loss: 2.8904 - kl_loss: 3.2687\n",
      "Epoch 142/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 8.9214 - reconstruction_loss: 5.7163 - kl_loss: 3.2051\n",
      "Epoch 143/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 7.9833 - reconstruction_loss: 4.6902 - kl_loss: 3.2931\n",
      "Epoch 144/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.0686 - reconstruction_loss: 3.8743 - kl_loss: 3.1943\n",
      "Epoch 145/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 6.9887 - reconstruction_loss: 3.6926 - kl_loss: 3.2961\n",
      "Epoch 146/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 8.3917 - reconstruction_loss: 5.0500 - kl_loss: 3.3417\n",
      "Epoch 147/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 7.2792 - reconstruction_loss: 4.0507 - kl_loss: 3.2285\n",
      "Epoch 148/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 6.1206 - reconstruction_loss: 2.8565 - kl_loss: 3.2641\n",
      "Epoch 149/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 5.9492 - reconstruction_loss: 2.7155 - kl_loss: 3.2337\n",
      "Epoch 150/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.1736 - reconstruction_loss: 2.8668 - kl_loss: 3.3068\n",
      "Epoch 151/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.6756 - reconstruction_loss: 2.4365 - kl_loss: 3.2390\n",
      "Epoch 152/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 5.9433 - reconstruction_loss: 2.7847 - kl_loss: 3.1586\n",
      "Epoch 153/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 5.5301 - reconstruction_loss: 2.2812 - kl_loss: 3.2489\n",
      "Epoch 154/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 5.7614 - reconstruction_loss: 2.5717 - kl_loss: 3.1897\n",
      "Epoch 155/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.4143 - reconstruction_loss: 2.1811 - kl_loss: 3.2332\n",
      "Epoch 156/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 5.6447 - reconstruction_loss: 2.3793 - kl_loss: 3.2653\n",
      "Epoch 157/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.0704 - reconstruction_loss: 2.9580 - kl_loss: 3.1125\n",
      "Epoch 158/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.9269 - reconstruction_loss: 2.8125 - kl_loss: 3.1144\n",
      "Epoch 159/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 5.7061 - reconstruction_loss: 2.4942 - kl_loss: 3.2119\n",
      "Epoch 160/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 5.9926 - reconstruction_loss: 2.8825 - kl_loss: 3.1101\n",
      "Epoch 161/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.3464 - reconstruction_loss: 3.1519 - kl_loss: 3.1945\n",
      "Epoch 162/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.1508 - reconstruction_loss: 3.0218 - kl_loss: 3.1290\n",
      "Epoch 163/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 6.5721 - reconstruction_loss: 3.3912 - kl_loss: 3.1809\n",
      "Epoch 164/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 6.8238 - reconstruction_loss: 3.6126 - kl_loss: 3.2112\n",
      "Epoch 165/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.6744 - reconstruction_loss: 2.5006 - kl_loss: 3.1738\n",
      "Epoch 166/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 5.8383 - reconstruction_loss: 2.6936 - kl_loss: 3.1447\n",
      "Epoch 167/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 5.5111 - reconstruction_loss: 2.3803 - kl_loss: 3.1308\n",
      "Epoch 168/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 5.6001 - reconstruction_loss: 2.4290 - kl_loss: 3.1711\n",
      "Epoch 169/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 6.2434 - reconstruction_loss: 3.1247 - kl_loss: 3.1187\n",
      "Epoch 170/1000\n",
      "4/4 [==============================] - 0s 878us/step - loss: 5.4734 - reconstruction_loss: 2.3590 - kl_loss: 3.1144\n",
      "Epoch 171/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 5.5868 - reconstruction_loss: 2.4331 - kl_loss: 3.1537\n",
      "Epoch 172/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 5.2289 - reconstruction_loss: 2.1042 - kl_loss: 3.1247\n",
      "Epoch 173/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.9593 - reconstruction_loss: 1.8578 - kl_loss: 3.1016\n",
      "Epoch 174/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 5.1895 - reconstruction_loss: 2.0165 - kl_loss: 3.1730\n",
      "Epoch 175/1000\n",
      "4/4 [==============================] - 0s 508us/step - loss: 5.2521 - reconstruction_loss: 2.1705 - kl_loss: 3.0816\n",
      "Epoch 176/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 5.4217 - reconstruction_loss: 2.3007 - kl_loss: 3.1210\n",
      "Epoch 177/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.3427 - reconstruction_loss: 2.2685 - kl_loss: 3.0742\n",
      "Epoch 178/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.4528 - reconstruction_loss: 2.4610 - kl_loss: 2.9919\n",
      "Epoch 179/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 4.7888 - reconstruction_loss: 1.6667 - kl_loss: 3.1221\n",
      "Epoch 180/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.3979 - reconstruction_loss: 2.4208 - kl_loss: 2.9772\n",
      "Epoch 181/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.1605 - reconstruction_loss: 2.0822 - kl_loss: 3.0783\n",
      "Epoch 182/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 4.6825 - reconstruction_loss: 1.6573 - kl_loss: 3.0253\n",
      "Epoch 183/1000\n",
      "4/4 [==============================] - 0s 626us/step - loss: 4.8571 - reconstruction_loss: 1.7258 - kl_loss: 3.1313\n",
      "Epoch 184/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 4.6977 - reconstruction_loss: 1.6483 - kl_loss: 3.0494\n",
      "Epoch 185/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 5.5119 - reconstruction_loss: 2.4487 - kl_loss: 3.0632\n",
      "Epoch 186/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 4.7525 - reconstruction_loss: 1.7255 - kl_loss: 3.0270\n",
      "Epoch 187/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 5.0084 - reconstruction_loss: 1.9543 - kl_loss: 3.0541\n",
      "Epoch 188/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 5.3639 - reconstruction_loss: 2.3861 - kl_loss: 2.9778\n",
      "Epoch 189/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.3039 - reconstruction_loss: 2.3336 - kl_loss: 2.9704\n",
      "Epoch 190/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.7874 - reconstruction_loss: 1.8520 - kl_loss: 2.9354\n",
      "Epoch 191/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.5375 - reconstruction_loss: 1.5992 - kl_loss: 2.9383\n",
      "Epoch 192/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.9590 - reconstruction_loss: 1.9964 - kl_loss: 2.9626\n",
      "Epoch 193/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.0900 - reconstruction_loss: 3.0831 - kl_loss: 3.0069\n",
      "Epoch 194/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 5.0792 - reconstruction_loss: 2.1447 - kl_loss: 2.9345\n",
      "Epoch 195/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.2216 - reconstruction_loss: 2.2596 - kl_loss: 2.9620\n",
      "Epoch 196/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.6874 - reconstruction_loss: 1.7158 - kl_loss: 2.9716\n",
      "Epoch 197/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.6803 - reconstruction_loss: 1.7154 - kl_loss: 2.9649\n",
      "Epoch 198/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 5.0175 - reconstruction_loss: 2.0977 - kl_loss: 2.9198\n",
      "Epoch 199/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.4905 - reconstruction_loss: 1.5733 - kl_loss: 2.9172\n",
      "Epoch 200/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.7461 - reconstruction_loss: 1.8083 - kl_loss: 2.9378\n",
      "Epoch 201/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.9225 - reconstruction_loss: 1.9643 - kl_loss: 2.9583\n",
      "Epoch 202/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 5.0013 - reconstruction_loss: 2.0900 - kl_loss: 2.9113\n",
      "Epoch 203/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 4.7761 - reconstruction_loss: 1.7785 - kl_loss: 2.9976\n",
      "Epoch 204/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 4.9821 - reconstruction_loss: 2.0447 - kl_loss: 2.9374\n",
      "Epoch 205/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.0343 - reconstruction_loss: 2.0211 - kl_loss: 3.0132\n",
      "Epoch 206/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 5.7328 - reconstruction_loss: 2.7248 - kl_loss: 3.0080\n",
      "Epoch 207/1000\n",
      "4/4 [==============================] - 0s 757us/step - loss: 6.0246 - reconstruction_loss: 3.0645 - kl_loss: 2.9601\n",
      "Epoch 208/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 6.7535 - reconstruction_loss: 3.8073 - kl_loss: 2.9463\n",
      "Epoch 209/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.5916 - reconstruction_loss: 3.6706 - kl_loss: 2.9210\n",
      "Epoch 210/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.6131 - reconstruction_loss: 2.6761 - kl_loss: 2.9370\n",
      "Epoch 211/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 5.5201 - reconstruction_loss: 2.7014 - kl_loss: 2.8187\n",
      "Epoch 212/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.7438 - reconstruction_loss: 2.8408 - kl_loss: 2.9030\n",
      "Epoch 213/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.7223 - reconstruction_loss: 2.8689 - kl_loss: 2.8534\n",
      "Epoch 214/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 5.5509 - reconstruction_loss: 2.6366 - kl_loss: 2.9143\n",
      "Epoch 215/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.9901 - reconstruction_loss: 2.0460 - kl_loss: 2.9441\n",
      "Epoch 216/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.9253 - reconstruction_loss: 1.9325 - kl_loss: 2.9928\n",
      "Epoch 217/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 5.1344 - reconstruction_loss: 2.0985 - kl_loss: 3.0359\n",
      "Epoch 218/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 5.0742 - reconstruction_loss: 2.1929 - kl_loss: 2.8813\n",
      "Epoch 219/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 5.1306 - reconstruction_loss: 2.2217 - kl_loss: 2.9089\n",
      "Epoch 220/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.9494 - reconstruction_loss: 2.0526 - kl_loss: 2.8968\n",
      "Epoch 221/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.7141 - reconstruction_loss: 1.9520 - kl_loss: 2.7621\n",
      "Epoch 222/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 4.8247 - reconstruction_loss: 1.9656 - kl_loss: 2.8591\n",
      "Epoch 223/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.9774 - reconstruction_loss: 2.1569 - kl_loss: 2.8204\n",
      "Epoch 224/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.0855 - reconstruction_loss: 2.1216 - kl_loss: 2.9640\n",
      "Epoch 225/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 4.6442 - reconstruction_loss: 1.8277 - kl_loss: 2.8165\n",
      "Epoch 226/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 5.1645 - reconstruction_loss: 2.3259 - kl_loss: 2.8385\n",
      "Epoch 227/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.6246 - reconstruction_loss: 2.8535 - kl_loss: 2.7711\n",
      "Epoch 228/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 5.4869 - reconstruction_loss: 2.6118 - kl_loss: 2.8751\n",
      "Epoch 229/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 4.9943 - reconstruction_loss: 2.1687 - kl_loss: 2.8256\n",
      "Epoch 230/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.8590 - reconstruction_loss: 2.0300 - kl_loss: 2.8291\n",
      "Epoch 231/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.7336 - reconstruction_loss: 1.8864 - kl_loss: 2.8472\n",
      "Epoch 232/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.6005 - reconstruction_loss: 1.7733 - kl_loss: 2.8272\n",
      "Epoch 233/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 5.0292 - reconstruction_loss: 2.2372 - kl_loss: 2.7920\n",
      "Epoch 234/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.2116 - reconstruction_loss: 2.3798 - kl_loss: 2.8318\n",
      "Epoch 235/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.0199 - reconstruction_loss: 2.0810 - kl_loss: 2.9389\n",
      "Epoch 236/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.4923 - reconstruction_loss: 1.6708 - kl_loss: 2.8215\n",
      "Epoch 237/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 4.2230 - reconstruction_loss: 1.4067 - kl_loss: 2.8164\n",
      "Epoch 238/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.7643 - reconstruction_loss: 1.9595 - kl_loss: 2.8048\n",
      "Epoch 239/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.7443 - reconstruction_loss: 1.9522 - kl_loss: 2.7921\n",
      "Epoch 240/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.5709 - reconstruction_loss: 1.7019 - kl_loss: 2.8690\n",
      "Epoch 241/1000\n",
      "4/4 [==============================] - 0s 512us/step - loss: 5.2604 - reconstruction_loss: 2.4712 - kl_loss: 2.7892\n",
      "Epoch 242/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 4.4919 - reconstruction_loss: 1.6821 - kl_loss: 2.8098\n",
      "Epoch 243/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.5875 - reconstruction_loss: 1.8033 - kl_loss: 2.7842\n",
      "Epoch 244/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.5775 - reconstruction_loss: 1.7540 - kl_loss: 2.8235\n",
      "Epoch 245/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 4.7423 - reconstruction_loss: 1.9285 - kl_loss: 2.8138\n",
      "Epoch 246/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 4.8046 - reconstruction_loss: 1.9602 - kl_loss: 2.8445\n",
      "Epoch 247/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.6074 - reconstruction_loss: 1.8189 - kl_loss: 2.7886\n",
      "Epoch 248/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.7241 - reconstruction_loss: 1.8966 - kl_loss: 2.8275\n",
      "Epoch 249/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 4.3221 - reconstruction_loss: 1.5386 - kl_loss: 2.7835\n",
      "Epoch 250/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 4.0966 - reconstruction_loss: 1.3351 - kl_loss: 2.7615\n",
      "Epoch 251/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.0462 - reconstruction_loss: 1.3206 - kl_loss: 2.7256\n",
      "Epoch 252/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.4708 - reconstruction_loss: 1.7868 - kl_loss: 2.6841\n",
      "Epoch 253/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 4.5561 - reconstruction_loss: 1.7607 - kl_loss: 2.7954\n",
      "Epoch 254/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.3163 - reconstruction_loss: 2.5510 - kl_loss: 2.7654\n",
      "Epoch 255/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.7518 - reconstruction_loss: 2.9979 - kl_loss: 2.7539\n",
      "Epoch 256/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 6.2659 - reconstruction_loss: 3.4565 - kl_loss: 2.8094\n",
      "Epoch 257/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.5997 - reconstruction_loss: 3.7116 - kl_loss: 2.8881\n",
      "Epoch 258/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.1165 - reconstruction_loss: 3.3903 - kl_loss: 2.7262\n",
      "Epoch 259/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.7031 - reconstruction_loss: 2.9810 - kl_loss: 2.7221\n",
      "Epoch 260/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.4640 - reconstruction_loss: 2.6451 - kl_loss: 2.8190\n",
      "Epoch 261/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.6600 - reconstruction_loss: 2.8472 - kl_loss: 2.8129\n",
      "Epoch 262/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 4.4943 - reconstruction_loss: 1.7501 - kl_loss: 2.7442\n",
      "Epoch 263/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 4.8619 - reconstruction_loss: 2.1135 - kl_loss: 2.7484\n",
      "Epoch 264/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 5.4520 - reconstruction_loss: 2.6824 - kl_loss: 2.7696\n",
      "Epoch 265/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 4.0801 - reconstruction_loss: 1.3137 - kl_loss: 2.7664\n",
      "Epoch 266/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 4.4111 - reconstruction_loss: 1.6529 - kl_loss: 2.7582\n",
      "Epoch 267/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.2090 - reconstruction_loss: 1.4527 - kl_loss: 2.7563\n",
      "Epoch 268/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.2148 - reconstruction_loss: 1.3831 - kl_loss: 2.8317\n",
      "Epoch 269/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.0476 - reconstruction_loss: 1.2636 - kl_loss: 2.7841\n",
      "Epoch 270/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.1133 - reconstruction_loss: 1.3292 - kl_loss: 2.7841\n",
      "Epoch 271/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.3926 - reconstruction_loss: 1.7215 - kl_loss: 2.6711\n",
      "Epoch 272/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.3244 - reconstruction_loss: 1.6461 - kl_loss: 2.6784\n",
      "Epoch 273/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.4873 - reconstruction_loss: 1.7133 - kl_loss: 2.7740\n",
      "Epoch 274/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.1678 - reconstruction_loss: 1.4755 - kl_loss: 2.6923\n",
      "Epoch 275/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.5119 - reconstruction_loss: 1.8206 - kl_loss: 2.6914\n",
      "Epoch 276/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 4.5804 - reconstruction_loss: 1.8355 - kl_loss: 2.7449\n",
      "Epoch 277/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.9436 - reconstruction_loss: 2.1756 - kl_loss: 2.7679\n",
      "Epoch 278/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 5.4884 - reconstruction_loss: 2.7485 - kl_loss: 2.7399\n",
      "Epoch 279/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.8827 - reconstruction_loss: 2.1082 - kl_loss: 2.7745\n",
      "Epoch 280/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 4.5339 - reconstruction_loss: 1.8676 - kl_loss: 2.6662\n",
      "Epoch 281/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 4.6245 - reconstruction_loss: 1.9434 - kl_loss: 2.6811\n",
      "Epoch 282/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 4.7597 - reconstruction_loss: 2.1103 - kl_loss: 2.6494\n",
      "Epoch 283/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.8029 - reconstruction_loss: 2.0998 - kl_loss: 2.7031\n",
      "Epoch 284/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.5806 - reconstruction_loss: 1.8861 - kl_loss: 2.6946\n",
      "Epoch 285/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 4.0926 - reconstruction_loss: 1.4290 - kl_loss: 2.6636\n",
      "Epoch 286/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.0017 - reconstruction_loss: 1.4155 - kl_loss: 2.5862\n",
      "Epoch 287/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 4.1692 - reconstruction_loss: 1.4384 - kl_loss: 2.7308\n",
      "Epoch 288/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.9875 - reconstruction_loss: 1.2940 - kl_loss: 2.6936\n",
      "Epoch 289/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.3329 - reconstruction_loss: 1.6310 - kl_loss: 2.7020\n",
      "Epoch 290/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.2777 - reconstruction_loss: 1.6316 - kl_loss: 2.6460\n",
      "Epoch 291/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 4.2210 - reconstruction_loss: 1.6081 - kl_loss: 2.6129\n",
      "Epoch 292/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.2615 - reconstruction_loss: 1.5943 - kl_loss: 2.6672\n",
      "Epoch 293/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.2870 - reconstruction_loss: 1.6721 - kl_loss: 2.6149\n",
      "Epoch 294/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.1757 - reconstruction_loss: 1.4215 - kl_loss: 2.7542\n",
      "Epoch 295/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.9681 - reconstruction_loss: 1.2832 - kl_loss: 2.6850\n",
      "Epoch 296/1000\n",
      "4/4 [==============================] - 0s 258us/step - loss: 4.0869 - reconstruction_loss: 1.4088 - kl_loss: 2.6781\n",
      "Epoch 297/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.4121 - reconstruction_loss: 1.8296 - kl_loss: 2.5825\n",
      "Epoch 298/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.5128 - reconstruction_loss: 2.8286 - kl_loss: 2.6842\n",
      "Epoch 299/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 4.6467 - reconstruction_loss: 1.9716 - kl_loss: 2.6751\n",
      "Epoch 300/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.6021 - reconstruction_loss: 1.9393 - kl_loss: 2.6628\n",
      "Epoch 301/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.5775 - reconstruction_loss: 1.9714 - kl_loss: 2.6060\n",
      "Epoch 302/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.4023 - reconstruction_loss: 1.7748 - kl_loss: 2.6275\n",
      "Epoch 303/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.2256 - reconstruction_loss: 1.6337 - kl_loss: 2.5919\n",
      "Epoch 304/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.3665 - reconstruction_loss: 1.7229 - kl_loss: 2.6436\n",
      "Epoch 305/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.1557 - reconstruction_loss: 1.4853 - kl_loss: 2.6704\n",
      "Epoch 306/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 4.4695 - reconstruction_loss: 1.8067 - kl_loss: 2.6628\n",
      "Epoch 307/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.1567 - reconstruction_loss: 2.5190 - kl_loss: 2.6378\n",
      "Epoch 308/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.6953 - reconstruction_loss: 2.0507 - kl_loss: 2.6446\n",
      "Epoch 309/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.4763 - reconstruction_loss: 1.8294 - kl_loss: 2.6469\n",
      "Epoch 310/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.4316 - reconstruction_loss: 1.8487 - kl_loss: 2.5829\n",
      "Epoch 311/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.9951 - reconstruction_loss: 1.4084 - kl_loss: 2.5867\n",
      "Epoch 312/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.0115 - reconstruction_loss: 1.3752 - kl_loss: 2.6363\n",
      "Epoch 313/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.8501 - reconstruction_loss: 2.2161 - kl_loss: 2.6340\n",
      "Epoch 314/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.3277 - reconstruction_loss: 1.6964 - kl_loss: 2.6313\n",
      "Epoch 315/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 4.7693 - reconstruction_loss: 2.2225 - kl_loss: 2.5468\n",
      "Epoch 316/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.1270 - reconstruction_loss: 1.5108 - kl_loss: 2.6161\n",
      "Epoch 317/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.1245 - reconstruction_loss: 1.5284 - kl_loss: 2.5961\n",
      "Epoch 318/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.1705 - reconstruction_loss: 1.5704 - kl_loss: 2.6000\n",
      "Epoch 319/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.9193 - reconstruction_loss: 1.3948 - kl_loss: 2.5246\n",
      "Epoch 320/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.1206 - reconstruction_loss: 1.4928 - kl_loss: 2.6278\n",
      "Epoch 321/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.0030 - reconstruction_loss: 1.3813 - kl_loss: 2.6216\n",
      "Epoch 322/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.0557 - reconstruction_loss: 1.3768 - kl_loss: 2.6789\n",
      "Epoch 323/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.0953 - reconstruction_loss: 1.5161 - kl_loss: 2.5793\n",
      "Epoch 324/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.8871 - reconstruction_loss: 2.2297 - kl_loss: 2.6574\n",
      "Epoch 325/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.1137 - reconstruction_loss: 4.5049 - kl_loss: 2.6087\n",
      "Epoch 326/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.5447 - reconstruction_loss: 3.9737 - kl_loss: 2.5710\n",
      "Epoch 327/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 5.8834 - reconstruction_loss: 3.2695 - kl_loss: 2.6139\n",
      "Epoch 328/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 9.4190 - reconstruction_loss: 6.8078 - kl_loss: 2.6112\n",
      "Epoch 329/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 9.5137 - reconstruction_loss: 6.9281 - kl_loss: 2.5856\n",
      "Epoch 330/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.8752 - reconstruction_loss: 3.1801 - kl_loss: 2.6951\n",
      "Epoch 331/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.2687 - reconstruction_loss: 6.5700 - kl_loss: 2.6987\n",
      "Epoch 332/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.9658 - reconstruction_loss: 7.3255 - kl_loss: 2.6404\n",
      "Epoch 333/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 7.7871 - reconstruction_loss: 5.1918 - kl_loss: 2.5953\n",
      "Epoch 334/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 6.1557 - reconstruction_loss: 3.5174 - kl_loss: 2.6383\n",
      "Epoch 335/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 6.1818 - reconstruction_loss: 3.6027 - kl_loss: 2.5791\n",
      "Epoch 336/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.1584 - reconstruction_loss: 3.5180 - kl_loss: 2.6404\n",
      "Epoch 337/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 5.8964 - reconstruction_loss: 3.2811 - kl_loss: 2.6152\n",
      "Epoch 338/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.7149 - reconstruction_loss: 2.1285 - kl_loss: 2.5864\n",
      "Epoch 339/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.6747 - reconstruction_loss: 2.0870 - kl_loss: 2.5877\n",
      "Epoch 340/1000\n",
      "4/4 [==============================] - 0s 691us/step - loss: 4.6616 - reconstruction_loss: 2.0503 - kl_loss: 2.6113\n",
      "Epoch 341/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 4.3642 - reconstruction_loss: 1.7847 - kl_loss: 2.5795\n",
      "Epoch 342/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.0757 - reconstruction_loss: 1.4929 - kl_loss: 2.5828\n",
      "Epoch 343/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.9041 - reconstruction_loss: 1.3010 - kl_loss: 2.6031\n",
      "Epoch 344/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.9969 - reconstruction_loss: 1.3227 - kl_loss: 2.6743\n",
      "Epoch 345/1000\n",
      "4/4 [==============================] - 0s 513us/step - loss: 4.1246 - reconstruction_loss: 1.4594 - kl_loss: 2.6652\n",
      "Epoch 346/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 4.0168 - reconstruction_loss: 1.4671 - kl_loss: 2.5497\n",
      "Epoch 347/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.0800 - reconstruction_loss: 1.4592 - kl_loss: 2.6208\n",
      "Epoch 348/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.0646 - reconstruction_loss: 1.5011 - kl_loss: 2.5635\n",
      "Epoch 349/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.1931 - reconstruction_loss: 1.6590 - kl_loss: 2.5341\n",
      "Epoch 350/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.0003 - reconstruction_loss: 1.4422 - kl_loss: 2.5581\n",
      "Epoch 351/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.3413 - reconstruction_loss: 1.7480 - kl_loss: 2.5933\n",
      "Epoch 352/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.3100 - reconstruction_loss: 1.7650 - kl_loss: 2.5450\n",
      "Epoch 353/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.0104 - reconstruction_loss: 1.4124 - kl_loss: 2.5980\n",
      "Epoch 354/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.7898 - reconstruction_loss: 1.2601 - kl_loss: 2.5298\n",
      "Epoch 355/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.6450 - reconstruction_loss: 1.1256 - kl_loss: 2.5194\n",
      "Epoch 356/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.7608 - reconstruction_loss: 1.2283 - kl_loss: 2.5325\n",
      "Epoch 357/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.7875 - reconstruction_loss: 1.2030 - kl_loss: 2.5845\n",
      "Epoch 358/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.5934 - reconstruction_loss: 1.0790 - kl_loss: 2.5144\n",
      "Epoch 359/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.5073 - reconstruction_loss: 0.9656 - kl_loss: 2.5416\n",
      "Epoch 360/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.5602 - reconstruction_loss: 1.0512 - kl_loss: 2.5090\n",
      "Epoch 361/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.5507 - reconstruction_loss: 1.0789 - kl_loss: 2.4718\n",
      "Epoch 362/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8093 - reconstruction_loss: 1.2627 - kl_loss: 2.5466\n",
      "Epoch 363/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.6812 - reconstruction_loss: 1.2115 - kl_loss: 2.4696\n",
      "Epoch 364/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 4.0385 - reconstruction_loss: 1.5355 - kl_loss: 2.5030\n",
      "Epoch 365/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.9903 - reconstruction_loss: 1.4829 - kl_loss: 2.5074\n",
      "Epoch 366/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 4.4328 - reconstruction_loss: 1.8625 - kl_loss: 2.5702\n",
      "Epoch 367/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8876 - reconstruction_loss: 1.4034 - kl_loss: 2.4842\n",
      "Epoch 368/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.9405 - reconstruction_loss: 1.4755 - kl_loss: 2.4650\n",
      "Epoch 369/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.9615 - reconstruction_loss: 1.5020 - kl_loss: 2.4594\n",
      "Epoch 370/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.9292 - reconstruction_loss: 1.3872 - kl_loss: 2.5419\n",
      "Epoch 371/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.7661 - reconstruction_loss: 1.3393 - kl_loss: 2.4269\n",
      "Epoch 372/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.7252 - reconstruction_loss: 1.2230 - kl_loss: 2.5022\n",
      "Epoch 373/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.8395 - reconstruction_loss: 1.3294 - kl_loss: 2.5101\n",
      "Epoch 374/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.6768 - reconstruction_loss: 1.2256 - kl_loss: 2.4512\n",
      "Epoch 375/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.5893 - reconstruction_loss: 1.1147 - kl_loss: 2.4746\n",
      "Epoch 376/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.5831 - reconstruction_loss: 1.0663 - kl_loss: 2.5168\n",
      "Epoch 377/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.5058 - reconstruction_loss: 1.0490 - kl_loss: 2.4569\n",
      "Epoch 378/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.6311 - reconstruction_loss: 1.1525 - kl_loss: 2.4786\n",
      "Epoch 379/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.6399 - reconstruction_loss: 1.2279 - kl_loss: 2.4120\n",
      "Epoch 380/1000\n",
      "4/4 [==============================] - 0s 265us/step - loss: 3.5567 - reconstruction_loss: 1.1253 - kl_loss: 2.4314\n",
      "Epoch 381/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.6778 - reconstruction_loss: 1.2356 - kl_loss: 2.4422\n",
      "Epoch 382/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.6708 - reconstruction_loss: 1.2139 - kl_loss: 2.4570\n",
      "Epoch 383/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.7684 - reconstruction_loss: 1.3699 - kl_loss: 2.3985\n",
      "Epoch 384/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.8239 - reconstruction_loss: 1.3251 - kl_loss: 2.4988\n",
      "Epoch 385/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.8551 - reconstruction_loss: 1.4251 - kl_loss: 2.4300\n",
      "Epoch 386/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8592 - reconstruction_loss: 1.4880 - kl_loss: 2.3712\n",
      "Epoch 387/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.5433 - reconstruction_loss: 1.1446 - kl_loss: 2.3987\n",
      "Epoch 388/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.4698 - reconstruction_loss: 1.1429 - kl_loss: 2.3269\n",
      "Epoch 389/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.5351 - reconstruction_loss: 1.0945 - kl_loss: 2.4406\n",
      "Epoch 390/1000\n",
      "4/4 [==============================] - 0s 757us/step - loss: 3.5177 - reconstruction_loss: 1.0728 - kl_loss: 2.4449\n",
      "Epoch 391/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.9602 - reconstruction_loss: 1.5821 - kl_loss: 2.3781\n",
      "Epoch 392/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.7587 - reconstruction_loss: 1.3754 - kl_loss: 2.3833\n",
      "Epoch 393/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.7654 - reconstruction_loss: 1.3182 - kl_loss: 2.4472\n",
      "Epoch 394/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.5495 - reconstruction_loss: 1.1153 - kl_loss: 2.4342\n",
      "Epoch 395/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.5157 - reconstruction_loss: 1.1037 - kl_loss: 2.4120\n",
      "Epoch 396/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.4207 - reconstruction_loss: 1.0070 - kl_loss: 2.4137\n",
      "Epoch 397/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.4012 - reconstruction_loss: 0.9928 - kl_loss: 2.4084\n",
      "Epoch 398/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.6873 - reconstruction_loss: 1.2931 - kl_loss: 2.3942\n",
      "Epoch 399/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.8168 - reconstruction_loss: 1.3765 - kl_loss: 2.4403\n",
      "Epoch 400/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.5723 - reconstruction_loss: 1.1646 - kl_loss: 2.4077\n",
      "Epoch 401/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.5068 - reconstruction_loss: 1.1299 - kl_loss: 2.3769\n",
      "Epoch 402/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.5280 - reconstruction_loss: 1.1181 - kl_loss: 2.4099\n",
      "Epoch 403/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.2717 - reconstruction_loss: 0.8445 - kl_loss: 2.4272\n",
      "Epoch 404/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.3054 - reconstruction_loss: 0.9180 - kl_loss: 2.3875\n",
      "Epoch 405/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.3486 - reconstruction_loss: 0.9749 - kl_loss: 2.3736\n",
      "Epoch 406/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.3374 - reconstruction_loss: 0.9959 - kl_loss: 2.3415\n",
      "Epoch 407/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8045 - reconstruction_loss: 1.3787 - kl_loss: 2.4258\n",
      "Epoch 408/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.3876 - reconstruction_loss: 1.0056 - kl_loss: 2.3820\n",
      "Epoch 409/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.5191 - reconstruction_loss: 1.0940 - kl_loss: 2.4250\n",
      "Epoch 410/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.3110 - reconstruction_loss: 0.8709 - kl_loss: 2.4401\n",
      "Epoch 411/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.1774 - reconstruction_loss: 0.8412 - kl_loss: 2.3362\n",
      "Epoch 412/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.5039 - reconstruction_loss: 1.1723 - kl_loss: 2.3316\n",
      "Epoch 413/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.3385 - reconstruction_loss: 0.9724 - kl_loss: 2.3661\n",
      "Epoch 414/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.4964 - reconstruction_loss: 1.1754 - kl_loss: 2.3210\n",
      "Epoch 415/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.4918 - reconstruction_loss: 1.1298 - kl_loss: 2.3620\n",
      "Epoch 416/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.4510 - reconstruction_loss: 1.0644 - kl_loss: 2.3867\n",
      "Epoch 417/1000\n",
      "4/4 [==============================] - ETA: 0s - loss: 3.3182 - reconstruction_loss: 0.9497 - kl_loss: 2.368 - 0s 2ms/step - loss: 3.7905 - reconstruction_loss: 1.3896 - kl_loss: 2.4009\n",
      "Epoch 418/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.7794 - reconstruction_loss: 1.4128 - kl_loss: 2.3666\n",
      "Epoch 419/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.9266 - reconstruction_loss: 1.5780 - kl_loss: 2.3486\n",
      "Epoch 420/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.4144 - reconstruction_loss: 1.0861 - kl_loss: 2.3283\n",
      "Epoch 421/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.3680 - reconstruction_loss: 0.9998 - kl_loss: 2.3682\n",
      "Epoch 422/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.4537 - reconstruction_loss: 1.1584 - kl_loss: 2.2953\n",
      "Epoch 423/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.5659 - reconstruction_loss: 1.2116 - kl_loss: 2.3543\n",
      "Epoch 424/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8241 - reconstruction_loss: 1.5297 - kl_loss: 2.2944\n",
      "Epoch 425/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.2863 - reconstruction_loss: 1.0338 - kl_loss: 2.2525\n",
      "Epoch 426/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.3443 - reconstruction_loss: 0.9944 - kl_loss: 2.3499\n",
      "Epoch 427/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.3511 - reconstruction_loss: 1.0376 - kl_loss: 2.3135\n",
      "Epoch 428/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.3779 - reconstruction_loss: 1.0902 - kl_loss: 2.2877\n",
      "Epoch 429/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.3729 - reconstruction_loss: 1.0143 - kl_loss: 2.3587\n",
      "Epoch 430/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.2427 - reconstruction_loss: 0.9750 - kl_loss: 2.2677\n",
      "Epoch 431/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 4.2637 - reconstruction_loss: 1.9601 - kl_loss: 2.3036\n",
      "Epoch 432/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 4.1254 - reconstruction_loss: 1.8483 - kl_loss: 2.2771\n",
      "Epoch 433/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.6223 - reconstruction_loss: 1.3463 - kl_loss: 2.2760\n",
      "Epoch 434/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.7336 - reconstruction_loss: 1.4707 - kl_loss: 2.2629\n",
      "Epoch 435/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.4077 - reconstruction_loss: 1.0937 - kl_loss: 2.3139\n",
      "Epoch 436/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.4021 - reconstruction_loss: 1.1224 - kl_loss: 2.2797\n",
      "Epoch 437/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.5285 - reconstruction_loss: 1.2691 - kl_loss: 2.2594\n",
      "Epoch 438/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.3607 - reconstruction_loss: 1.0910 - kl_loss: 2.2697\n",
      "Epoch 439/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.4337 - reconstruction_loss: 1.1623 - kl_loss: 2.2714\n",
      "Epoch 440/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8664 - reconstruction_loss: 1.5135 - kl_loss: 2.3528\n",
      "Epoch 441/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.7512 - reconstruction_loss: 1.5036 - kl_loss: 2.2475\n",
      "Epoch 442/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.7108 - reconstruction_loss: 1.4650 - kl_loss: 2.2459\n",
      "Epoch 443/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.9575 - reconstruction_loss: 1.6980 - kl_loss: 2.2596\n",
      "Epoch 444/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.9860 - reconstruction_loss: 1.7101 - kl_loss: 2.2760\n",
      "Epoch 445/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.0157 - reconstruction_loss: 1.7206 - kl_loss: 2.2951\n",
      "Epoch 446/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.3373 - reconstruction_loss: 2.0834 - kl_loss: 2.2539\n",
      "Epoch 447/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.7996 - reconstruction_loss: 1.5446 - kl_loss: 2.2550\n",
      "Epoch 448/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.7286 - reconstruction_loss: 1.4476 - kl_loss: 2.2810\n",
      "Epoch 449/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.4054 - reconstruction_loss: 1.1257 - kl_loss: 2.2797\n",
      "Epoch 450/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.5827 - reconstruction_loss: 1.3289 - kl_loss: 2.2537\n",
      "Epoch 451/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.3999 - reconstruction_loss: 1.1070 - kl_loss: 2.2929\n",
      "Epoch 452/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.5647 - reconstruction_loss: 1.2734 - kl_loss: 2.2912\n",
      "Epoch 453/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.4364 - reconstruction_loss: 1.1802 - kl_loss: 2.2563\n",
      "Epoch 454/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.5542 - reconstruction_loss: 1.2393 - kl_loss: 2.3149\n",
      "Epoch 455/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.4898 - reconstruction_loss: 1.2075 - kl_loss: 2.2823\n",
      "Epoch 456/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.3385 - reconstruction_loss: 1.1039 - kl_loss: 2.2346\n",
      "Epoch 457/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.4063 - reconstruction_loss: 1.1558 - kl_loss: 2.2505\n",
      "Epoch 458/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.3367 - reconstruction_loss: 1.1112 - kl_loss: 2.2255\n",
      "Epoch 459/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.5277 - reconstruction_loss: 1.2896 - kl_loss: 2.2380\n",
      "Epoch 460/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.5114 - reconstruction_loss: 1.2807 - kl_loss: 2.2307\n",
      "Epoch 461/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.8466 - reconstruction_loss: 1.6587 - kl_loss: 2.1879\n",
      "Epoch 462/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.1671 - reconstruction_loss: 1.9185 - kl_loss: 2.2486\n",
      "Epoch 463/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.8704 - reconstruction_loss: 1.6399 - kl_loss: 2.2305\n",
      "Epoch 464/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 5.4770 - reconstruction_loss: 3.2538 - kl_loss: 2.2232\n",
      "Epoch 465/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.9780 - reconstruction_loss: 2.7460 - kl_loss: 2.2321\n",
      "Epoch 466/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.0039 - reconstruction_loss: 1.7986 - kl_loss: 2.2053\n",
      "Epoch 467/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.6957 - reconstruction_loss: 1.4792 - kl_loss: 2.2165\n",
      "Epoch 468/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.5911 - reconstruction_loss: 1.3911 - kl_loss: 2.2000\n",
      "Epoch 469/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.5524 - reconstruction_loss: 1.3738 - kl_loss: 2.1785\n",
      "Epoch 470/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.4844 - reconstruction_loss: 1.2446 - kl_loss: 2.2399\n",
      "Epoch 471/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.4127 - reconstruction_loss: 1.1987 - kl_loss: 2.2139\n",
      "Epoch 472/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.3913 - reconstruction_loss: 1.1779 - kl_loss: 2.2134\n",
      "Epoch 473/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.3833 - reconstruction_loss: 1.2267 - kl_loss: 2.1565\n",
      "Epoch 474/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.5307 - reconstruction_loss: 1.3491 - kl_loss: 2.1816\n",
      "Epoch 475/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.4815 - reconstruction_loss: 1.3007 - kl_loss: 2.1808\n",
      "Epoch 476/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.5259 - reconstruction_loss: 1.3356 - kl_loss: 2.1903\n",
      "Epoch 477/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.6890 - reconstruction_loss: 1.5446 - kl_loss: 2.1444\n",
      "Epoch 478/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.6109 - reconstruction_loss: 1.4107 - kl_loss: 2.2002\n",
      "Epoch 479/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.7643 - reconstruction_loss: 1.5735 - kl_loss: 2.1908\n",
      "Epoch 480/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.4569 - reconstruction_loss: 1.2514 - kl_loss: 2.2055\n",
      "Epoch 481/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.2701 - reconstruction_loss: 1.0761 - kl_loss: 2.1940\n",
      "Epoch 482/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.2552 - reconstruction_loss: 1.0718 - kl_loss: 2.1834\n",
      "Epoch 483/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.9885 - reconstruction_loss: 0.8297 - kl_loss: 2.1588\n",
      "Epoch 484/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.1296 - reconstruction_loss: 0.8903 - kl_loss: 2.2393\n",
      "Epoch 485/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.2175 - reconstruction_loss: 1.0009 - kl_loss: 2.2166\n",
      "Epoch 486/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.0815 - reconstruction_loss: 0.9248 - kl_loss: 2.1566\n",
      "Epoch 487/1000\n",
      "4/4 [==============================] - 0s 766us/step - loss: 3.1510 - reconstruction_loss: 1.0196 - kl_loss: 2.1315\n",
      "Epoch 488/1000\n",
      "4/4 [==============================] - 0s 501us/step - loss: 3.3687 - reconstruction_loss: 1.2039 - kl_loss: 2.1648\n",
      "Epoch 489/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.8738 - reconstruction_loss: 1.7141 - kl_loss: 2.1596\n",
      "Epoch 490/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.9415 - reconstruction_loss: 1.7808 - kl_loss: 2.1606\n",
      "Epoch 491/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.8369 - reconstruction_loss: 1.7122 - kl_loss: 2.1246\n",
      "Epoch 492/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.1943 - reconstruction_loss: 2.0397 - kl_loss: 2.1546\n",
      "Epoch 493/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.7247 - reconstruction_loss: 1.5275 - kl_loss: 2.1972\n",
      "Epoch 494/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.8025 - reconstruction_loss: 1.5987 - kl_loss: 2.2037\n",
      "Epoch 495/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.7115 - reconstruction_loss: 1.5450 - kl_loss: 2.1665\n",
      "Epoch 496/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.9366 - reconstruction_loss: 1.7691 - kl_loss: 2.1675\n",
      "Epoch 497/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.7554 - reconstruction_loss: 1.6365 - kl_loss: 2.1189\n",
      "Epoch 498/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.4630 - reconstruction_loss: 1.3526 - kl_loss: 2.1105\n",
      "Epoch 499/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.3916 - reconstruction_loss: 1.2535 - kl_loss: 2.1381\n",
      "Epoch 500/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.4113 - reconstruction_loss: 1.2677 - kl_loss: 2.1435\n",
      "Epoch 501/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.1699 - reconstruction_loss: 1.0089 - kl_loss: 2.1610\n",
      "Epoch 502/1000\n",
      "4/4 [==============================] - 0s 268us/step - loss: 3.0984 - reconstruction_loss: 1.0029 - kl_loss: 2.0954\n",
      "Epoch 503/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.1387 - reconstruction_loss: 0.9790 - kl_loss: 2.1597\n",
      "Epoch 504/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.3039 - reconstruction_loss: 1.1585 - kl_loss: 2.1454\n",
      "Epoch 505/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.0583 - reconstruction_loss: 0.9468 - kl_loss: 2.1115\n",
      "Epoch 506/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.0791 - reconstruction_loss: 0.9172 - kl_loss: 2.1620\n",
      "Epoch 507/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.2160 - reconstruction_loss: 1.0986 - kl_loss: 2.1174\n",
      "Epoch 508/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.2737 - reconstruction_loss: 1.1688 - kl_loss: 2.1049\n",
      "Epoch 509/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.2406 - reconstruction_loss: 1.1148 - kl_loss: 2.1258\n",
      "Epoch 510/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.4392 - reconstruction_loss: 1.2805 - kl_loss: 2.1587\n",
      "Epoch 511/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.3222 - reconstruction_loss: 1.1731 - kl_loss: 2.1492\n",
      "Epoch 512/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.2201 - reconstruction_loss: 1.0671 - kl_loss: 2.1529\n",
      "Epoch 513/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.4900 - reconstruction_loss: 2.3735 - kl_loss: 2.1166\n",
      "Epoch 514/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.2654 - reconstruction_loss: 1.1798 - kl_loss: 2.0857\n",
      "Epoch 515/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.3115 - reconstruction_loss: 1.1700 - kl_loss: 2.1415\n",
      "Epoch 516/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.4248 - reconstruction_loss: 1.3358 - kl_loss: 2.0891\n",
      "Epoch 517/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.0988 - reconstruction_loss: 0.9859 - kl_loss: 2.1129\n",
      "Epoch 518/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.2623 - reconstruction_loss: 1.1600 - kl_loss: 2.1023\n",
      "Epoch 519/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.6157 - reconstruction_loss: 1.4791 - kl_loss: 2.1365\n",
      "Epoch 520/1000\n",
      "4/4 [==============================] - 0s 265us/step - loss: 4.0344 - reconstruction_loss: 1.9765 - kl_loss: 2.0579\n",
      "Epoch 521/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.8440 - reconstruction_loss: 1.7466 - kl_loss: 2.0974\n",
      "Epoch 522/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.6910 - reconstruction_loss: 1.6511 - kl_loss: 2.0399\n",
      "Epoch 523/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.9661 - reconstruction_loss: 1.8623 - kl_loss: 2.1038\n",
      "Epoch 524/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 4.3609 - reconstruction_loss: 2.2927 - kl_loss: 2.0682\n",
      "Epoch 525/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.9645 - reconstruction_loss: 1.9100 - kl_loss: 2.0545\n",
      "Epoch 526/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.2659 - reconstruction_loss: 2.1377 - kl_loss: 2.1282\n",
      "Epoch 527/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.1058 - reconstruction_loss: 1.9837 - kl_loss: 2.1221\n",
      "Epoch 528/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.4069 - reconstruction_loss: 2.3226 - kl_loss: 2.0843\n",
      "Epoch 529/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.2372 - reconstruction_loss: 2.1745 - kl_loss: 2.0627\n",
      "Epoch 530/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.6900 - reconstruction_loss: 1.6099 - kl_loss: 2.0801\n",
      "Epoch 531/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.7358 - reconstruction_loss: 1.6549 - kl_loss: 2.0809\n",
      "Epoch 532/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.4019 - reconstruction_loss: 1.3071 - kl_loss: 2.0948\n",
      "Epoch 533/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.0621 - reconstruction_loss: 2.0055 - kl_loss: 2.0565\n",
      "Epoch 534/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.7101 - reconstruction_loss: 1.6231 - kl_loss: 2.0870\n",
      "Epoch 535/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.4956 - reconstruction_loss: 1.4164 - kl_loss: 2.0792\n",
      "Epoch 536/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.3567 - reconstruction_loss: 1.3249 - kl_loss: 2.0318\n",
      "Epoch 537/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.1442 - reconstruction_loss: 1.1256 - kl_loss: 2.0187\n",
      "Epoch 538/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.3601 - reconstruction_loss: 1.3352 - kl_loss: 2.0249\n",
      "Epoch 539/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.5344 - reconstruction_loss: 1.4703 - kl_loss: 2.0642\n",
      "Epoch 540/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.1425 - reconstruction_loss: 1.1003 - kl_loss: 2.0422\n",
      "Epoch 541/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.4227 - reconstruction_loss: 1.3556 - kl_loss: 2.0671\n",
      "Epoch 542/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.2630 - reconstruction_loss: 1.1911 - kl_loss: 2.0719\n",
      "Epoch 543/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.9747 - reconstruction_loss: 0.9040 - kl_loss: 2.0707\n",
      "Epoch 544/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.0165 - reconstruction_loss: 1.0250 - kl_loss: 1.9916\n",
      "Epoch 545/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.1343 - reconstruction_loss: 1.0911 - kl_loss: 2.0432\n",
      "Epoch 546/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.9758 - reconstruction_loss: 0.9188 - kl_loss: 2.0570\n",
      "Epoch 547/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.0229 - reconstruction_loss: 1.0290 - kl_loss: 1.9939\n",
      "Epoch 548/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.9460 - reconstruction_loss: 0.8986 - kl_loss: 2.0474\n",
      "Epoch 549/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.8348 - reconstruction_loss: 0.8414 - kl_loss: 1.9934\n",
      "Epoch 550/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.9387 - reconstruction_loss: 0.9302 - kl_loss: 2.0085\n",
      "Epoch 551/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.9037 - reconstruction_loss: 0.8839 - kl_loss: 2.0198\n",
      "Epoch 552/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.0910 - reconstruction_loss: 1.0806 - kl_loss: 2.0104\n",
      "Epoch 553/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.2009 - reconstruction_loss: 1.1688 - kl_loss: 2.0322\n",
      "Epoch 554/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.2390 - reconstruction_loss: 1.2327 - kl_loss: 2.0063\n",
      "Epoch 555/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.0658 - reconstruction_loss: 1.0540 - kl_loss: 2.0118\n",
      "Epoch 556/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.8643 - reconstruction_loss: 0.8944 - kl_loss: 1.9700\n",
      "Epoch 557/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.9301 - reconstruction_loss: 0.9648 - kl_loss: 1.9653\n",
      "Epoch 558/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.8659 - reconstruction_loss: 0.8426 - kl_loss: 2.0233\n",
      "Epoch 559/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.7656 - reconstruction_loss: 0.7694 - kl_loss: 1.9961\n",
      "Epoch 560/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.9752 - reconstruction_loss: 0.9733 - kl_loss: 2.0018\n",
      "Epoch 561/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.0438 - reconstruction_loss: 1.0447 - kl_loss: 1.9991\n",
      "Epoch 562/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.2032 - reconstruction_loss: 1.2098 - kl_loss: 1.9934\n",
      "Epoch 563/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.6243 - reconstruction_loss: 1.6115 - kl_loss: 2.0128\n",
      "Epoch 564/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.1812 - reconstruction_loss: 1.2346 - kl_loss: 1.9465\n",
      "Epoch 565/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.4530 - reconstruction_loss: 1.4873 - kl_loss: 1.9657\n",
      "Epoch 566/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.6905 - reconstruction_loss: 1.6821 - kl_loss: 2.0084\n",
      "Epoch 567/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 4.3999 - reconstruction_loss: 2.3729 - kl_loss: 2.0270\n",
      "Epoch 568/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.3387 - reconstruction_loss: 3.3320 - kl_loss: 2.0068\n",
      "Epoch 569/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.3314 - reconstruction_loss: 2.2909 - kl_loss: 2.0405\n",
      "Epoch 570/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.3889 - reconstruction_loss: 2.4100 - kl_loss: 1.9789\n",
      "Epoch 571/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.7814 - reconstruction_loss: 1.7903 - kl_loss: 1.9910\n",
      "Epoch 572/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.5395 - reconstruction_loss: 1.5393 - kl_loss: 2.0002\n",
      "Epoch 573/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.3692 - reconstruction_loss: 1.4241 - kl_loss: 1.9451\n",
      "Epoch 574/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.0413 - reconstruction_loss: 1.0777 - kl_loss: 1.9636\n",
      "Epoch 575/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.9605 - reconstruction_loss: 0.9967 - kl_loss: 1.9639\n",
      "Epoch 576/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.0002 - reconstruction_loss: 0.9929 - kl_loss: 2.0073\n",
      "Epoch 577/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.2940 - reconstruction_loss: 1.3071 - kl_loss: 1.9869\n",
      "Epoch 578/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.0012 - reconstruction_loss: 1.0419 - kl_loss: 1.9594\n",
      "Epoch 579/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.0722 - reconstruction_loss: 1.0887 - kl_loss: 1.9835\n",
      "Epoch 580/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.1813 - reconstruction_loss: 1.1708 - kl_loss: 2.0106\n",
      "Epoch 581/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.9582 - reconstruction_loss: 0.9852 - kl_loss: 1.9731\n",
      "Epoch 582/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.0139 - reconstruction_loss: 1.0148 - kl_loss: 1.9991\n",
      "Epoch 583/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.3958 - reconstruction_loss: 1.3792 - kl_loss: 2.0167\n",
      "Epoch 584/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.1769 - reconstruction_loss: 1.2134 - kl_loss: 1.9634\n",
      "Epoch 585/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.0785 - reconstruction_loss: 1.1291 - kl_loss: 1.9494\n",
      "Epoch 586/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.1531 - reconstruction_loss: 1.1724 - kl_loss: 1.9807\n",
      "Epoch 587/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.4537 - reconstruction_loss: 1.4709 - kl_loss: 1.9828\n",
      "Epoch 588/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.4356 - reconstruction_loss: 1.5162 - kl_loss: 1.9194\n",
      "Epoch 589/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.1488 - reconstruction_loss: 1.2162 - kl_loss: 1.9326\n",
      "Epoch 590/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.0297 - reconstruction_loss: 1.1048 - kl_loss: 1.9249\n",
      "Epoch 591/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.0150 - reconstruction_loss: 1.0907 - kl_loss: 1.9243\n",
      "Epoch 592/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.8600 - reconstruction_loss: 0.9097 - kl_loss: 1.9503\n",
      "Epoch 593/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.8112 - reconstruction_loss: 0.8899 - kl_loss: 1.9212\n",
      "Epoch 594/1000\n",
      "4/4 [==============================] - 0s 765us/step - loss: 3.0200 - reconstruction_loss: 1.0518 - kl_loss: 1.9682\n",
      "Epoch 595/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.3021 - reconstruction_loss: 1.3994 - kl_loss: 1.9027\n",
      "Epoch 596/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.0922 - reconstruction_loss: 1.2003 - kl_loss: 1.8919\n",
      "Epoch 597/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.2493 - reconstruction_loss: 1.3563 - kl_loss: 1.8930\n",
      "Epoch 598/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.1483 - reconstruction_loss: 1.2061 - kl_loss: 1.9422\n",
      "Epoch 599/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.0307 - reconstruction_loss: 1.0896 - kl_loss: 1.9412\n",
      "Epoch 600/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.7449 - reconstruction_loss: 0.8492 - kl_loss: 1.8957\n",
      "Epoch 601/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.9602 - reconstruction_loss: 1.0391 - kl_loss: 1.9211\n",
      "Epoch 602/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.9811 - reconstruction_loss: 1.0629 - kl_loss: 1.9182\n",
      "Epoch 603/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.2144 - reconstruction_loss: 1.3209 - kl_loss: 1.8935\n",
      "Epoch 604/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.2104 - reconstruction_loss: 1.2717 - kl_loss: 1.9387\n",
      "Epoch 605/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.0789 - reconstruction_loss: 1.1586 - kl_loss: 1.9203\n",
      "Epoch 606/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.8367 - reconstruction_loss: 0.9483 - kl_loss: 1.8885\n",
      "Epoch 607/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.9426 - reconstruction_loss: 1.0329 - kl_loss: 1.9097\n",
      "Epoch 608/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.9625 - reconstruction_loss: 1.0869 - kl_loss: 1.8755\n",
      "Epoch 609/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.9792 - reconstruction_loss: 1.0862 - kl_loss: 1.8930\n",
      "Epoch 610/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.8174 - reconstruction_loss: 0.9317 - kl_loss: 1.8857\n",
      "Epoch 611/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.6315 - reconstruction_loss: 0.7132 - kl_loss: 1.9182\n",
      "Epoch 612/1000\n",
      "4/4 [==============================] - 0s 262us/step - loss: 2.6260 - reconstruction_loss: 0.7428 - kl_loss: 1.8831\n",
      "Epoch 613/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.5516 - reconstruction_loss: 0.6109 - kl_loss: 1.9406\n",
      "Epoch 614/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.5539 - reconstruction_loss: 0.6655 - kl_loss: 1.8884\n",
      "Epoch 615/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.6851 - reconstruction_loss: 0.8242 - kl_loss: 1.8609\n",
      "Epoch 616/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.6658 - reconstruction_loss: 0.7795 - kl_loss: 1.8863\n",
      "Epoch 617/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.7392 - reconstruction_loss: 0.8596 - kl_loss: 1.8796\n",
      "Epoch 618/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.6256 - reconstruction_loss: 0.7367 - kl_loss: 1.8889\n",
      "Epoch 619/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.6153 - reconstruction_loss: 0.7343 - kl_loss: 1.8810\n",
      "Epoch 620/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.6479 - reconstruction_loss: 0.7897 - kl_loss: 1.8582\n",
      "Epoch 621/1000\n",
      "4/4 [==============================] - 0s 509us/step - loss: 2.7284 - reconstruction_loss: 0.8676 - kl_loss: 1.8608\n",
      "Epoch 622/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.6770 - reconstruction_loss: 0.8060 - kl_loss: 1.8710\n",
      "Epoch 623/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5719 - reconstruction_loss: 0.6725 - kl_loss: 1.8993\n",
      "Epoch 624/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.9038 - reconstruction_loss: 1.0248 - kl_loss: 1.8790\n",
      "Epoch 625/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.8611 - reconstruction_loss: 1.0174 - kl_loss: 1.8437\n",
      "Epoch 626/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.6152 - reconstruction_loss: 0.7872 - kl_loss: 1.8281\n",
      "Epoch 627/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.0040 - reconstruction_loss: 1.1729 - kl_loss: 1.8312\n",
      "Epoch 628/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.1676 - reconstruction_loss: 1.2985 - kl_loss: 1.8691\n",
      "Epoch 629/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.5058 - reconstruction_loss: 1.6447 - kl_loss: 1.8611\n",
      "Epoch 630/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.3813 - reconstruction_loss: 1.5372 - kl_loss: 1.8441\n",
      "Epoch 631/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.0102 - reconstruction_loss: 1.1550 - kl_loss: 1.8552\n",
      "Epoch 632/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.2606 - reconstruction_loss: 1.3894 - kl_loss: 1.8712\n",
      "Epoch 633/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.9323 - reconstruction_loss: 1.1064 - kl_loss: 1.8259\n",
      "Epoch 634/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.9796 - reconstruction_loss: 1.0952 - kl_loss: 1.8844\n",
      "Epoch 635/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.7728 - reconstruction_loss: 0.9087 - kl_loss: 1.8642\n",
      "Epoch 636/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.9099 - reconstruction_loss: 1.0742 - kl_loss: 1.8356\n",
      "Epoch 637/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.9732 - reconstruction_loss: 1.1078 - kl_loss: 1.8654\n",
      "Epoch 638/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.7498 - reconstruction_loss: 0.9124 - kl_loss: 1.8375\n",
      "Epoch 639/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.8069 - reconstruction_loss: 0.9659 - kl_loss: 1.8411\n",
      "Epoch 640/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.0034 - reconstruction_loss: 1.1609 - kl_loss: 1.8425\n",
      "Epoch 641/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.1228 - reconstruction_loss: 1.3034 - kl_loss: 1.8194\n",
      "Epoch 642/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.0213 - reconstruction_loss: 2.1746 - kl_loss: 1.8467\n",
      "Epoch 643/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.1569 - reconstruction_loss: 1.3349 - kl_loss: 1.8220\n",
      "Epoch 644/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.2660 - reconstruction_loss: 1.4347 - kl_loss: 1.8313\n",
      "Epoch 645/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.6475 - reconstruction_loss: 1.8059 - kl_loss: 1.8417\n",
      "Epoch 646/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8033 - reconstruction_loss: 1.9520 - kl_loss: 1.8513\n",
      "Epoch 647/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.2414 - reconstruction_loss: 1.4568 - kl_loss: 1.7846\n",
      "Epoch 648/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.0661 - reconstruction_loss: 2.1985 - kl_loss: 1.8676\n",
      "Epoch 649/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.7256 - reconstruction_loss: 0.9260 - kl_loss: 1.7996\n",
      "Epoch 650/1000\n",
      "4/4 [==============================] - 0s 377us/step - loss: 2.8785 - reconstruction_loss: 1.0571 - kl_loss: 1.8215\n",
      "Epoch 651/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.8521 - reconstruction_loss: 1.0383 - kl_loss: 1.8137\n",
      "Epoch 652/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.9411 - reconstruction_loss: 1.1055 - kl_loss: 1.8356\n",
      "Epoch 653/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.9271 - reconstruction_loss: 1.1015 - kl_loss: 1.8257\n",
      "Epoch 654/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.1458 - reconstruction_loss: 1.3216 - kl_loss: 1.8242\n",
      "Epoch 655/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.0007 - reconstruction_loss: 1.1630 - kl_loss: 1.8377\n",
      "Epoch 656/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.8677 - reconstruction_loss: 1.0282 - kl_loss: 1.8395\n",
      "Epoch 657/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.7106 - reconstruction_loss: 0.9026 - kl_loss: 1.8080\n",
      "Epoch 658/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5967 - reconstruction_loss: 0.8249 - kl_loss: 1.7719\n",
      "Epoch 659/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5011 - reconstruction_loss: 0.7248 - kl_loss: 1.7763\n",
      "Epoch 660/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.6955 - reconstruction_loss: 0.8681 - kl_loss: 1.8274\n",
      "Epoch 661/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.7620 - reconstruction_loss: 0.9440 - kl_loss: 1.8181\n",
      "Epoch 662/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.8084 - reconstruction_loss: 1.0235 - kl_loss: 1.7849\n",
      "Epoch 663/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.0075 - reconstruction_loss: 1.2197 - kl_loss: 1.7878\n",
      "Epoch 664/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.7341 - reconstruction_loss: 0.9133 - kl_loss: 1.8208\n",
      "Epoch 665/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.7551 - reconstruction_loss: 0.9620 - kl_loss: 1.7932\n",
      "Epoch 666/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.9576 - reconstruction_loss: 1.1732 - kl_loss: 1.7844\n",
      "Epoch 667/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.8947 - reconstruction_loss: 1.1305 - kl_loss: 1.7642\n",
      "Epoch 668/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.0673 - reconstruction_loss: 1.2905 - kl_loss: 1.7768\n",
      "Epoch 669/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.6955 - reconstruction_loss: 0.8970 - kl_loss: 1.7985\n",
      "Epoch 670/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.7417 - reconstruction_loss: 0.9730 - kl_loss: 1.7687\n",
      "Epoch 671/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.8664 - reconstruction_loss: 1.0715 - kl_loss: 1.7949\n",
      "Epoch 672/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.6046 - reconstruction_loss: 0.7888 - kl_loss: 1.8158\n",
      "Epoch 673/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.6507 - reconstruction_loss: 0.8882 - kl_loss: 1.7625\n",
      "Epoch 674/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.7150 - reconstruction_loss: 0.9221 - kl_loss: 1.7929\n",
      "Epoch 675/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.6439 - reconstruction_loss: 0.9014 - kl_loss: 1.7425\n",
      "Epoch 676/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.6249 - reconstruction_loss: 0.8501 - kl_loss: 1.7748\n",
      "Epoch 677/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5607 - reconstruction_loss: 0.8190 - kl_loss: 1.7417\n",
      "Epoch 678/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.7740 - reconstruction_loss: 0.9940 - kl_loss: 1.7800\n",
      "Epoch 679/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.6733 - reconstruction_loss: 0.9274 - kl_loss: 1.7459\n",
      "Epoch 680/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5689 - reconstruction_loss: 0.8451 - kl_loss: 1.7239\n",
      "Epoch 681/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.4622 - reconstruction_loss: 0.6975 - kl_loss: 1.7646\n",
      "Epoch 682/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.3535 - reconstruction_loss: 0.6365 - kl_loss: 1.7170\n",
      "Epoch 683/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.6003 - reconstruction_loss: 0.8741 - kl_loss: 1.7263\n",
      "Epoch 684/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.6509 - reconstruction_loss: 0.9037 - kl_loss: 1.7471\n",
      "Epoch 685/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.7495 - reconstruction_loss: 0.9832 - kl_loss: 1.7662\n",
      "Epoch 686/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.6873 - reconstruction_loss: 0.9822 - kl_loss: 1.7051\n",
      "Epoch 687/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.6851 - reconstruction_loss: 0.9728 - kl_loss: 1.7122\n",
      "Epoch 688/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.6986 - reconstruction_loss: 0.9490 - kl_loss: 1.7496\n",
      "Epoch 689/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.6957 - reconstruction_loss: 0.9690 - kl_loss: 1.7267\n",
      "Epoch 690/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.0704 - reconstruction_loss: 1.3140 - kl_loss: 1.7563\n",
      "Epoch 691/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.1206 - reconstruction_loss: 1.4131 - kl_loss: 1.7076\n",
      "Epoch 692/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.8564 - reconstruction_loss: 1.1279 - kl_loss: 1.7285\n",
      "Epoch 693/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.4619 - reconstruction_loss: 1.7316 - kl_loss: 1.7303\n",
      "Epoch 694/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 5.5650 - reconstruction_loss: 3.8269 - kl_loss: 1.7380\n",
      "Epoch 695/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.8647 - reconstruction_loss: 3.1134 - kl_loss: 1.7513\n",
      "Epoch 696/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.4902 - reconstruction_loss: 1.7314 - kl_loss: 1.7588\n",
      "Epoch 697/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.9912 - reconstruction_loss: 2.2303 - kl_loss: 1.7609\n",
      "Epoch 698/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.9465 - reconstruction_loss: 2.2016 - kl_loss: 1.7449\n",
      "Epoch 699/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.4370 - reconstruction_loss: 2.6945 - kl_loss: 1.7425\n",
      "Epoch 700/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.0072 - reconstruction_loss: 2.2604 - kl_loss: 1.7468\n",
      "Epoch 701/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.4610 - reconstruction_loss: 1.6811 - kl_loss: 1.7799\n",
      "Epoch 702/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.0421 - reconstruction_loss: 1.2704 - kl_loss: 1.7717\n",
      "Epoch 703/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.8802 - reconstruction_loss: 1.1530 - kl_loss: 1.7272\n",
      "Epoch 704/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.7895 - reconstruction_loss: 1.0326 - kl_loss: 1.7569\n",
      "Epoch 705/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.8245 - reconstruction_loss: 1.1066 - kl_loss: 1.7179\n",
      "Epoch 706/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.0187 - reconstruction_loss: 1.2802 - kl_loss: 1.7385\n",
      "Epoch 707/1000\n",
      "4/4 [==============================] - ETA: 0s - loss: 2.9388 - reconstruction_loss: 1.1931 - kl_loss: 1.745 - 0s 0s/step - loss: 2.8420 - reconstruction_loss: 1.1111 - kl_loss: 1.7309\n",
      "Epoch 708/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.7138 - reconstruction_loss: 0.9617 - kl_loss: 1.7521\n",
      "Epoch 709/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5286 - reconstruction_loss: 0.7724 - kl_loss: 1.7561\n",
      "Epoch 710/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.5972 - reconstruction_loss: 0.8491 - kl_loss: 1.7481\n",
      "Epoch 711/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.7104 - reconstruction_loss: 0.9901 - kl_loss: 1.7203\n",
      "Epoch 712/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.4762 - reconstruction_loss: 0.7609 - kl_loss: 1.7153\n",
      "Epoch 713/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.4012 - reconstruction_loss: 0.6797 - kl_loss: 1.7215\n",
      "Epoch 714/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.3213 - reconstruction_loss: 0.6439 - kl_loss: 1.6774\n",
      "Epoch 715/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5071 - reconstruction_loss: 0.8052 - kl_loss: 1.7018\n",
      "Epoch 716/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.4580 - reconstruction_loss: 0.7528 - kl_loss: 1.7051\n",
      "Epoch 717/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.4657 - reconstruction_loss: 0.7575 - kl_loss: 1.7082\n",
      "Epoch 718/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5231 - reconstruction_loss: 0.8041 - kl_loss: 1.7190\n",
      "Epoch 719/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.3743 - reconstruction_loss: 0.6623 - kl_loss: 1.7119\n",
      "Epoch 720/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.3318 - reconstruction_loss: 0.6442 - kl_loss: 1.6876\n",
      "Epoch 721/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.5042 - reconstruction_loss: 0.7964 - kl_loss: 1.7078\n",
      "Epoch 722/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.5328 - reconstruction_loss: 0.8643 - kl_loss: 1.6685\n",
      "Epoch 723/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.7260 - reconstruction_loss: 0.9985 - kl_loss: 1.7275\n",
      "Epoch 724/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.5680 - reconstruction_loss: 0.9071 - kl_loss: 1.6609\n",
      "Epoch 725/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5579 - reconstruction_loss: 0.8724 - kl_loss: 1.6855\n",
      "Epoch 726/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5210 - reconstruction_loss: 0.8092 - kl_loss: 1.7118\n",
      "Epoch 727/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.4249 - reconstruction_loss: 0.7481 - kl_loss: 1.6768\n",
      "Epoch 728/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.4196 - reconstruction_loss: 0.7401 - kl_loss: 1.6795\n",
      "Epoch 729/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5213 - reconstruction_loss: 0.8266 - kl_loss: 1.6947\n",
      "Epoch 730/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.3920 - reconstruction_loss: 0.6912 - kl_loss: 1.7008\n",
      "Epoch 731/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5826 - reconstruction_loss: 0.8910 - kl_loss: 1.6917\n",
      "Epoch 732/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.9549 - reconstruction_loss: 1.3087 - kl_loss: 1.6462\n",
      "Epoch 733/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5033 - reconstruction_loss: 0.8372 - kl_loss: 1.6661\n",
      "Epoch 734/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5096 - reconstruction_loss: 0.7879 - kl_loss: 1.7217\n",
      "Epoch 735/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.7277 - reconstruction_loss: 1.0525 - kl_loss: 1.6752\n",
      "Epoch 736/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.4881 - reconstruction_loss: 0.8152 - kl_loss: 1.6729\n",
      "Epoch 737/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5177 - reconstruction_loss: 0.8213 - kl_loss: 1.6964\n",
      "Epoch 738/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5636 - reconstruction_loss: 0.8854 - kl_loss: 1.6782\n",
      "Epoch 739/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.6197 - reconstruction_loss: 0.8944 - kl_loss: 1.7253\n",
      "Epoch 740/1000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.6248 - reconstruction_loss: 0.9633 - kl_loss: 1.6614\n",
      "Epoch 741/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.7437 - reconstruction_loss: 1.1152 - kl_loss: 1.6285\n",
      "Epoch 742/1000\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 3.8538 - reconstruction_loss: 2.2156 - kl_loss: 1.6382\n",
      "Epoch 743/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 6.0021 - reconstruction_loss: 4.3364 - kl_loss: 1.6657\n",
      "Epoch 744/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.1459 - reconstruction_loss: 2.5083 - kl_loss: 1.6376\n",
      "Epoch 745/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.7948 - reconstruction_loss: 2.1030 - kl_loss: 1.6919\n",
      "Epoch 746/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.8916 - reconstruction_loss: 1.2491 - kl_loss: 1.6425\n",
      "Epoch 747/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.7592 - reconstruction_loss: 1.1165 - kl_loss: 1.6427\n",
      "Epoch 748/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.5242 - reconstruction_loss: 0.8889 - kl_loss: 1.6353\n",
      "Epoch 749/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.8283 - reconstruction_loss: 1.1463 - kl_loss: 1.6820\n",
      "Epoch 750/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.4406 - reconstruction_loss: 0.8040 - kl_loss: 1.6366\n",
      "Epoch 751/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.5131 - reconstruction_loss: 0.8661 - kl_loss: 1.6470\n",
      "Epoch 752/1000\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 2.6724 - reconstruction_loss: 1.0074 - kl_loss: 1.6650\n",
      "Epoch 753/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.4647 - reconstruction_loss: 0.8304 - kl_loss: 1.6343\n",
      "Epoch 754/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.2916 - reconstruction_loss: 0.6238 - kl_loss: 1.6678\n",
      "Epoch 755/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.2461 - reconstruction_loss: 0.5866 - kl_loss: 1.6595\n",
      "Epoch 756/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.2195 - reconstruction_loss: 0.5642 - kl_loss: 1.6553\n",
      "Epoch 757/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.3382 - reconstruction_loss: 0.6650 - kl_loss: 1.6732\n",
      "Epoch 758/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.4423 - reconstruction_loss: 0.7696 - kl_loss: 1.6726\n",
      "Epoch 759/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.3363 - reconstruction_loss: 0.7043 - kl_loss: 1.6320\n",
      "Epoch 760/1000\n",
      "4/4 [==============================] - 0s 762us/step - loss: 2.6975 - reconstruction_loss: 1.0401 - kl_loss: 1.6574\n",
      "Epoch 761/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.9328 - reconstruction_loss: 1.2936 - kl_loss: 1.6392\n",
      "Epoch 762/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5988 - reconstruction_loss: 0.9729 - kl_loss: 1.6259\n",
      "Epoch 763/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.6793 - reconstruction_loss: 1.0297 - kl_loss: 1.6497\n",
      "Epoch 764/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5411 - reconstruction_loss: 0.9080 - kl_loss: 1.6330\n",
      "Epoch 765/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.4415 - reconstruction_loss: 0.8180 - kl_loss: 1.6234\n",
      "Epoch 766/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.4519 - reconstruction_loss: 0.7830 - kl_loss: 1.6689\n",
      "Epoch 767/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.8994 - reconstruction_loss: 1.2720 - kl_loss: 1.6273\n",
      "Epoch 768/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.7734 - reconstruction_loss: 1.1536 - kl_loss: 1.6198\n",
      "Epoch 769/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.8298 - reconstruction_loss: 1.1870 - kl_loss: 1.6428\n",
      "Epoch 770/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.7635 - reconstruction_loss: 1.1135 - kl_loss: 1.6500\n",
      "Epoch 771/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.5166 - reconstruction_loss: 0.8560 - kl_loss: 1.6606\n",
      "Epoch 772/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.4529 - reconstruction_loss: 0.8266 - kl_loss: 1.6263\n",
      "Epoch 773/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.6387 - reconstruction_loss: 1.0170 - kl_loss: 1.6218\n",
      "Epoch 774/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.3403 - reconstruction_loss: 0.7052 - kl_loss: 1.6352\n",
      "Epoch 775/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.4222 - reconstruction_loss: 0.8070 - kl_loss: 1.6152\n",
      "Epoch 776/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.4724 - reconstruction_loss: 0.8252 - kl_loss: 1.6472\n",
      "Epoch 777/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5553 - reconstruction_loss: 0.9526 - kl_loss: 1.6028\n",
      "Epoch 778/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.8812 - reconstruction_loss: 2.2857 - kl_loss: 1.5955\n",
      "Epoch 779/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.7397 - reconstruction_loss: 3.1205 - kl_loss: 1.6191\n",
      "Epoch 780/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.8862 - reconstruction_loss: 3.2872 - kl_loss: 1.5990\n",
      "Epoch 781/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 5.9620 - reconstruction_loss: 4.3672 - kl_loss: 1.5949\n",
      "Epoch 782/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 5.4584 - reconstruction_loss: 3.8459 - kl_loss: 1.6125\n",
      "Epoch 783/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.5427 - reconstruction_loss: 1.9104 - kl_loss: 1.6323\n",
      "Epoch 784/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.9817 - reconstruction_loss: 3.3719 - kl_loss: 1.6098\n",
      "Epoch 785/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 4.0087 - reconstruction_loss: 2.4031 - kl_loss: 1.6056\n",
      "Epoch 786/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 4.8967 - reconstruction_loss: 3.2621 - kl_loss: 1.6347\n",
      "Epoch 787/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.2802 - reconstruction_loss: 2.6613 - kl_loss: 1.6189\n",
      "Epoch 788/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.5549 - reconstruction_loss: 1.8895 - kl_loss: 1.6654\n",
      "Epoch 789/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.0904 - reconstruction_loss: 1.4422 - kl_loss: 1.6482\n",
      "Epoch 790/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.9128 - reconstruction_loss: 1.2586 - kl_loss: 1.6542\n",
      "Epoch 791/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5123 - reconstruction_loss: 0.8623 - kl_loss: 1.6501\n",
      "Epoch 792/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.2909 - reconstruction_loss: 0.6444 - kl_loss: 1.6465\n",
      "Epoch 793/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.4007 - reconstruction_loss: 0.7845 - kl_loss: 1.6162\n",
      "Epoch 794/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.3545 - reconstruction_loss: 0.7072 - kl_loss: 1.6473\n",
      "Epoch 795/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.3306 - reconstruction_loss: 0.7001 - kl_loss: 1.6305\n",
      "Epoch 796/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.3766 - reconstruction_loss: 0.7402 - kl_loss: 1.6364\n",
      "Epoch 797/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.4957 - reconstruction_loss: 0.8651 - kl_loss: 1.6306\n",
      "Epoch 798/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.2881 - reconstruction_loss: 0.7067 - kl_loss: 1.5815\n",
      "Epoch 799/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.2242 - reconstruction_loss: 0.6352 - kl_loss: 1.5890\n",
      "Epoch 800/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.3812 - reconstruction_loss: 0.7748 - kl_loss: 1.6064\n",
      "Epoch 801/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.3084 - reconstruction_loss: 0.6977 - kl_loss: 1.6107\n",
      "Epoch 802/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2545 - reconstruction_loss: 0.6473 - kl_loss: 1.6072\n",
      "Epoch 803/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2410 - reconstruction_loss: 0.6566 - kl_loss: 1.5844\n",
      "Epoch 804/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2683 - reconstruction_loss: 0.6380 - kl_loss: 1.6303\n",
      "Epoch 805/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2343 - reconstruction_loss: 0.6749 - kl_loss: 1.5593\n",
      "Epoch 806/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2515 - reconstruction_loss: 0.6712 - kl_loss: 1.5803\n",
      "Epoch 807/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.2645 - reconstruction_loss: 0.6634 - kl_loss: 1.6011\n",
      "Epoch 808/1000\n",
      "4/4 [==============================] - 0s 759us/step - loss: 2.1498 - reconstruction_loss: 0.5548 - kl_loss: 1.5950\n",
      "Epoch 809/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.2027 - reconstruction_loss: 0.6308 - kl_loss: 1.5719\n",
      "Epoch 810/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.3164 - reconstruction_loss: 0.7116 - kl_loss: 1.6048\n",
      "Epoch 811/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2326 - reconstruction_loss: 0.6225 - kl_loss: 1.6101\n",
      "Epoch 812/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2207 - reconstruction_loss: 0.6501 - kl_loss: 1.5707\n",
      "Epoch 813/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1588 - reconstruction_loss: 0.5704 - kl_loss: 1.5884\n",
      "Epoch 814/1000\n",
      "4/4 [==============================] - 0s 516us/step - loss: 2.1993 - reconstruction_loss: 0.6003 - kl_loss: 1.5991\n",
      "Epoch 815/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.2566 - reconstruction_loss: 0.6750 - kl_loss: 1.5817\n",
      "Epoch 816/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.6126 - reconstruction_loss: 1.0250 - kl_loss: 1.5876\n",
      "Epoch 817/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5343 - reconstruction_loss: 0.9825 - kl_loss: 1.5518\n",
      "Epoch 818/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2718 - reconstruction_loss: 0.6965 - kl_loss: 1.5754\n",
      "Epoch 819/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.3021 - reconstruction_loss: 0.7330 - kl_loss: 1.5691\n",
      "Epoch 820/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.2109 - reconstruction_loss: 0.6398 - kl_loss: 1.5710\n",
      "Epoch 821/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.0543 - reconstruction_loss: 0.4916 - kl_loss: 1.5627\n",
      "Epoch 822/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.0813 - reconstruction_loss: 0.5166 - kl_loss: 1.5648\n",
      "Epoch 823/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.0598 - reconstruction_loss: 0.4707 - kl_loss: 1.5890\n",
      "Epoch 824/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1114 - reconstruction_loss: 0.5322 - kl_loss: 1.5791\n",
      "Epoch 825/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0693 - reconstruction_loss: 0.5043 - kl_loss: 1.5649\n",
      "Epoch 826/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0986 - reconstruction_loss: 0.5590 - kl_loss: 1.5396\n",
      "Epoch 827/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1149 - reconstruction_loss: 0.5398 - kl_loss: 1.5751\n",
      "Epoch 828/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1075 - reconstruction_loss: 0.5325 - kl_loss: 1.5749\n",
      "Epoch 829/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2114 - reconstruction_loss: 0.6419 - kl_loss: 1.5695\n",
      "Epoch 830/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.2040 - reconstruction_loss: 0.6147 - kl_loss: 1.5893\n",
      "Epoch 831/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.6465 - reconstruction_loss: 1.0950 - kl_loss: 1.5514\n",
      "Epoch 832/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.5429 - reconstruction_loss: 1.9963 - kl_loss: 1.5466\n",
      "Epoch 833/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.1361 - reconstruction_loss: 1.5698 - kl_loss: 1.5663\n",
      "Epoch 834/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.6602 - reconstruction_loss: 2.0564 - kl_loss: 1.6038\n",
      "Epoch 835/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.6604 - reconstruction_loss: 4.0814 - kl_loss: 1.5789\n",
      "Epoch 836/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.7577 - reconstruction_loss: 3.1606 - kl_loss: 1.5971\n",
      "Epoch 837/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.6855 - reconstruction_loss: 3.1068 - kl_loss: 1.5787\n",
      "Epoch 838/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.2727 - reconstruction_loss: 3.6778 - kl_loss: 1.5950\n",
      "Epoch 839/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.7097 - reconstruction_loss: 3.1181 - kl_loss: 1.5916\n",
      "Epoch 840/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.6734 - reconstruction_loss: 2.0961 - kl_loss: 1.5773\n",
      "Epoch 841/1000\n",
      "4/4 [==============================] - 0s 763us/step - loss: 3.7268 - reconstruction_loss: 2.1479 - kl_loss: 1.5789\n",
      "Epoch 842/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.9108 - reconstruction_loss: 2.3316 - kl_loss: 1.5792\n",
      "Epoch 843/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.6663 - reconstruction_loss: 2.0584 - kl_loss: 1.6078\n",
      "Epoch 844/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.9610 - reconstruction_loss: 2.3719 - kl_loss: 1.5892\n",
      "Epoch 845/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 5.0586 - reconstruction_loss: 3.4646 - kl_loss: 1.5940\n",
      "Epoch 846/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 4.2372 - reconstruction_loss: 2.6602 - kl_loss: 1.5770\n",
      "Epoch 847/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.5455 - reconstruction_loss: 3.9522 - kl_loss: 1.5933\n",
      "Epoch 848/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8009 - reconstruction_loss: 2.2002 - kl_loss: 1.6008\n",
      "Epoch 849/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.7978 - reconstruction_loss: 2.2362 - kl_loss: 1.5616\n",
      "Epoch 850/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.7223 - reconstruction_loss: 2.1095 - kl_loss: 1.6128\n",
      "Epoch 851/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.1845 - reconstruction_loss: 1.5814 - kl_loss: 1.6031\n",
      "Epoch 852/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.8402 - reconstruction_loss: 1.2719 - kl_loss: 1.5683\n",
      "Epoch 853/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.5768 - reconstruction_loss: 0.9927 - kl_loss: 1.5841\n",
      "Epoch 854/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.3662 - reconstruction_loss: 0.7809 - kl_loss: 1.5853\n",
      "Epoch 855/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.3661 - reconstruction_loss: 0.8007 - kl_loss: 1.5654\n",
      "Epoch 856/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.3004 - reconstruction_loss: 0.6955 - kl_loss: 1.6050\n",
      "Epoch 857/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.3485 - reconstruction_loss: 0.7756 - kl_loss: 1.5729\n",
      "Epoch 858/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.3429 - reconstruction_loss: 0.7885 - kl_loss: 1.5544\n",
      "Epoch 859/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2442 - reconstruction_loss: 0.6876 - kl_loss: 1.5566\n",
      "Epoch 860/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2908 - reconstruction_loss: 0.7256 - kl_loss: 1.5652\n",
      "Epoch 861/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1088 - reconstruction_loss: 0.5668 - kl_loss: 1.5419\n",
      "Epoch 862/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1658 - reconstruction_loss: 0.6125 - kl_loss: 1.5533\n",
      "Epoch 863/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1567 - reconstruction_loss: 0.5845 - kl_loss: 1.5722\n",
      "Epoch 864/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.1673 - reconstruction_loss: 0.5948 - kl_loss: 1.5724\n",
      "Epoch 865/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2001 - reconstruction_loss: 0.6399 - kl_loss: 1.5602\n",
      "Epoch 866/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1188 - reconstruction_loss: 0.5555 - kl_loss: 1.5633\n",
      "Epoch 867/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.1111 - reconstruction_loss: 0.5667 - kl_loss: 1.5444\n",
      "Epoch 868/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1492 - reconstruction_loss: 0.5832 - kl_loss: 1.5660\n",
      "Epoch 869/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0273 - reconstruction_loss: 0.4802 - kl_loss: 1.5471\n",
      "Epoch 870/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.1140 - reconstruction_loss: 0.5848 - kl_loss: 1.5291\n",
      "Epoch 871/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.1741 - reconstruction_loss: 0.6240 - kl_loss: 1.5501\n",
      "Epoch 872/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.1608 - reconstruction_loss: 0.5950 - kl_loss: 1.5659\n",
      "Epoch 873/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.0784 - reconstruction_loss: 0.5371 - kl_loss: 1.5413\n",
      "Epoch 874/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1635 - reconstruction_loss: 0.6201 - kl_loss: 1.5434\n",
      "Epoch 875/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.1105 - reconstruction_loss: 0.5514 - kl_loss: 1.5591\n",
      "Epoch 876/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1319 - reconstruction_loss: 0.5931 - kl_loss: 1.5388\n",
      "Epoch 877/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.0910 - reconstruction_loss: 0.5444 - kl_loss: 1.5466\n",
      "Epoch 878/1000\n",
      "4/4 [==============================] - 0s 759us/step - loss: 2.2275 - reconstruction_loss: 0.6813 - kl_loss: 1.5463\n",
      "Epoch 879/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.2534 - reconstruction_loss: 0.7285 - kl_loss: 1.5250\n",
      "Epoch 880/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0469 - reconstruction_loss: 0.5546 - kl_loss: 1.4923\n",
      "Epoch 881/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.3198 - reconstruction_loss: 0.8021 - kl_loss: 1.5177\n",
      "Epoch 882/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.4113 - reconstruction_loss: 0.8710 - kl_loss: 1.5403\n",
      "Epoch 883/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.3498 - reconstruction_loss: 0.7906 - kl_loss: 1.5592\n",
      "Epoch 884/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.3733 - reconstruction_loss: 0.8257 - kl_loss: 1.5476\n",
      "Epoch 885/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1474 - reconstruction_loss: 0.6059 - kl_loss: 1.5415\n",
      "Epoch 886/1000\n",
      "4/4 [==============================] - 0s 769us/step - loss: 2.1794 - reconstruction_loss: 0.6374 - kl_loss: 1.5420\n",
      "Epoch 887/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.0100 - reconstruction_loss: 0.4586 - kl_loss: 1.5515\n",
      "Epoch 888/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.0408 - reconstruction_loss: 0.4906 - kl_loss: 1.5502\n",
      "Epoch 889/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.0310 - reconstruction_loss: 0.4835 - kl_loss: 1.5474\n",
      "Epoch 890/1000\n",
      "4/4 [==============================] - 0s 763us/step - loss: 2.0676 - reconstruction_loss: 0.5090 - kl_loss: 1.5586\n",
      "Epoch 891/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.1008 - reconstruction_loss: 0.5621 - kl_loss: 1.5386\n",
      "Epoch 892/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.1499 - reconstruction_loss: 0.6010 - kl_loss: 1.5489\n",
      "Epoch 893/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1120 - reconstruction_loss: 0.5879 - kl_loss: 1.5241\n",
      "Epoch 894/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0889 - reconstruction_loss: 0.5404 - kl_loss: 1.5485\n",
      "Epoch 895/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.1165 - reconstruction_loss: 0.5967 - kl_loss: 1.5198\n",
      "Epoch 896/1000\n",
      "4/4 [==============================] - 0s 13us/step - loss: 2.1047 - reconstruction_loss: 0.5601 - kl_loss: 1.5446\n",
      "Epoch 897/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.1805 - reconstruction_loss: 0.6571 - kl_loss: 1.5234\n",
      "Epoch 898/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2680 - reconstruction_loss: 0.7356 - kl_loss: 1.5323\n",
      "Epoch 899/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.1927 - reconstruction_loss: 0.6590 - kl_loss: 1.5337\n",
      "Epoch 900/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.1234 - reconstruction_loss: 0.5729 - kl_loss: 1.5505\n",
      "Epoch 901/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1283 - reconstruction_loss: 0.6090 - kl_loss: 1.5193\n",
      "Epoch 902/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.1869 - reconstruction_loss: 0.6434 - kl_loss: 1.5435\n",
      "Epoch 903/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.1683 - reconstruction_loss: 0.6350 - kl_loss: 1.5333\n",
      "Epoch 904/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.1179 - reconstruction_loss: 0.5948 - kl_loss: 1.5231\n",
      "Epoch 905/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0623 - reconstruction_loss: 0.5318 - kl_loss: 1.5305\n",
      "Epoch 906/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.1618 - reconstruction_loss: 0.6289 - kl_loss: 1.5329\n",
      "Epoch 907/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1612 - reconstruction_loss: 0.6440 - kl_loss: 1.5172\n",
      "Epoch 908/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0545 - reconstruction_loss: 0.5350 - kl_loss: 1.5195\n",
      "Epoch 909/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.0549 - reconstruction_loss: 0.5389 - kl_loss: 1.5160\n",
      "Epoch 910/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.0634 - reconstruction_loss: 0.5493 - kl_loss: 1.5141\n",
      "Epoch 911/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.1242 - reconstruction_loss: 0.6151 - kl_loss: 1.5091\n",
      "Epoch 912/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2026 - reconstruction_loss: 0.6435 - kl_loss: 1.5591\n",
      "Epoch 913/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.1048 - reconstruction_loss: 0.5625 - kl_loss: 1.5422\n",
      "Epoch 914/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.0018 - reconstruction_loss: 0.4570 - kl_loss: 1.5448\n",
      "Epoch 915/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0617 - reconstruction_loss: 0.5321 - kl_loss: 1.5297\n",
      "Epoch 916/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0448 - reconstruction_loss: 0.5184 - kl_loss: 1.5263\n",
      "Epoch 917/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0203 - reconstruction_loss: 0.4965 - kl_loss: 1.5238\n",
      "Epoch 918/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1460 - reconstruction_loss: 0.6346 - kl_loss: 1.5114\n",
      "Epoch 919/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.1179 - reconstruction_loss: 0.6063 - kl_loss: 1.5116\n",
      "Epoch 920/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.2061 - reconstruction_loss: 0.6767 - kl_loss: 1.5294\n",
      "Epoch 921/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.0800 - reconstruction_loss: 0.5584 - kl_loss: 1.5217\n",
      "Epoch 922/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0992 - reconstruction_loss: 0.5885 - kl_loss: 1.5107\n",
      "Epoch 923/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.0705 - reconstruction_loss: 0.5372 - kl_loss: 1.5334\n",
      "Epoch 924/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0509 - reconstruction_loss: 0.5214 - kl_loss: 1.5295\n",
      "Epoch 925/1000\n",
      "4/4 [==============================] - 0s 531us/step - loss: 2.0737 - reconstruction_loss: 0.5308 - kl_loss: 1.5428\n",
      "Epoch 926/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.0675 - reconstruction_loss: 0.5546 - kl_loss: 1.5129\n",
      "Epoch 927/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0887 - reconstruction_loss: 0.5701 - kl_loss: 1.5186\n",
      "Epoch 928/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0763 - reconstruction_loss: 0.5641 - kl_loss: 1.5122\n",
      "Epoch 929/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.2002 - reconstruction_loss: 0.6843 - kl_loss: 1.5159\n",
      "Epoch 930/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1632 - reconstruction_loss: 0.6444 - kl_loss: 1.5188\n",
      "Epoch 931/1000\n",
      "4/4 [==============================] - 0s 266us/step - loss: 2.0835 - reconstruction_loss: 0.5689 - kl_loss: 1.5147\n",
      "Epoch 932/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.1704 - reconstruction_loss: 0.6396 - kl_loss: 1.5308\n",
      "Epoch 933/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.2137 - reconstruction_loss: 0.6999 - kl_loss: 1.5138\n",
      "Epoch 934/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0719 - reconstruction_loss: 0.5551 - kl_loss: 1.5168\n",
      "Epoch 935/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0834 - reconstruction_loss: 0.5741 - kl_loss: 1.5093\n",
      "Epoch 936/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1.9930 - reconstruction_loss: 0.5000 - kl_loss: 1.4930\n",
      "Epoch 937/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1815 - reconstruction_loss: 0.6552 - kl_loss: 1.5263\n",
      "Epoch 938/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0781 - reconstruction_loss: 0.5653 - kl_loss: 1.5128\n",
      "Epoch 939/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.4421 - reconstruction_loss: 0.9261 - kl_loss: 1.5161\n",
      "Epoch 940/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.1121 - reconstruction_loss: 0.6058 - kl_loss: 1.5063\n",
      "Epoch 941/1000\n",
      "4/4 [==============================] - ETA: 0s - loss: 2.0353 - reconstruction_loss: 0.5412 - kl_loss: 1.494 - 0s 0s/step - loss: 2.2332 - reconstruction_loss: 0.7078 - kl_loss: 1.5254\n",
      "Epoch 942/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2167 - reconstruction_loss: 0.7049 - kl_loss: 1.5117\n",
      "Epoch 943/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2623 - reconstruction_loss: 0.7657 - kl_loss: 1.4966\n",
      "Epoch 944/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.6475 - reconstruction_loss: 1.1379 - kl_loss: 1.5096\n",
      "Epoch 945/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.5285 - reconstruction_loss: 1.0165 - kl_loss: 1.5121\n",
      "Epoch 946/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.5518 - reconstruction_loss: 1.0350 - kl_loss: 1.5168\n",
      "Epoch 947/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.3659 - reconstruction_loss: 0.8520 - kl_loss: 1.5139\n",
      "Epoch 948/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.3521 - reconstruction_loss: 0.8414 - kl_loss: 1.5107\n",
      "Epoch 949/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.4815 - reconstruction_loss: 0.9573 - kl_loss: 1.5242\n",
      "Epoch 950/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.4866 - reconstruction_loss: 0.9812 - kl_loss: 1.5054\n",
      "Epoch 951/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.0364 - reconstruction_loss: 1.5291 - kl_loss: 1.5073\n",
      "Epoch 952/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.8878 - reconstruction_loss: 1.3777 - kl_loss: 1.5101\n",
      "Epoch 953/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5723 - reconstruction_loss: 1.0528 - kl_loss: 1.5195\n",
      "Epoch 954/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.2260 - reconstruction_loss: 1.7295 - kl_loss: 1.4964\n",
      "Epoch 955/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 3.1301 - reconstruction_loss: 1.6257 - kl_loss: 1.5044\n",
      "Epoch 956/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.7995 - reconstruction_loss: 1.2807 - kl_loss: 1.5187\n",
      "Epoch 957/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.8327 - reconstruction_loss: 2.3150 - kl_loss: 1.5177\n",
      "Epoch 958/1000\n",
      "4/4 [==============================] - 0s 757us/step - loss: 2.6406 - reconstruction_loss: 1.1291 - kl_loss: 1.5115\n",
      "Epoch 959/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.2657 - reconstruction_loss: 0.7387 - kl_loss: 1.5270\n",
      "Epoch 960/1000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.1392 - reconstruction_loss: 0.6197 - kl_loss: 1.5194\n",
      "Epoch 961/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1479 - reconstruction_loss: 0.6493 - kl_loss: 1.4986\n",
      "Epoch 962/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.1882 - reconstruction_loss: 0.7024 - kl_loss: 1.4859\n",
      "Epoch 963/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2909 - reconstruction_loss: 0.7694 - kl_loss: 1.5216\n",
      "Epoch 964/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1460 - reconstruction_loss: 0.6576 - kl_loss: 1.4884\n",
      "Epoch 965/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1854 - reconstruction_loss: 0.6746 - kl_loss: 1.5108\n",
      "Epoch 966/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2557 - reconstruction_loss: 0.7376 - kl_loss: 1.5181\n",
      "Epoch 967/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1710 - reconstruction_loss: 0.6536 - kl_loss: 1.5174\n",
      "Epoch 968/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.1701 - reconstruction_loss: 0.6778 - kl_loss: 1.4923\n",
      "Epoch 969/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1101 - reconstruction_loss: 0.6021 - kl_loss: 1.5081\n",
      "Epoch 970/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.9740 - reconstruction_loss: 0.4901 - kl_loss: 1.4839\n",
      "Epoch 971/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1.9847 - reconstruction_loss: 0.4992 - kl_loss: 1.4855\n",
      "Epoch 972/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0123 - reconstruction_loss: 0.5211 - kl_loss: 1.4911\n",
      "Epoch 973/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.3522 - reconstruction_loss: 0.8531 - kl_loss: 1.4991\n",
      "Epoch 974/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.2164 - reconstruction_loss: 0.7266 - kl_loss: 1.4898\n",
      "Epoch 975/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2443 - reconstruction_loss: 0.7337 - kl_loss: 1.5106\n",
      "Epoch 976/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1983 - reconstruction_loss: 0.7203 - kl_loss: 1.4779\n",
      "Epoch 977/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.1041 - reconstruction_loss: 0.6073 - kl_loss: 1.4967\n",
      "Epoch 978/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.1083 - reconstruction_loss: 0.6115 - kl_loss: 1.4967\n",
      "Epoch 979/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1535 - reconstruction_loss: 0.6365 - kl_loss: 1.5170\n",
      "Epoch 980/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2032 - reconstruction_loss: 0.6888 - kl_loss: 1.5144\n",
      "Epoch 981/1000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.2091 - reconstruction_loss: 0.7049 - kl_loss: 1.5042\n",
      "Epoch 982/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.0513 - reconstruction_loss: 0.5615 - kl_loss: 1.4899\n",
      "Epoch 983/1000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.0504 - reconstruction_loss: 0.5711 - kl_loss: 1.4794\n",
      "Epoch 984/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.2230 - reconstruction_loss: 0.7195 - kl_loss: 1.5035\n",
      "Epoch 985/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.1749 - reconstruction_loss: 0.6816 - kl_loss: 1.4933\n",
      "Epoch 986/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.9879 - reconstruction_loss: 0.4918 - kl_loss: 1.4962\n",
      "Epoch 987/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.0795 - reconstruction_loss: 0.5652 - kl_loss: 1.5142\n",
      "Epoch 988/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.9524 - reconstruction_loss: 0.4738 - kl_loss: 1.4786\n",
      "Epoch 989/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.3163 - reconstruction_loss: 0.8212 - kl_loss: 1.4951\n",
      "Epoch 990/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0558 - reconstruction_loss: 0.5670 - kl_loss: 1.4888\n",
      "Epoch 991/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.0216 - reconstruction_loss: 0.5135 - kl_loss: 1.5081\n",
      "Epoch 992/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0051 - reconstruction_loss: 0.5082 - kl_loss: 1.4968\n",
      "Epoch 993/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.1736 - reconstruction_loss: 0.6833 - kl_loss: 1.4903\n",
      "Epoch 994/1000\n",
      "4/4 [==============================] - 0s 757us/step - loss: 2.2088 - reconstruction_loss: 0.7128 - kl_loss: 1.4960\n",
      "Epoch 995/1000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.0809 - reconstruction_loss: 0.5786 - kl_loss: 1.5023\n",
      "Epoch 996/1000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.2024 - reconstruction_loss: 0.7181 - kl_loss: 1.4842\n",
      "Epoch 997/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.3580 - reconstruction_loss: 0.8491 - kl_loss: 1.5089\n",
      "Epoch 998/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.3497 - reconstruction_loss: 0.8836 - kl_loss: 1.4661\n",
      "Epoch 999/1000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.3206 - reconstruction_loss: 0.8517 - kl_loss: 1.4689\n",
      "Epoch 1000/1000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 2.2072 - reconstruction_loss: 0.7327 - kl_loss: 1.4744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27d04587730>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_abnormal_dataset = unique_abnormal_df.to_numpy()\n",
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01))\n",
    "# callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "vae.fit(unique_abnormal_dataset, unique_abnormal_dataset, shuffle=True, batch_size=512, epochs=1000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./{}/twolayermodel'.format(folder)):\n",
    "    os.makedirs('./{}/twolayermodel'.format(folder))\n",
    "encoder.save('./{}/twolayermodel/encoder.h5'.format(folder))\n",
    "decoder.save('./{}/twolayermodel/decoder.h5'.format(folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate abnormal data using decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_reconstructed = keras.models.load_model('./{}/twolayermodel/decoder.h5'.format(folder), compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                176       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 20)                2580      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 12,164\n",
      "Trainable params: 12,164\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_reconstructed.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal cases: 66807\n",
      "abnormal cases: 1327\n"
     ]
    }
   ],
   "source": [
    "X_train_normal = X_train[y_train['Label'] == 0]\n",
    "X_train_abnormal = X_train[y_train['Label'] == 1]\n",
    "X_train_normal_cases = len(X_train[y_train['Label'] == 0])\n",
    "X_train_abnormal_cases = len(X_train[y_train['Label'] == 1])\n",
    "\n",
    "print(\"normal cases: {}\".format(X_train_normal_cases))\n",
    "print(\"abnormal cases: {}\".format(X_train_abnormal_cases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Noise:\n",
      "tf.Tensor(\n",
      "[[ 1.6933831  -1.3289758  -0.08288796 ... -0.62159646  2.2482708\n",
      "   0.5481547 ]\n",
      " [-0.8488968  -0.04466591  0.52049506 ... -0.0561193  -1.258215\n",
      "  -0.9972413 ]\n",
      " [-0.9749879  -0.4871282   0.03693629 ...  1.0377609   1.221005\n",
      "  -1.9281647 ]\n",
      " ...\n",
      " [ 0.1178509  -0.2701514  -0.36190924 ... -1.5109152   0.8102831\n",
      "  -1.33616   ]\n",
      " [ 0.5985779  -0.07057431  1.7941307  ... -0.823333   -0.07097457\n",
      "   1.0082552 ]\n",
      " [-0.7431556  -0.10529099  0.16730161 ... -1.2808744   0.03969169\n",
      "  -0.31770155]], shape=(64816, 10), dtype=float32)\n",
      "------------------------------------------------------------------\n",
      "Generated abnormal datapoints:\n",
      "[[ 1.1157053e+00  8.9127150e+00 -2.1450067e-01 ... -2.3411747e-02\n",
      "   3.8305145e-02  6.2641911e-02]\n",
      " [ 8.8081509e-01  1.0160060e+01  2.2062175e+00 ... -4.5051344e-02\n",
      "  -2.9785695e-02 -1.3362819e-02]\n",
      " [-4.5142773e-01  1.0403548e+01  6.1393194e+00 ...  9.3910143e-02\n",
      "   1.4610987e-02 -2.6555689e-02]\n",
      " ...\n",
      " [ 1.1122376e+01  1.0094225e+01 -1.7940903e-01 ... -2.1843415e-02\n",
      "  -4.5547588e-03  2.9522516e-02]\n",
      " [ 2.2931106e+00  9.2408218e+00 -1.5820358e+00 ...  6.6203162e-02\n",
      "  -3.7917688e-02 -4.3299198e-02]\n",
      " [ 1.1170636e+01  1.0507632e+01 -2.4954309e-01 ... -1.4647727e-02\n",
      "  -6.8585752e-03 -3.5326377e-02]]\n",
      "(64816, 20)\n",
      "<class 'numpy.ndarray'>\n",
      "[1 1 1 ... 1 1 1]\n",
      "(64816,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "cost = 1.5\n",
    "num_of_generated_point = int(X_train_normal_cases - X_train_abnormal_cases * cost)\n",
    "latent_dim = 10\n",
    "latent_size = (num_of_generated_point, latent_dim)\n",
    "gaussian_noise = tf.keras.backend.random_normal(shape=latent_size)\n",
    "print(\"Gaussian Noise:\")\n",
    "print(gaussian_noise)\n",
    "print(\"------------------------------------------------------------------\")\n",
    "\n",
    "# np.testing.assert_allclose(decoder(gaussian_noise), decoder_reconstructed(gaussian_noise))\n",
    "X_train_abnormal_generated = decoder_reconstructed.predict(gaussian_noise, batch_size=256)\n",
    "print(\"Generated abnormal datapoints:\")\n",
    "print(X_train_abnormal_generated)\n",
    "print(X_train_abnormal_generated.shape)\n",
    "print(type(X_train_abnormal_generated))\n",
    "\n",
    "y_train_abnormal_generated = np.ones(num_of_generated_point, dtype=int)\n",
    "print(y_train_abnormal_generated)\n",
    "print(y_train_abnormal_generated.shape)\n",
    "print(type(y_train_abnormal_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.115705</td>\n",
       "      <td>8.912715</td>\n",
       "      <td>-0.214501</td>\n",
       "      <td>-0.235322</td>\n",
       "      <td>0.019120</td>\n",
       "      <td>-0.073375</td>\n",
       "      <td>2.777320</td>\n",
       "      <td>-0.478355</td>\n",
       "      <td>-0.453227</td>\n",
       "      <td>-0.048981</td>\n",
       "      <td>-0.521784</td>\n",
       "      <td>-0.443148</td>\n",
       "      <td>-0.477083</td>\n",
       "      <td>-0.034278</td>\n",
       "      <td>0.167004</td>\n",
       "      <td>-0.017335</td>\n",
       "      <td>-0.011540</td>\n",
       "      <td>-0.023412</td>\n",
       "      <td>0.038305</td>\n",
       "      <td>0.062642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.880815</td>\n",
       "      <td>10.160060</td>\n",
       "      <td>2.206218</td>\n",
       "      <td>-0.112490</td>\n",
       "      <td>-0.021998</td>\n",
       "      <td>-0.011425</td>\n",
       "      <td>1.108720</td>\n",
       "      <td>2.994531</td>\n",
       "      <td>2.983815</td>\n",
       "      <td>-0.003360</td>\n",
       "      <td>3.079229</td>\n",
       "      <td>3.257251</td>\n",
       "      <td>3.030388</td>\n",
       "      <td>0.020031</td>\n",
       "      <td>-0.017122</td>\n",
       "      <td>-0.041607</td>\n",
       "      <td>-0.015996</td>\n",
       "      <td>-0.045051</td>\n",
       "      <td>-0.029786</td>\n",
       "      <td>-0.013363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.451428</td>\n",
       "      <td>10.403548</td>\n",
       "      <td>6.139319</td>\n",
       "      <td>14.063939</td>\n",
       "      <td>-0.001141</td>\n",
       "      <td>-0.031700</td>\n",
       "      <td>3.298990</td>\n",
       "      <td>2.084962</td>\n",
       "      <td>1.967148</td>\n",
       "      <td>0.137638</td>\n",
       "      <td>2.038908</td>\n",
       "      <td>2.054239</td>\n",
       "      <td>2.019951</td>\n",
       "      <td>-0.015414</td>\n",
       "      <td>0.028837</td>\n",
       "      <td>0.015019</td>\n",
       "      <td>-0.071341</td>\n",
       "      <td>0.093910</td>\n",
       "      <td>0.014611</td>\n",
       "      <td>-0.026556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.103623</td>\n",
       "      <td>10.709201</td>\n",
       "      <td>2.075667</td>\n",
       "      <td>3.708000</td>\n",
       "      <td>-0.005365</td>\n",
       "      <td>-0.002478</td>\n",
       "      <td>0.326025</td>\n",
       "      <td>0.437852</td>\n",
       "      <td>0.378502</td>\n",
       "      <td>0.034949</td>\n",
       "      <td>0.359325</td>\n",
       "      <td>0.375356</td>\n",
       "      <td>0.383469</td>\n",
       "      <td>0.004138</td>\n",
       "      <td>-0.005658</td>\n",
       "      <td>0.005921</td>\n",
       "      <td>-0.029612</td>\n",
       "      <td>-0.012635</td>\n",
       "      <td>-0.008296</td>\n",
       "      <td>-0.005526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.662894</td>\n",
       "      <td>10.327458</td>\n",
       "      <td>-0.344029</td>\n",
       "      <td>9.116079</td>\n",
       "      <td>-0.023819</td>\n",
       "      <td>-0.012825</td>\n",
       "      <td>0.470921</td>\n",
       "      <td>17.431015</td>\n",
       "      <td>17.204048</td>\n",
       "      <td>-0.038825</td>\n",
       "      <td>17.384966</td>\n",
       "      <td>17.389751</td>\n",
       "      <td>17.346701</td>\n",
       "      <td>-0.023873</td>\n",
       "      <td>-0.002618</td>\n",
       "      <td>-0.039010</td>\n",
       "      <td>-0.020582</td>\n",
       "      <td>-0.021936</td>\n",
       "      <td>-0.057758</td>\n",
       "      <td>-0.033373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64811</th>\n",
       "      <td>2.341566</td>\n",
       "      <td>10.553798</td>\n",
       "      <td>16.800617</td>\n",
       "      <td>0.189729</td>\n",
       "      <td>-0.014365</td>\n",
       "      <td>-0.046676</td>\n",
       "      <td>12.231989</td>\n",
       "      <td>6.369936</td>\n",
       "      <td>6.298771</td>\n",
       "      <td>-0.002455</td>\n",
       "      <td>6.275505</td>\n",
       "      <td>6.342332</td>\n",
       "      <td>6.368938</td>\n",
       "      <td>-0.049973</td>\n",
       "      <td>0.033098</td>\n",
       "      <td>-0.034865</td>\n",
       "      <td>0.040504</td>\n",
       "      <td>-0.032140</td>\n",
       "      <td>-0.017096</td>\n",
       "      <td>0.006175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64812</th>\n",
       "      <td>-0.358853</td>\n",
       "      <td>26.539164</td>\n",
       "      <td>-0.699497</td>\n",
       "      <td>1.477788</td>\n",
       "      <td>0.121409</td>\n",
       "      <td>-0.004212</td>\n",
       "      <td>6.330398</td>\n",
       "      <td>5.668484</td>\n",
       "      <td>5.596511</td>\n",
       "      <td>-0.034934</td>\n",
       "      <td>5.365867</td>\n",
       "      <td>5.458812</td>\n",
       "      <td>5.598749</td>\n",
       "      <td>-0.079552</td>\n",
       "      <td>0.129989</td>\n",
       "      <td>-0.080324</td>\n",
       "      <td>-0.060262</td>\n",
       "      <td>-0.029574</td>\n",
       "      <td>-0.064908</td>\n",
       "      <td>-0.038233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64813</th>\n",
       "      <td>11.122376</td>\n",
       "      <td>10.094225</td>\n",
       "      <td>-0.179409</td>\n",
       "      <td>-0.052608</td>\n",
       "      <td>0.065096</td>\n",
       "      <td>-0.042077</td>\n",
       "      <td>0.239462</td>\n",
       "      <td>0.593556</td>\n",
       "      <td>0.634020</td>\n",
       "      <td>-0.005169</td>\n",
       "      <td>0.546854</td>\n",
       "      <td>0.436433</td>\n",
       "      <td>0.511746</td>\n",
       "      <td>0.154332</td>\n",
       "      <td>-0.009608</td>\n",
       "      <td>-0.030861</td>\n",
       "      <td>-0.039789</td>\n",
       "      <td>-0.021843</td>\n",
       "      <td>-0.004555</td>\n",
       "      <td>0.029523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64814</th>\n",
       "      <td>2.293111</td>\n",
       "      <td>9.240822</td>\n",
       "      <td>-1.582036</td>\n",
       "      <td>-0.228382</td>\n",
       "      <td>-0.042653</td>\n",
       "      <td>0.049716</td>\n",
       "      <td>2.531285</td>\n",
       "      <td>8.603413</td>\n",
       "      <td>8.811975</td>\n",
       "      <td>-0.040083</td>\n",
       "      <td>9.280260</td>\n",
       "      <td>8.985354</td>\n",
       "      <td>8.749722</td>\n",
       "      <td>-0.030843</td>\n",
       "      <td>-0.065824</td>\n",
       "      <td>-0.053217</td>\n",
       "      <td>-0.076127</td>\n",
       "      <td>0.066203</td>\n",
       "      <td>-0.037918</td>\n",
       "      <td>-0.043299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64815</th>\n",
       "      <td>11.170636</td>\n",
       "      <td>10.507632</td>\n",
       "      <td>-0.249543</td>\n",
       "      <td>2.194473</td>\n",
       "      <td>-0.031998</td>\n",
       "      <td>-0.001589</td>\n",
       "      <td>0.150218</td>\n",
       "      <td>4.223138</td>\n",
       "      <td>4.087596</td>\n",
       "      <td>-0.007715</td>\n",
       "      <td>4.180505</td>\n",
       "      <td>4.122836</td>\n",
       "      <td>4.241879</td>\n",
       "      <td>0.104031</td>\n",
       "      <td>-0.028475</td>\n",
       "      <td>-0.054659</td>\n",
       "      <td>-0.039906</td>\n",
       "      <td>-0.014648</td>\n",
       "      <td>-0.006859</td>\n",
       "      <td>-0.035326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64816 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0          1          2          3         4         5  \\\n",
       "0       1.115705   8.912715  -0.214501  -0.235322  0.019120 -0.073375   \n",
       "1       0.880815  10.160060   2.206218  -0.112490 -0.021998 -0.011425   \n",
       "2      -0.451428  10.403548   6.139319  14.063939 -0.001141 -0.031700   \n",
       "3      -0.103623  10.709201   2.075667   3.708000 -0.005365 -0.002478   \n",
       "4      12.662894  10.327458  -0.344029   9.116079 -0.023819 -0.012825   \n",
       "...          ...        ...        ...        ...       ...       ...   \n",
       "64811   2.341566  10.553798  16.800617   0.189729 -0.014365 -0.046676   \n",
       "64812  -0.358853  26.539164  -0.699497   1.477788  0.121409 -0.004212   \n",
       "64813  11.122376  10.094225  -0.179409  -0.052608  0.065096 -0.042077   \n",
       "64814   2.293111   9.240822  -1.582036  -0.228382 -0.042653  0.049716   \n",
       "64815  11.170636  10.507632  -0.249543   2.194473 -0.031998 -0.001589   \n",
       "\n",
       "               6          7          8         9         10         11  \\\n",
       "0       2.777320  -0.478355  -0.453227 -0.048981  -0.521784  -0.443148   \n",
       "1       1.108720   2.994531   2.983815 -0.003360   3.079229   3.257251   \n",
       "2       3.298990   2.084962   1.967148  0.137638   2.038908   2.054239   \n",
       "3       0.326025   0.437852   0.378502  0.034949   0.359325   0.375356   \n",
       "4       0.470921  17.431015  17.204048 -0.038825  17.384966  17.389751   \n",
       "...          ...        ...        ...       ...        ...        ...   \n",
       "64811  12.231989   6.369936   6.298771 -0.002455   6.275505   6.342332   \n",
       "64812   6.330398   5.668484   5.596511 -0.034934   5.365867   5.458812   \n",
       "64813   0.239462   0.593556   0.634020 -0.005169   0.546854   0.436433   \n",
       "64814   2.531285   8.603413   8.811975 -0.040083   9.280260   8.985354   \n",
       "64815   0.150218   4.223138   4.087596 -0.007715   4.180505   4.122836   \n",
       "\n",
       "              12        13        14        15        16        17        18  \\\n",
       "0      -0.477083 -0.034278  0.167004 -0.017335 -0.011540 -0.023412  0.038305   \n",
       "1       3.030388  0.020031 -0.017122 -0.041607 -0.015996 -0.045051 -0.029786   \n",
       "2       2.019951 -0.015414  0.028837  0.015019 -0.071341  0.093910  0.014611   \n",
       "3       0.383469  0.004138 -0.005658  0.005921 -0.029612 -0.012635 -0.008296   \n",
       "4      17.346701 -0.023873 -0.002618 -0.039010 -0.020582 -0.021936 -0.057758   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "64811   6.368938 -0.049973  0.033098 -0.034865  0.040504 -0.032140 -0.017096   \n",
       "64812   5.598749 -0.079552  0.129989 -0.080324 -0.060262 -0.029574 -0.064908   \n",
       "64813   0.511746  0.154332 -0.009608 -0.030861 -0.039789 -0.021843 -0.004555   \n",
       "64814   8.749722 -0.030843 -0.065824 -0.053217 -0.076127  0.066203 -0.037918   \n",
       "64815   4.241879  0.104031 -0.028475 -0.054659 -0.039906 -0.014648 -0.006859   \n",
       "\n",
       "             19  \n",
       "0      0.062642  \n",
       "1     -0.013363  \n",
       "2     -0.026556  \n",
       "3     -0.005526  \n",
       "4     -0.033373  \n",
       "...         ...  \n",
       "64811  0.006175  \n",
       "64812 -0.038233  \n",
       "64813  0.029523  \n",
       "64814 -0.043299  \n",
       "64815 -0.035326  \n",
       "\n",
       "[64816 rows x 20 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(X_train_abnormal_generated)\n",
    "# test_df.sort_values(by=[7], ascending=False)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use encoder first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_reconstructed = keras.models.load_model('./{}/twolayermodel/encoder.h5'.format(folder), compile=False, custom_objects={'Sampling': Sampling})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          2688        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           8256        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           1040        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 16)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4)            68          leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 4)            0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 10)           50          leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "logvar (Dense)                  (None, 10)           50          leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sampling (Sampling)             (None, 10)           0           mean[0][0]                       \n",
      "                                                                 logvar[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 12,152\n",
      "Trainable params: 12,152\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_reconstructed.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost sensitive labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68134, 20)\n",
      "(68134, 4)\n",
      "(132950, 20)\n",
      "(132950, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HSL\\.conda\\envs\\TensorFlow\\lib\\site-packages\\pandas\\core\\indexing.py:1596: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "C:\\Users\\HSL\\.conda\\envs\\TensorFlow\\lib\\site-packages\\pandas\\core\\indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n",
      "C:\\Users\\HSL\\.conda\\envs\\TensorFlow\\lib\\site-packages\\pandas\\core\\indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>ATT&amp;CK_Tactic</th>\n",
       "      <th>ATT&amp;CK_Technique</th>\n",
       "      <th>Sample_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16933</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57237</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10863</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9743</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56951</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62316</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119548</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120236</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87321</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22896</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132950 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Label  ATT&CK_Tactic  ATT&CK_Technique  Sample_weight\n",
       "16933       0            0.0               0.0            1.0\n",
       "57237       0            0.0               0.0            1.0\n",
       "10863       0            0.0               0.0            1.0\n",
       "9743        0            0.0               0.0            1.0\n",
       "56951       0            0.0               0.0            1.0\n",
       "...       ...            ...               ...            ...\n",
       "62316       1            2.0               2.0            1.5\n",
       "119548      1           -1.0              -1.0            1.0\n",
       "120236      1           -1.0              -1.0            1.0\n",
       "87321       1           -1.0              -1.0            1.0\n",
       "22896       0            0.0               0.0            1.0\n",
       "\n",
       "[132950 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.loc[:, \"Sample_weight\"] = 1.0\n",
    "idx = y_train[y_train[\"Label\"] == 1].index\n",
    "y_train.loc[idx, \"Sample_weight\"] = cost\n",
    "# print(y_train)\n",
    "\n",
    "X_train_abnormal_df = pd.DataFrame(X_train_abnormal_generated, columns = X_train.columns)\n",
    "y_train_abnormal_df = pd.DataFrame({\"Label\": y_train_abnormal_generated})\n",
    "y_train_abnormal_df.loc[:, \"Sample_weight\"] = 1.0\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "X_train_new = X_train.append(X_train_abnormal_df, ignore_index = True)\n",
    "y_train_new = y_train.append(y_train_abnormal_df, ignore_index = True)\n",
    "y_train_new = y_train_new.fillna(-1)\n",
    "# X_train_new = X_train\n",
    "# y_train_new = y_train\n",
    "\n",
    "idx = np.random.permutation(X_train_new.index)\n",
    "X_train_new = X_train_new.reindex(idx)\n",
    "y_train_new = y_train_new.reindex(idx)\n",
    "\n",
    "print(X_train_new.shape)\n",
    "print(y_train_new.shape)\n",
    "y_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66807\n",
      "66143\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train_new[y_train_new[\"Label\"] == 0]))\n",
    "print(len(y_train_new[y_train_new[\"Label\"] == 1]))\n",
    "\n",
    "# weight_for_0 = (1 / len(y_train_new[y_train_new[\"Anomaly\"] == 0]))*(len(y_train_new))/2.0 \n",
    "# weight_for_1 = (1 / len(y_train_new[y_train_new[\"Anomaly\"] == 1]))*(len(y_train_new))/2.0\n",
    "# class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "# class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>ATT&amp;CK_Tactic</th>\n",
       "      <th>ATT&amp;CK_Technique</th>\n",
       "      <th>Sample_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115564</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71716</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70235</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71063</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129903</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105577</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73437</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119548</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120236</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87321</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64816 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Label  ATT&CK_Tactic  ATT&CK_Technique  Sample_weight\n",
       "115564      1           -1.0              -1.0            1.0\n",
       "71716       1           -1.0              -1.0            1.0\n",
       "70235       1           -1.0              -1.0            1.0\n",
       "71063       1           -1.0              -1.0            1.0\n",
       "129903      1           -1.0              -1.0            1.0\n",
       "...       ...            ...               ...            ...\n",
       "105577      1           -1.0              -1.0            1.0\n",
       "73437       1           -1.0              -1.0            1.0\n",
       "119548      1           -1.0              -1.0            1.0\n",
       "120236      1           -1.0              -1.0            1.0\n",
       "87321       1           -1.0              -1.0            1.0\n",
       "\n",
       "[64816 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_new[y_train_new['ATT&CK_Technique']==-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder + MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed data into Encoder to get latent code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303\n",
      "303\n"
     ]
    }
   ],
   "source": [
    "idx = y_test[y_test[\"Label\"] == 0].index\n",
    "idx_normal_selected = idx[:len(y_test[y_test[\"Label\"] == 0]) - len(y_test[y_test[\"Label\"] == 1])]\n",
    "X_test = X_test.drop(idx_normal_selected)\n",
    "y_test = y_test.drop(idx_normal_selected)\n",
    "\n",
    "print(len(y_test[y_test['Label'] == 0]))\n",
    "print(len(y_test[y_test['Label'] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new_dataset = X_train_new.to_numpy()\n",
    "train_mean, train_logvar, train_z = encoder_reconstructed.predict(X_train_new_dataset, batch_size=512)\n",
    "\n",
    "X_test_new_dataset = X_test.to_numpy()\n",
    "test_mean, test_logvar, test_z = encoder_reconstructed.predict(X_test_new_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "(132950, 10)\n",
      "(606, 10)\n"
     ]
    }
   ],
   "source": [
    "print(latent_dim)\n",
    "print(train_z.shape)\n",
    "print(test_z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mlp\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               1408      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 16,993\n",
      "Trainable params: 16,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(128, activation=\"relu\")(mlp_inputs)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "mlp_outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "mlp = keras.Model(mlp_inputs, mlp_outputs, name=\"mlp\")\n",
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.2235 - binary_accuracy: 0.9148 - precision: 0.9153 - recall: 0.9131 - val_loss: 0.1540 - val_binary_accuracy: 0.9454 - val_precision: 0.9479 - val_recall: 0.9430\n",
      "Epoch 2/100\n",
      "468/468 [==============================] - 2s 4ms/step - loss: 0.1524 - binary_accuracy: 0.9466 - precision: 0.9486 - recall: 0.9438 - val_loss: 0.1282 - val_binary_accuracy: 0.9558 - val_precision: 0.9673 - val_recall: 0.9439\n",
      "Epoch 3/100\n",
      "468/468 [==============================] - 2s 4ms/step - loss: 0.1333 - binary_accuracy: 0.9545 - precision: 0.9566 - recall: 0.9517 - val_loss: 0.1173 - val_binary_accuracy: 0.9607 - val_precision: 0.9699 - val_recall: 0.9511\n",
      "Epoch 4/100\n",
      "468/468 [==============================] - 2s 4ms/step - loss: 0.1225 - binary_accuracy: 0.9590 - precision: 0.9604 - recall: 0.9570 - val_loss: 0.0997 - val_binary_accuracy: 0.9677 - val_precision: 0.9745 - val_recall: 0.9609\n",
      "Epoch 5/100\n",
      "468/468 [==============================] - 2s 4ms/step - loss: 0.1147 - binary_accuracy: 0.9616 - precision: 0.9615 - recall: 0.9613 - val_loss: 0.0959 - val_binary_accuracy: 0.9674 - val_precision: 0.9673 - val_recall: 0.9676\n",
      "Epoch 6/100\n",
      "468/468 [==============================] - 2s 4ms/step - loss: 0.1086 - binary_accuracy: 0.9639 - precision: 0.9647 - recall: 0.9627 - val_loss: 0.0956 - val_binary_accuracy: 0.9682 - val_precision: 0.9699 - val_recall: 0.9666\n",
      "Epoch 7/100\n",
      "468/468 [==============================] - 2s 4ms/step - loss: 0.1040 - binary_accuracy: 0.9656 - precision: 0.9657 - recall: 0.9651 - val_loss: 0.0917 - val_binary_accuracy: 0.9704 - val_precision: 0.9770 - val_recall: 0.9637\n",
      "Epoch 8/100\n",
      "468/468 [==============================] - 2s 4ms/step - loss: 0.1019 - binary_accuracy: 0.9665 - precision: 0.9682 - recall: 0.9642 - val_loss: 0.0862 - val_binary_accuracy: 0.9734 - val_precision: 0.9770 - val_recall: 0.9699\n",
      "Epoch 9/100\n",
      "468/468 [==============================] - 2s 4ms/step - loss: 0.0990 - binary_accuracy: 0.9679 - precision: 0.9689 - recall: 0.9664 - val_loss: 0.0841 - val_binary_accuracy: 0.9721 - val_precision: 0.9724 - val_recall: 0.9720\n",
      "Epoch 10/100\n",
      "468/468 [==============================] - 2s 4ms/step - loss: 0.0968 - binary_accuracy: 0.9682 - precision: 0.9705 - recall: 0.9654 - val_loss: 0.0857 - val_binary_accuracy: 0.9738 - val_precision: 0.9797 - val_recall: 0.9679\n",
      "Epoch 11/100\n",
      "468/468 [==============================] - 2s 4ms/step - loss: 0.0957 - binary_accuracy: 0.9694 - precision: 0.9716 - recall: 0.9667 - val_loss: 0.0823 - val_binary_accuracy: 0.9736 - val_precision: 0.9778 - val_recall: 0.9694\n",
      "Epoch 12/100\n",
      "468/468 [==============================] - 2s 4ms/step - loss: 0.0931 - binary_accuracy: 0.9703 - precision: 0.9723 - recall: 0.9679 - val_loss: 0.0787 - val_binary_accuracy: 0.9741 - val_precision: 0.9769 - val_recall: 0.9714\n",
      "Epoch 13/100\n",
      "468/468 [==============================] - 2s 4ms/step - loss: 0.0928 - binary_accuracy: 0.9703 - precision: 0.9729 - recall: 0.9672 - val_loss: 0.0800 - val_binary_accuracy: 0.9750 - val_precision: 0.9791 - val_recall: 0.9708\n",
      "Epoch 14/100\n",
      "468/468 [==============================] - 2s 4ms/step - loss: 0.0918 - binary_accuracy: 0.9706 - precision: 0.9733 - recall: 0.9673 - val_loss: 0.0804 - val_binary_accuracy: 0.9753 - val_precision: 0.9847 - val_recall: 0.9657\n",
      "Epoch 15/100\n",
      "468/468 [==============================] - 2s 4ms/step - loss: 0.0891 - binary_accuracy: 0.9716 - precision: 0.9742 - recall: 0.9685 - val_loss: 0.0752 - val_binary_accuracy: 0.9767 - val_precision: 0.9833 - val_recall: 0.9700\n",
      "Epoch 16/100\n",
      "468/468 [==============================] - 2s 5ms/step - loss: 0.0889 - binary_accuracy: 0.9713 - precision: 0.9738 - recall: 0.9682 - val_loss: 0.0775 - val_binary_accuracy: 0.9754 - val_precision: 0.9816 - val_recall: 0.9691\n",
      "Epoch 17/100\n",
      "468/468 [==============================] - 2s 4ms/step - loss: 0.0870 - binary_accuracy: 0.9726 - precision: 0.9751 - recall: 0.9695 - val_loss: 0.0752 - val_binary_accuracy: 0.9762 - val_precision: 0.9818 - val_recall: 0.9705\n",
      "Epoch 18/100\n",
      "468/468 [==============================] - 2s 4ms/step - loss: 0.0862 - binary_accuracy: 0.9727 - precision: 0.9754 - recall: 0.9696 - val_loss: 0.0710 - val_binary_accuracy: 0.9784 - val_precision: 0.9817 - val_recall: 0.9751\n",
      "Epoch 19/100\n",
      "468/468 [==============================] - 2s 5ms/step - loss: 0.0872 - binary_accuracy: 0.9722 - precision: 0.9740 - recall: 0.9701 - val_loss: 0.0739 - val_binary_accuracy: 0.9774 - val_precision: 0.9852 - val_recall: 0.9696\n",
      "Epoch 20/100\n",
      "468/468 [==============================] - 2s 5ms/step - loss: 0.0845 - binary_accuracy: 0.9735 - precision: 0.9763 - recall: 0.9703 - val_loss: 0.0731 - val_binary_accuracy: 0.9770 - val_precision: 0.9820 - val_recall: 0.9720\n",
      "Epoch 21/100\n",
      "468/468 [==============================] - 2s 4ms/step - loss: 0.0855 - binary_accuracy: 0.9729 - precision: 0.9755 - recall: 0.9698 - val_loss: 0.0751 - val_binary_accuracy: 0.9755 - val_precision: 0.9818 - val_recall: 0.9691\n",
      "Epoch 22/100\n",
      "468/468 [==============================] - 2s 5ms/step - loss: 0.0838 - binary_accuracy: 0.9735 - precision: 0.9763 - recall: 0.9703 - val_loss: 0.0718 - val_binary_accuracy: 0.9771 - val_precision: 0.9848 - val_recall: 0.9694\n",
      "Epoch 23/100\n",
      "468/468 [==============================] - 2s 5ms/step - loss: 0.0836 - binary_accuracy: 0.9736 - precision: 0.9762 - recall: 0.9706 - val_loss: 0.0701 - val_binary_accuracy: 0.9774 - val_precision: 0.9818 - val_recall: 0.9729\n",
      "Epoch 24/100\n",
      "468/468 [==============================] - 3s 5ms/step - loss: 0.0813 - binary_accuracy: 0.9748 - precision: 0.9775 - recall: 0.9717 - val_loss: 0.0701 - val_binary_accuracy: 0.9783 - val_precision: 0.9852 - val_recall: 0.9712\n",
      "Epoch 25/100\n",
      "468/468 [==============================] - 2s 5ms/step - loss: 0.0811 - binary_accuracy: 0.9746 - precision: 0.9778 - recall: 0.9710 - val_loss: 0.0726 - val_binary_accuracy: 0.9782 - val_precision: 0.9870 - val_recall: 0.9693\n",
      "Epoch 26/100\n",
      "468/468 [==============================] - 2s 5ms/step - loss: 0.0817 - binary_accuracy: 0.9749 - precision: 0.9781 - recall: 0.9712 - val_loss: 0.0709 - val_binary_accuracy: 0.9787 - val_precision: 0.9831 - val_recall: 0.9744\n",
      "Epoch 27/100\n",
      "468/468 [==============================] - 2s 5ms/step - loss: 0.0806 - binary_accuracy: 0.9752 - precision: 0.9785 - recall: 0.9714 - val_loss: 0.0699 - val_binary_accuracy: 0.9789 - val_precision: 0.9851 - val_recall: 0.9726\n",
      "Epoch 28/100\n",
      "468/468 [==============================] - 2s 5ms/step - loss: 0.0805 - binary_accuracy: 0.9750 - precision: 0.9779 - recall: 0.9717 - val_loss: 0.0724 - val_binary_accuracy: 0.9780 - val_precision: 0.9836 - val_recall: 0.9724\n",
      "Epoch 29/100\n",
      "468/468 [==============================] - 2s 5ms/step - loss: 0.0814 - binary_accuracy: 0.9747 - precision: 0.9777 - recall: 0.9712 - val_loss: 0.0684 - val_binary_accuracy: 0.9798 - val_precision: 0.9857 - val_recall: 0.9738\n",
      "Epoch 30/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0794 - binary_accuracy: 0.9754 - precision: 0.9786 - recall: 0.9717 - val_loss: 0.0725 - val_binary_accuracy: 0.9777 - val_precision: 0.9873 - val_recall: 0.9681\n",
      "Epoch 31/100\n",
      "468/468 [==============================] - 2s 5ms/step - loss: 0.0794 - binary_accuracy: 0.9757 - precision: 0.9794 - recall: 0.9716 - val_loss: 0.0661 - val_binary_accuracy: 0.9800 - val_precision: 0.9859 - val_recall: 0.9741\n",
      "Epoch 32/100\n",
      "468/468 [==============================] - 2s 5ms/step - loss: 0.0784 - binary_accuracy: 0.9761 - precision: 0.9793 - recall: 0.9724 - val_loss: 0.0679 - val_binary_accuracy: 0.9792 - val_precision: 0.9854 - val_recall: 0.9729\n",
      "Epoch 33/100\n",
      "468/468 [==============================] - 2s 5ms/step - loss: 0.0779 - binary_accuracy: 0.9759 - precision: 0.9790 - recall: 0.9723 - val_loss: 0.0662 - val_binary_accuracy: 0.9807 - val_precision: 0.9866 - val_recall: 0.9747\n",
      "Epoch 34/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0774 - binary_accuracy: 0.9759 - precision: 0.9792 - recall: 0.9721 - val_loss: 0.0660 - val_binary_accuracy: 0.9802 - val_precision: 0.9863 - val_recall: 0.9741\n",
      "Epoch 35/100\n",
      "468/468 [==============================] - 2s 5ms/step - loss: 0.0778 - binary_accuracy: 0.9760 - precision: 0.9793 - recall: 0.9723 - val_loss: 0.0625 - val_binary_accuracy: 0.9819 - val_precision: 0.9873 - val_recall: 0.9766\n",
      "Epoch 36/100\n",
      "468/468 [==============================] - 3s 5ms/step - loss: 0.0763 - binary_accuracy: 0.9769 - precision: 0.9803 - recall: 0.9731 - val_loss: 0.0653 - val_binary_accuracy: 0.9806 - val_precision: 0.9869 - val_recall: 0.9742\n",
      "Epoch 37/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0760 - binary_accuracy: 0.9763 - precision: 0.9799 - recall: 0.9723 - val_loss: 0.0662 - val_binary_accuracy: 0.9793 - val_precision: 0.9879 - val_recall: 0.9706\n",
      "Epoch 38/100\n",
      "468/468 [==============================] - 2s 5ms/step - loss: 0.0770 - binary_accuracy: 0.9765 - precision: 0.9798 - recall: 0.9727 - val_loss: 0.0689 - val_binary_accuracy: 0.9786 - val_precision: 0.9882 - val_recall: 0.9688\n",
      "Epoch 39/100\n",
      "468/468 [==============================] - 2s 5ms/step - loss: 0.0755 - binary_accuracy: 0.9768 - precision: 0.9804 - recall: 0.9728 - val_loss: 0.0675 - val_binary_accuracy: 0.9803 - val_precision: 0.9846 - val_recall: 0.9760\n",
      "Epoch 40/100\n",
      "468/468 [==============================] - 3s 5ms/step - loss: 0.0755 - binary_accuracy: 0.9769 - precision: 0.9805 - recall: 0.9728 - val_loss: 0.0669 - val_binary_accuracy: 0.9792 - val_precision: 0.9888 - val_recall: 0.9696\n",
      "Epoch 41/100\n",
      "468/468 [==============================] - 2s 5ms/step - loss: 0.0752 - binary_accuracy: 0.9770 - precision: 0.9807 - recall: 0.9729 - val_loss: 0.0648 - val_binary_accuracy: 0.9805 - val_precision: 0.9871 - val_recall: 0.9739\n",
      "Epoch 42/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0750 - binary_accuracy: 0.9769 - precision: 0.9806 - recall: 0.9729 - val_loss: 0.0633 - val_binary_accuracy: 0.9804 - val_precision: 0.9878 - val_recall: 0.9730\n",
      "Epoch 43/100\n",
      "468/468 [==============================] - 2s 5ms/step - loss: 0.0743 - binary_accuracy: 0.9776 - precision: 0.9816 - recall: 0.9733 - val_loss: 0.0638 - val_binary_accuracy: 0.9804 - val_precision: 0.9865 - val_recall: 0.9742\n",
      "Epoch 44/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0746 - binary_accuracy: 0.9770 - precision: 0.9803 - recall: 0.9733 - val_loss: 0.0657 - val_binary_accuracy: 0.9808 - val_precision: 0.9869 - val_recall: 0.9747\n",
      "Epoch 45/100\n",
      "468/468 [==============================] - 3s 5ms/step - loss: 0.0748 - binary_accuracy: 0.9773 - precision: 0.9813 - recall: 0.9730 - val_loss: 0.0663 - val_binary_accuracy: 0.9807 - val_precision: 0.9886 - val_recall: 0.9727\n",
      "Epoch 46/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0747 - binary_accuracy: 0.9774 - precision: 0.9811 - recall: 0.9733 - val_loss: 0.0642 - val_binary_accuracy: 0.9810 - val_precision: 0.9890 - val_recall: 0.9730\n",
      "Epoch 47/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0746 - binary_accuracy: 0.9772 - precision: 0.9807 - recall: 0.9732 - val_loss: 0.0622 - val_binary_accuracy: 0.9812 - val_precision: 0.9892 - val_recall: 0.9732\n",
      "Epoch 48/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0731 - binary_accuracy: 0.9782 - precision: 0.9821 - recall: 0.9739 - val_loss: 0.0650 - val_binary_accuracy: 0.9798 - val_precision: 0.9865 - val_recall: 0.9730\n",
      "Epoch 49/100\n",
      "468/468 [==============================] - 2s 5ms/step - loss: 0.0737 - binary_accuracy: 0.9776 - precision: 0.9816 - recall: 0.9731 - val_loss: 0.0659 - val_binary_accuracy: 0.9801 - val_precision: 0.9875 - val_recall: 0.9727\n",
      "Epoch 50/100\n",
      "468/468 [==============================] - 3s 5ms/step - loss: 0.0732 - binary_accuracy: 0.9778 - precision: 0.9809 - recall: 0.9744 - val_loss: 0.0630 - val_binary_accuracy: 0.9817 - val_precision: 0.9876 - val_recall: 0.9759\n",
      "Epoch 51/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0732 - binary_accuracy: 0.9777 - precision: 0.9809 - recall: 0.9741 - val_loss: 0.0635 - val_binary_accuracy: 0.9810 - val_precision: 0.9875 - val_recall: 0.9744\n",
      "Epoch 52/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0741 - binary_accuracy: 0.9771 - precision: 0.9806 - recall: 0.9733 - val_loss: 0.0603 - val_binary_accuracy: 0.9819 - val_precision: 0.9867 - val_recall: 0.9771\n",
      "Epoch 53/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0737 - binary_accuracy: 0.9778 - precision: 0.9809 - recall: 0.9743 - val_loss: 0.0630 - val_binary_accuracy: 0.9804 - val_precision: 0.9850 - val_recall: 0.9759\n",
      "Epoch 54/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0719 - binary_accuracy: 0.9784 - precision: 0.9820 - recall: 0.9745 - val_loss: 0.0624 - val_binary_accuracy: 0.9805 - val_precision: 0.9852 - val_recall: 0.9759\n",
      "Epoch 55/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0722 - binary_accuracy: 0.9784 - precision: 0.9816 - recall: 0.9748 - val_loss: 0.0657 - val_binary_accuracy: 0.9810 - val_precision: 0.9898 - val_recall: 0.9721\n",
      "Epoch 56/100\n",
      "468/468 [==============================] - 3s 5ms/step - loss: 0.0724 - binary_accuracy: 0.9782 - precision: 0.9817 - recall: 0.9743 - val_loss: 0.0640 - val_binary_accuracy: 0.9803 - val_precision: 0.9869 - val_recall: 0.9736\n",
      "Epoch 57/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0729 - binary_accuracy: 0.9781 - precision: 0.9817 - recall: 0.9740 - val_loss: 0.0607 - val_binary_accuracy: 0.9810 - val_precision: 0.9881 - val_recall: 0.9738\n",
      "Epoch 58/100\n",
      "468/468 [==============================] - 3s 5ms/step - loss: 0.0720 - binary_accuracy: 0.9780 - precision: 0.9817 - recall: 0.9739 - val_loss: 0.0627 - val_binary_accuracy: 0.9804 - val_precision: 0.9865 - val_recall: 0.9742\n",
      "Epoch 59/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0715 - binary_accuracy: 0.9782 - precision: 0.9816 - recall: 0.9743 - val_loss: 0.0639 - val_binary_accuracy: 0.9804 - val_precision: 0.9868 - val_recall: 0.9739\n",
      "Epoch 60/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0714 - binary_accuracy: 0.9785 - precision: 0.9817 - recall: 0.9749 - val_loss: 0.0614 - val_binary_accuracy: 0.9811 - val_precision: 0.9884 - val_recall: 0.9738\n",
      "Epoch 61/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0716 - binary_accuracy: 0.9786 - precision: 0.9819 - recall: 0.9749 - val_loss: 0.0619 - val_binary_accuracy: 0.9816 - val_precision: 0.9867 - val_recall: 0.9766\n",
      "Epoch 62/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0707 - binary_accuracy: 0.9790 - precision: 0.9824 - recall: 0.9751 - val_loss: 0.0604 - val_binary_accuracy: 0.9818 - val_precision: 0.9898 - val_recall: 0.9738\n",
      "Epoch 63/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0709 - binary_accuracy: 0.9788 - precision: 0.9821 - recall: 0.9750 - val_loss: 0.0640 - val_binary_accuracy: 0.9805 - val_precision: 0.9871 - val_recall: 0.9739\n",
      "Epoch 64/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0720 - binary_accuracy: 0.9784 - precision: 0.9821 - recall: 0.9744 - val_loss: 0.0613 - val_binary_accuracy: 0.9815 - val_precision: 0.9883 - val_recall: 0.9747\n",
      "Epoch 65/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0711 - binary_accuracy: 0.9781 - precision: 0.9810 - recall: 0.9749 - val_loss: 0.0637 - val_binary_accuracy: 0.9812 - val_precision: 0.9889 - val_recall: 0.9735\n",
      "Epoch 66/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0706 - binary_accuracy: 0.9789 - precision: 0.9825 - recall: 0.9750 - val_loss: 0.0626 - val_binary_accuracy: 0.9801 - val_precision: 0.9884 - val_recall: 0.9717\n",
      "Epoch 67/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0695 - binary_accuracy: 0.9790 - precision: 0.9830 - recall: 0.9746 - val_loss: 0.0597 - val_binary_accuracy: 0.9822 - val_precision: 0.9870 - val_recall: 0.9774\n",
      "Epoch 68/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0698 - binary_accuracy: 0.9789 - precision: 0.9829 - recall: 0.9746 - val_loss: 0.0614 - val_binary_accuracy: 0.9826 - val_precision: 0.9889 - val_recall: 0.9763\n",
      "Epoch 69/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0701 - binary_accuracy: 0.9791 - precision: 0.9827 - recall: 0.9750 - val_loss: 0.0625 - val_binary_accuracy: 0.9810 - val_precision: 0.9892 - val_recall: 0.9729\n",
      "Epoch 70/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0701 - binary_accuracy: 0.9788 - precision: 0.9822 - recall: 0.9751 - val_loss: 0.0640 - val_binary_accuracy: 0.9808 - val_precision: 0.9890 - val_recall: 0.9726\n",
      "Epoch 71/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0695 - binary_accuracy: 0.9793 - precision: 0.9828 - recall: 0.9753 - val_loss: 0.0596 - val_binary_accuracy: 0.9825 - val_precision: 0.9880 - val_recall: 0.9769\n",
      "Epoch 72/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0697 - binary_accuracy: 0.9789 - precision: 0.9825 - recall: 0.9748 - val_loss: 0.0624 - val_binary_accuracy: 0.9809 - val_precision: 0.9890 - val_recall: 0.9727\n",
      "Epoch 73/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0691 - binary_accuracy: 0.9793 - precision: 0.9829 - recall: 0.9754 - val_loss: 0.0597 - val_binary_accuracy: 0.9829 - val_precision: 0.9885 - val_recall: 0.9772\n",
      "Epoch 74/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0695 - binary_accuracy: 0.9789 - precision: 0.9824 - recall: 0.9750 - val_loss: 0.0621 - val_binary_accuracy: 0.9822 - val_precision: 0.9873 - val_recall: 0.9771\n",
      "Epoch 75/100\n",
      "468/468 [==============================] - 3s 7ms/step - loss: 0.0695 - binary_accuracy: 0.9792 - precision: 0.9823 - recall: 0.9758 - val_loss: 0.0619 - val_binary_accuracy: 0.9828 - val_precision: 0.9898 - val_recall: 0.9757\n",
      "Epoch 76/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0690 - binary_accuracy: 0.9791 - precision: 0.9826 - recall: 0.9752 - val_loss: 0.0616 - val_binary_accuracy: 0.9818 - val_precision: 0.9868 - val_recall: 0.9768\n",
      "Epoch 77/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0686 - binary_accuracy: 0.9791 - precision: 0.9824 - recall: 0.9755 - val_loss: 0.0608 - val_binary_accuracy: 0.9822 - val_precision: 0.9873 - val_recall: 0.9772\n",
      "Epoch 78/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0698 - binary_accuracy: 0.9790 - precision: 0.9824 - recall: 0.9753 - val_loss: 0.0623 - val_binary_accuracy: 0.9822 - val_precision: 0.9877 - val_recall: 0.9766\n",
      "Epoch 79/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0682 - binary_accuracy: 0.9795 - precision: 0.9831 - recall: 0.9756 - val_loss: 0.0611 - val_binary_accuracy: 0.9813 - val_precision: 0.9881 - val_recall: 0.9745\n",
      "Epoch 80/100\n",
      "468/468 [==============================] - 3s 5ms/step - loss: 0.0687 - binary_accuracy: 0.9794 - precision: 0.9827 - recall: 0.9756 - val_loss: 0.0636 - val_binary_accuracy: 0.9805 - val_precision: 0.9860 - val_recall: 0.9750\n",
      "Epoch 81/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0690 - binary_accuracy: 0.9793 - precision: 0.9830 - recall: 0.9753 - val_loss: 0.0618 - val_binary_accuracy: 0.9817 - val_precision: 0.9910 - val_recall: 0.9724\n",
      "Epoch 82/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0680 - binary_accuracy: 0.9798 - precision: 0.9839 - recall: 0.9753 - val_loss: 0.0606 - val_binary_accuracy: 0.9813 - val_precision: 0.9880 - val_recall: 0.9745\n",
      "Epoch 83/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0691 - binary_accuracy: 0.9795 - precision: 0.9836 - recall: 0.9749 - val_loss: 0.0589 - val_binary_accuracy: 0.9815 - val_precision: 0.9876 - val_recall: 0.9754\n",
      "Epoch 84/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0686 - binary_accuracy: 0.9797 - precision: 0.9834 - recall: 0.9756 - val_loss: 0.0630 - val_binary_accuracy: 0.9810 - val_precision: 0.9855 - val_recall: 0.9766\n",
      "Epoch 85/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0679 - binary_accuracy: 0.9800 - precision: 0.9839 - recall: 0.9758 - val_loss: 0.0595 - val_binary_accuracy: 0.9819 - val_precision: 0.9887 - val_recall: 0.9751\n",
      "Epoch 86/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0677 - binary_accuracy: 0.9795 - precision: 0.9828 - recall: 0.9758 - val_loss: 0.0586 - val_binary_accuracy: 0.9829 - val_precision: 0.9880 - val_recall: 0.9778\n",
      "Epoch 87/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0675 - binary_accuracy: 0.9796 - precision: 0.9836 - recall: 0.9752 - val_loss: 0.0587 - val_binary_accuracy: 0.9822 - val_precision: 0.9896 - val_recall: 0.9748\n",
      "Epoch 88/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0684 - binary_accuracy: 0.9794 - precision: 0.9827 - recall: 0.9756 - val_loss: 0.0590 - val_binary_accuracy: 0.9821 - val_precision: 0.9883 - val_recall: 0.9759\n",
      "Epoch 89/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0672 - binary_accuracy: 0.9799 - precision: 0.9833 - recall: 0.9760 - val_loss: 0.0602 - val_binary_accuracy: 0.9816 - val_precision: 0.9883 - val_recall: 0.9750\n",
      "Epoch 90/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0664 - binary_accuracy: 0.9802 - precision: 0.9830 - recall: 0.9770 - val_loss: 0.0614 - val_binary_accuracy: 0.9820 - val_precision: 0.9895 - val_recall: 0.9745\n",
      "Epoch 91/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0674 - binary_accuracy: 0.9798 - precision: 0.9837 - recall: 0.9755 - val_loss: 0.0603 - val_binary_accuracy: 0.9822 - val_precision: 0.9876 - val_recall: 0.9768\n",
      "Epoch 92/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0685 - binary_accuracy: 0.9798 - precision: 0.9831 - recall: 0.9760 - val_loss: 0.0594 - val_binary_accuracy: 0.9817 - val_precision: 0.9896 - val_recall: 0.9738\n",
      "Epoch 93/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0679 - binary_accuracy: 0.9798 - precision: 0.9834 - recall: 0.9759 - val_loss: 0.0616 - val_binary_accuracy: 0.9816 - val_precision: 0.9907 - val_recall: 0.9726\n",
      "Epoch 94/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0675 - binary_accuracy: 0.9797 - precision: 0.9836 - recall: 0.9754 - val_loss: 0.0612 - val_binary_accuracy: 0.9819 - val_precision: 0.9902 - val_recall: 0.9736\n",
      "Epoch 95/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0670 - binary_accuracy: 0.9799 - precision: 0.9835 - recall: 0.9759 - val_loss: 0.0603 - val_binary_accuracy: 0.9819 - val_precision: 0.9885 - val_recall: 0.9754\n",
      "Epoch 96/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0661 - binary_accuracy: 0.9805 - precision: 0.9840 - recall: 0.9767 - val_loss: 0.0624 - val_binary_accuracy: 0.9809 - val_precision: 0.9887 - val_recall: 0.9730\n",
      "Epoch 97/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0673 - binary_accuracy: 0.9798 - precision: 0.9837 - recall: 0.9755 - val_loss: 0.0609 - val_binary_accuracy: 0.9822 - val_precision: 0.9889 - val_recall: 0.9756\n",
      "Epoch 98/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0677 - binary_accuracy: 0.9799 - precision: 0.9836 - recall: 0.9757 - val_loss: 0.0607 - val_binary_accuracy: 0.9822 - val_precision: 0.9873 - val_recall: 0.9772\n",
      "Epoch 99/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0671 - binary_accuracy: 0.9800 - precision: 0.9839 - recall: 0.9758 - val_loss: 0.0596 - val_binary_accuracy: 0.9824 - val_precision: 0.9892 - val_recall: 0.9756\n",
      "Epoch 100/100\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.0664 - binary_accuracy: 0.9803 - precision: 0.9836 - recall: 0.9767 - val_loss: 0.0590 - val_binary_accuracy: 0.9820 - val_precision: 0.9873 - val_recall: 0.9768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27d60f0d490>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_new_dataset = y_train_new[\"Label\"].to_numpy()\n",
    "sample_weight = y_train_new[\"Sample_weight\"].to_numpy()\n",
    "\n",
    "y_test_new_dataset = y_test[\"Label\"].to_numpy()\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=0.003)\n",
    "bin_acc = keras.metrics.BinaryAccuracy()\n",
    "presicion = keras.metrics.Precision()\n",
    "recall = tf.keras.metrics.Recall()\n",
    "# mlp.compile(loss=[focal_loss.binary_focal_loss()], optimizer=opt, metrics=[bin_acc, presicion, recall])\n",
    "mlp.compile(loss='binary_crossentropy', optimizer=opt, metrics=[bin_acc, presicion, recall])\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "mlp.fit(train_z, y_train_new_dataset, shuffle=True, batch_size=256, epochs=100, sample_weight=sample_weight, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9895453286847558\n"
     ]
    }
   ],
   "source": [
    "precision = 0.9917\n",
    "recall = 0.9874\n",
    "f1 = 2 * ((precision*recall) / (precision+recall))\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.save('./{}/twolayermodel/mlp.h5'.format(folder, seqlen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and Calculate F1 score (binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_reconstructed = keras.models.load_model('./{}/twolayermodel/mlp.h5'.format(folder, seqlen), compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold=0.041, F-Score=0.87319\n"
     ]
    }
   ],
   "source": [
    "thresholds = np.arange(0, 1, 0.001)\n",
    "def to_labels(pos_probs, threshold):\n",
    "    return (pos_probs >= threshold).astype('int')\n",
    "\n",
    "y_test_predict = mlp_reconstructed.predict(test_z, batch_size=256)\n",
    "scores = [f1_score(y_test_new_dataset, to_labels(y_test_predict, t)) for t in thresholds]\n",
    "ix = np.argmax(scores)\n",
    "print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict = mlp_reconstructed.predict(test_z, batch_size=256)\n",
    "y_test_predict = np.where(y_test_predict >= thresholds[ix], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 303, 1: 303}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(y_test_new_dataset, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 286, 1: 320}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(y_test_predict, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8731942215088283"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test_new_dataset, y_test_predict, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8696369636963697"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_new_dataset, y_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test_new_dataset, y_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8976897689768977"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test_new_dataset, y_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[255,  48],\n",
       "       [ 31, 272]], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test_new_dataset, y_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fifth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HistGradientBoostingClassifier (LightGBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the result from first layer model (training dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_reconstructed = keras.models.load_model('./{}/twolayermodel/mlp.h5'.format(folder, seqlen), compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict = mlp_reconstructed.predict(train_z, batch_size=256)\n",
    "y_train_predict = np.where(y_train_predict >= thresholds[ix], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11304"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_training_abnormal_list = np.where(y_train_predict == 1)[0].tolist()\n",
    "\n",
    "X_train_new = X_train_new.reset_index(drop=True)\n",
    "y_train_new = y_train_new.reset_index(drop=True)\n",
    "\n",
    "filter_X_train = X_train_new.iloc[vae_training_abnormal_list, :]\n",
    "filter_y_train = y_train_new.iloc[vae_training_abnormal_list, :]\n",
    "filter_idx = filter_y_train[filter_y_train['ATT&CK_Technique'] >= 0].index.tolist()\n",
    "filter_X_train = filter_X_train.loc[filter_idx]\n",
    "filter_y_train = filter_y_train.loc[filter_idx]\n",
    "\n",
    "len(filter_y_train[filter_y_train['ATT&CK_Technique'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "# now you can import normally from ensemble\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HistGradientBoostingClassifier(learning_rate=0.03,\n",
       "                               loss='categorical_crossentropy')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = HistGradientBoostingClassifier(loss='categorical_crossentropy', learning_rate=0.03)\n",
    "clf.fit(filter_X_train.to_numpy(), filter_y_train['ATT&CK_Technique'].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_testing_abnormal_list = np.where(y_test_predict == 1)[0].tolist()\n",
    "\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "filter_X_test = X_test.iloc[vae_testing_abnormal_list, :]\n",
    "filter_y_test = y_test.iloc[vae_testing_abnormal_list, :]\n",
    "# filter_idx = filter_y_test[filter_y_test['ATT&CK_Technique'] >= 0].index.tolist()\n",
    "# filter_X_test = filter_X_test.loc[filter_idx]\n",
    "# filter_y_test = filter_y_test.loc[filter_idx]\n",
    "\n",
    "len(filter_y_test[filter_y_test['ATT&CK_Technique'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 0., 4., 4., 0., 2., 4., 2., 2., 4., 2., 2., 4., 4., 4., 2., 4.,\n",
       "       4., 0., 0., 4., 4., 4., 4., 4., 2., 4., 1., 2., 4., 2., 4., 2., 4.,\n",
       "       0., 4., 4., 4., 4., 2., 4., 4., 4., 4., 4., 4., 0., 4., 4., 2., 4.,\n",
       "       2., 4., 4., 0., 4., 2., 2., 4., 2., 2., 4., 4., 2., 2., 4., 4., 4.,\n",
       "       4., 1., 4., 4., 4., 0., 0., 4., 2., 4., 4., 2., 4., 0., 4., 2., 2.,\n",
       "       4., 2., 2., 2., 0., 4., 4., 2., 2., 2., 2., 4., 4., 0., 4., 2., 2.,\n",
       "       4., 0., 2., 4., 2., 4., 0., 4., 4., 4., 0., 2., 4., 4., 4., 4., 2.,\n",
       "       4., 2., 4., 3., 0., 4., 4., 4., 2., 0., 4., 1., 2., 4., 0., 1., 2.,\n",
       "       4., 4., 2., 4., 4., 4., 2., 4., 2., 2., 2., 4., 2., 4., 4., 4., 4.,\n",
       "       4., 4., 4., 0., 2., 4., 0., 4., 4., 2., 4., 0., 4., 2., 2., 2., 4.,\n",
       "       2., 0., 0., 2., 2., 2., 4., 4., 4., 4., 4., 2., 0., 4., 4., 4., 2.,\n",
       "       0., 2., 2., 2., 0., 2., 4., 2., 4., 0., 0., 0., 0., 4., 2., 4., 4.,\n",
       "       4., 0., 4., 4., 2., 4., 4., 4., 4., 2., 4., 4., 2., 2., 4., 1., 4.,\n",
       "       4., 4., 4., 4., 2., 2., 4., 2., 4., 0., 4., 4., 4., 4., 0., 4., 4.,\n",
       "       4., 2., 2., 4., 4., 2., 2., 2., 0., 4., 2., 2., 4., 2., 2., 2., 0.,\n",
       "       2., 4., 4., 4., 2., 0., 2., 4., 2., 0., 0., 4., 0., 4., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 2., 2., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 4., 4., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 4., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre = clf.predict(filter_X_test.to_numpy())\n",
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5430537525744014"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(filter_y_test['ATT&CK_Technique'].to_numpy(), pre, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7375"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(filter_y_test['ATT&CK_Technique'].to_numpy(), pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5956039098924434"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(filter_y_test['ATT&CK_Technique'].to_numpy(), pre, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5495456027990274"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(filter_y_test['ATT&CK_Technique'].to_numpy(), pre, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 46,   0,   0,   0,   2],\n",
       "       [  0,   4,   1,   0,   6],\n",
       "       [ 15,   1,  73,   0,  23],\n",
       "       [  3,   0,   0,   0,   0],\n",
       "       [ 19,   0,  13,   1, 113]], dtype=int64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(filter_y_test['ATT&CK_Technique'].to_numpy(), pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare: Directly use LightGBM to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HistGradientBoostingClassifier(learning_rate=0.03,\n",
       "                               loss='categorical_crossentropy')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_clf = HistGradientBoostingClassifier(loss='categorical_crossentropy', learning_rate=0.03)\n",
    "\n",
    "compare_clf.fit(X_train.to_numpy(), y_train['ATT&CK_Technique'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = compare_clf.predict(X_test_copy.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30632699918444345"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test_copy['ATT&CK_Technique'].to_numpy(), pre, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9855582951743571"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_copy['ATT&CK_Technique'].to_numpy(), pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42400802877745875"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test_copy['ATT&CK_Technique'].to_numpy(), pre, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HSL\\.conda\\envs\\TensorFlow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.268724155993753"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test_copy['ATT&CK_Technique'].to_numpy(), pre, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16716,     5,     0,     0,     2,     8],\n",
       "       [    1,     1,     2,     4,     3,     0],\n",
       "       [   55,     5,    44,     0,    18,     0],\n",
       "       [    3,     0,     0,     0,     0,     0],\n",
       "       [  132,     6,     0,     1,    27,     1],\n",
       "       [    0,     0,     0,     0,     0,     0]], dtype=int64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test_copy['ATT&CK_Technique'].to_numpy(), pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
